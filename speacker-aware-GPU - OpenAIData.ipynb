{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4100183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ydata-profiling in c:\\program files\\python311\\lib\\site-packages (4.2.0)\n",
      "Collecting demoji\n",
      "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
      "                                              0.0/42.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.9/42.9 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.5/1.5 MB 48.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn in c:\\program files\\python311\\lib\\site-packages (1.2.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "                                              0.0/24.0 MB ? eta -:--:--\n",
      "     --------                                5.2/24.0 MB 111.3 MB/s eta 0:00:01\n",
      "     ---------------                         9.4/24.0 MB 100.2 MB/s eta 0:00:01\n",
      "     ----------------------                  13.6/24.0 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------                  13.6/24.0 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------                  13.6/24.0 MB 81.8 MB/s eta 0:00:01\n",
      "     ----------------------------            17.5/24.0 MB 46.7 MB/s eta 0:00:01\n",
      "     -------------------------------------   23.1/24.0 MB 50.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  24.0/24.0 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 24.0/24.0 MB 59.5 MB/s eta 0:00:00\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "                                              0.0/636.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 636.8/636.8 kB 39.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy in c:\\program files\\python311\\lib\\site-packages (3.5.3)\n",
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "                                              0.0/105.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 105.1/105.1 kB ? eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "                                              0.0/7.9 MB ? eta -:--:--\n",
      "     -----------------------------            5.8/7.9 MB 122.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.9/7.9 MB 84.2 MB/s eta 0:00:00\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.1-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "                                              0.0/192.3 MB ? eta -:--:--\n",
      "                                             4.2/192.3 MB 88.9 MB/s eta 0:00:03\n",
      "     -                                       7.4/192.3 MB 67.5 MB/s eta 0:00:03\n",
      "     --                                     13.0/192.3 MB 93.9 MB/s eta 0:00:02\n",
      "     ---                                   18.7/192.3 MB 110.0 MB/s eta 0:00:02\n",
      "     ----                                  23.3/192.3 MB 131.2 MB/s eta 0:00:02\n",
      "     -----                                  26.2/192.3 MB 93.0 MB/s eta 0:00:02\n",
      "     ------                                 33.1/192.3 MB 93.9 MB/s eta 0:00:02\n",
      "     -------                               39.0/192.3 MB 131.2 MB/s eta 0:00:02\n",
      "     --------                              44.5/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "     ---------                             50.4/192.3 MB 131.2 MB/s eta 0:00:02\n",
      "     ----------                            56.0/192.3 MB 129.5 MB/s eta 0:00:02\n",
      "     -----------                           61.5/192.3 MB 110.0 MB/s eta 0:00:02\n",
      "     ------------                          67.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -------------                         72.1/192.3 MB 108.8 MB/s eta 0:00:02\n",
      "     ---------------                        77.9/192.3 MB 93.0 MB/s eta 0:00:02\n",
      "     ----------------                      84.0/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "     -----------------                     89.8/192.3 MB 110.0 MB/s eta 0:00:01\n",
      "     ------------------                    95.5/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------                   101.1/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------                  106.1/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     ---------------------                112.4/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     ----------------------               118.0/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------              123.6/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------------             129.0/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------------            135.0/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     --------------------------           140.6/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     ---------------------------          145.5/192.3 MB 108.8 MB/s eta 0:00:01\n",
      "     ----------------------------         151.6/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------        157.6/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------------------       163.0/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -------------------------------      168.8/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     --------------------------------     174.4/192.3 MB 129.5 MB/s eta 0:00:01\n",
      "     ---------------------------------    179.9/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     ----------------------------------   185.5/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  191.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     -----------------------------------  192.3/192.3 MB 131.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 192.3/192.3 MB 14.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: seaborn in c:\\program files\\python311\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: scipy<1.11,>=1.4.1 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (1.10.1)\n",
      "Requirement already satisfied: pandas!=1.4.0,<2,>1.1 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (1.5.3)\n",
      "Requirement already satisfied: matplotlib<4,>=3.2 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (3.7.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.1 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (1.10.8)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (6.0)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (3.1.2)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.5 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (0.7.5)\n",
      "Requirement already satisfied: numpy<1.24,>=1.16.0 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (1.23.5)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (0.1.12)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (0.12.3)\n",
      "Requirement already satisfied: requests<3,>=2.24.0 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (4.65.0)\n",
      "Requirement already satisfied: multimethod<2,>=1.4 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (1.9.1)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (0.14.0)\n",
      "Requirement already satisfied: typeguard<3,>=2.13.2 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (2.13.3)\n",
      "Requirement already satisfied: imagehash==4.3.1 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (4.3.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.1 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (1.9.2)\n",
      "Requirement already satisfied: dacite>=1.8 in c:\\program files\\python311\\lib\\site-packages (from ydata-profiling) (1.8.1)\n",
      "Requirement already satisfied: PyWavelets in c:\\program files\\python311\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling) (1.4.1)\n",
      "Requirement already satisfied: pillow in c:\\program files\\python311\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling) (9.5.0)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\program files\\python311\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (23.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\program files\\python311\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (3.1)\n",
      "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in c:\\program files\\python311\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (0.2.0)\n",
      "Requirement already satisfied: click in c:\\program files\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python311\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.10.3-cp311-cp311-win_amd64.whl (269 kB)\n",
      "                                              0.0/269.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 269.6/269.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\program files\\python311\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\program files\\python311\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\program files\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\program files\\python311\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\program files\\python311\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\program files\\python311\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\program files\\python311\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\program files\\python311\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\program files\\python311\\lib\\site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\program files\\python311\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\program files\\python311\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\program files\\python311\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\program files\\python311\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python311\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python311\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\program files\\python311\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "                                              0.0/2.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 2.0/2.0 MB 63.5 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "                                              0.0/311.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 311.7/311.7 kB ? eta 0:00:00\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl (2.2 MB)\n",
      "                                              0.0/2.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 2.2/2.2 MB 70.6 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.0-cp311-none-win_amd64.whl (277 kB)\n",
      "                                              0.0/277.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 277.4/277.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python311\\lib\\site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "                                              0.0/166.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 166.4/166.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\program files\\python311\\lib\\site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib<4,>=3.2->ydata-profiling) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python311\\lib\\site-packages (from matplotlib<4,>=3.2->ydata-profiling) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python311\\lib\\site-packages (from matplotlib<4,>=3.2->ydata-profiling) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib<4,>=3.2->ydata-profiling) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\program files\\python311\\lib\\site-packages (from matplotlib<4,>=3.2->ydata-profiling) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\program files\\python311\\lib\\site-packages (from matplotlib<4,>=3.2->ydata-profiling) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python311\\lib\\site-packages (from pandas!=1.4.0,<2,>1.1->ydata-profiling) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\program files\\python311\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (2023.5.7)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\program files\\python311\\lib\\site-packages (from statsmodels<1,>=0.13.2->ydata-profiling) (0.5.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\program files\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\program files\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\program files\\python311\\lib\\site-packages (from tqdm<5,>=4.48.2->ydata-profiling) (0.4.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\program files\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six in c:\\program files\\python311\\lib\\site-packages (from patsy>=0.5.2->statsmodels<1,>=0.13.2->ydata-profiling) (1.16.0)\n",
      "Installing collected packages: safetensors, regex, pyphen, fsspec, filelock, demoji, torch, textstat, nltk, huggingface-hub, gensim, tokenizers, textblob, transformers\n",
      "Successfully installed demoji-1.1.0 filelock-3.13.1 fsspec-2023.10.0 gensim-4.3.2 huggingface-hub-0.19.4 nltk-3.8.1 pyphen-0.14.0 regex-2023.10.3 safetensors-0.4.0 textblob-0.17.1 textstat-0.7.3 tokenizers-0.15.0 torch-2.1.1 transformers-4.35.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script demoji.exe is installed in 'C:\\Users\\766619\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\766619\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\766619\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\766619\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\766619\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install ydata-profiling demoji nltk scikit-learn gensim  textblob spacy textstat transformers torch seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe5d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python311\\Lib\\site-packages\\numba\\core\\decorators.py:262: NumbaDeprecationWarning: \u001b[1mnumba.generated_jit is deprecated. Please see the documentation at: https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-generated-jit for more information and advice on a suitable replacement.\u001b[0m\n",
      "  warnings.warn(msg, NumbaDeprecationWarning)\n",
      "C:\\Program Files\\Python311\\Lib\\site-packages\\visions\\backends\\shared\\nan_handling.py:50: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @nb.jit\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from ydata_profiling import ProfileReport\n",
    "from textstat import flesch_reading_ease  # exploring text complixity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertTokenizer, AdamW\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f918d401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>conversation</th>\n",
       "      <th>word_count</th>\n",
       "      <th>A_bad_intent</th>\n",
       "      <th>B_bad_intent</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first participant in conversation is deceiving...</td>\n",
       "      <td>A: Hey, I have something really important to t...</td>\n",
       "      <td>437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>second participant in conversation is deceivin...</td>\n",
       "      <td>A: Hey Sarah, I heard you recently went on a t...</td>\n",
       "      <td>368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            category  \\\n",
       "0  first participant in conversation is deceiving...   \n",
       "1  second participant in conversation is deceivin...   \n",
       "\n",
       "                                        conversation  word_count  \\\n",
       "0  A: Hey, I have something really important to t...         437   \n",
       "1  A: Hey Sarah, I heard you recently went on a t...         368   \n",
       "\n",
       "   A_bad_intent  B_bad_intent description  \n",
       "0           1.0           0.0         NaN  \n",
       "1           0.0           1.0         NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('conversation_dataset.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c1981cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sep_token(conversation):\n",
    "    token = '[SEP]' # the seperator token for BERT\n",
    "    # split the conversation into turns using line breaks as delimiters\n",
    "    turns = conversation.split('\\n\\n')\n",
    "    turns_with_sep = [turn + token for turn in turns]\n",
    "    return '[CLS]' + ''.join(turns_with_sep)\n",
    " \n",
    "df['conversation'] = df['conversation'].apply(add_sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd523135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS]A: Hey, I have something really important to tell you.[SEP]B: Sure, what's up?[SEP]A: You know that job opportunity I told you about last month? Well, I got it![SEP]B: Wow, that's amazing news! Congratulations![SEP]A: Thank you! I knew you'd be happy for me. B:ut here's the thing, the company wants me to relocate to another city for this position.[SEP]B: Oh, really? That's a big change. Are you considering it?[SEP]A: Well, I've been thinking about it, but I'm not entirely sure yet. There are a few things to consider, like the cost of living and leaving my family behind.[SEP]B: Of course, those are valid concerns. B:ut if it's a great opportunity, maybe it's worth taking the risk?[SEP]A: That's true, but I've been doing some research, and it seems like the company has a lot of internal issues. I'm starting to doubt if it's the right move for me.[SEP]B: Oh, really? That's concerning. What kind of issues are you talking about?[SEP]A: Well, I've heard rumors that they have financial problems and might even be facing bankruptcy. It's making me question their stability.[SEP]B: That does sound risky. Have you tried reaching out to anyone who works there to get more information?[SEP]A: Yes, I've been trying to get in touch with some employees, but surprisingly, they aren't responding to my inquiries. It's making me even more suspicious.[SEP]B: That's strange. Maybe they're just busy or maybe the rumors are true. It's hard to say without concrete information.[SEP]A: Yeah, exactly. I'm really torn about what to do. On one hand, it could be a great opportunity, but on the other hand, all these red flags are adding up.[SEP]B: I completely understand your dilemma. It's a tough decision to make. Maybe you could give it some more time and gather more information before making a final choice?[SEP]A: You're right, I need to be cautious. I'll continue researching and see if I can dig up any concrete evidence to support or debunk these rumors.[SEP]B: That sounds like a smart plan. Trust your instincts and make sure you have all the facts before committing to anything.[SEP]A: Thanks for understanding and giving me your advice. I really appreciate it.[SEP]B: Of course, anytime! I'm here to support you no matter what decision you make. Just remember to prioritize your own well-being and happiness.[SEP]A:[SEP]\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]['conversation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56035e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>conversation</th>\n",
       "      <th>word_count</th>\n",
       "      <th>A_bad_intent</th>\n",
       "      <th>B_bad_intent</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first participant in conversation is deceiving...</td>\n",
       "      <td>[CLS]A: Hey, I have something really important...</td>\n",
       "      <td>437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>second participant in conversation is deceivin...</td>\n",
       "      <td>[CLS]A: Hey Sarah, I heard you recently went o...</td>\n",
       "      <td>368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>both participants in conversation are deceivin...</td>\n",
       "      <td>[CLS]A: Hey, how have you been? It's been a wh...</td>\n",
       "      <td>400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>none of participants in conversation try to de...</td>\n",
       "      <td>[CLS]A: Hey Sarah! How was your weekend?[SEP]B...</td>\n",
       "      <td>494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first participant in conversation is deceiving...</td>\n",
       "      <td>[CLS]A: Hey there! How are you doing today?[SE...</td>\n",
       "      <td>534</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>none of participants in conversation try to de...</td>\n",
       "      <td>[CLS]A: Good morning! How are you today?[SEP]B...</td>\n",
       "      <td>714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>first participant in conversation is deceiving...</td>\n",
       "      <td>[CLS]A: Hey, how have you been? I was just thi...</td>\n",
       "      <td>560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>second participant in conversation is deceivin...</td>\n",
       "      <td>[CLS]A: Hey, I've been thinking about that inv...</td>\n",
       "      <td>724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>both participants in conversation are deceivin...</td>\n",
       "      <td>[CLS]A: Hey, long time no see! How have you be...</td>\n",
       "      <td>556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Both A and B were deceiving each other through...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>none of participants in conversation try to de...</td>\n",
       "      <td>[CLS]A: Did you have a chance to review the re...</td>\n",
       "      <td>558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             category  \\\n",
       "0   first participant in conversation is deceiving...   \n",
       "1   second participant in conversation is deceivin...   \n",
       "2   both participants in conversation are deceivin...   \n",
       "3   none of participants in conversation try to de...   \n",
       "4   first participant in conversation is deceiving...   \n",
       "..                                                ...   \n",
       "95  none of participants in conversation try to de...   \n",
       "96  first participant in conversation is deceiving...   \n",
       "97  second participant in conversation is deceivin...   \n",
       "98  both participants in conversation are deceivin...   \n",
       "99  none of participants in conversation try to de...   \n",
       "\n",
       "                                         conversation  word_count  \\\n",
       "0   [CLS]A: Hey, I have something really important...         437   \n",
       "1   [CLS]A: Hey Sarah, I heard you recently went o...         368   \n",
       "2   [CLS]A: Hey, how have you been? It's been a wh...         400   \n",
       "3   [CLS]A: Hey Sarah! How was your weekend?[SEP]B...         494   \n",
       "4   [CLS]A: Hey there! How are you doing today?[SE...         534   \n",
       "..                                                ...         ...   \n",
       "95  [CLS]A: Good morning! How are you today?[SEP]B...         714   \n",
       "96  [CLS]A: Hey, how have you been? I was just thi...         560   \n",
       "97  [CLS]A: Hey, I've been thinking about that inv...         724   \n",
       "98  [CLS]A: Hey, long time no see! How have you be...         556   \n",
       "99  [CLS]A: Did you have a chance to review the re...         558   \n",
       "\n",
       "    A_bad_intent  B_bad_intent  \\\n",
       "0            1.0           0.0   \n",
       "1            0.0           1.0   \n",
       "2            1.0           1.0   \n",
       "3            0.0           0.0   \n",
       "4            1.0           0.0   \n",
       "..           ...           ...   \n",
       "95           0.0           0.0   \n",
       "96           1.0           0.0   \n",
       "97           0.0           1.0   \n",
       "98           1.0           1.0   \n",
       "99           0.0           0.0   \n",
       "\n",
       "                                          description  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "..                                                ...  \n",
       "95                                                NaN  \n",
       "96                                                NaN  \n",
       "97                                                NaN  \n",
       "98  Both A and B were deceiving each other through...  \n",
       "99                                                NaN  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9add2b",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85cb1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da434bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>conversation</th>\n",
       "      <th>word_count</th>\n",
       "      <th>A_bad_intent</th>\n",
       "      <th>B_bad_intent</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [category, conversation, word_count, A_bad_intent, B_bad_intent, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['conversation'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e698299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(message):\n",
    "    processed = []   \n",
    "    for text in message:\n",
    "        # replaace URLs\n",
    "        text = re.sub(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\",' <URL>',text)        \n",
    "        # Remove HTML/XML tags (if any)\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        # Remove punctuation and symbols (not for now)\n",
    "        # text = re.sub(r'[^\\w\\s]', '', text) \n",
    "        # Remove numbers - not in this dataset!\n",
    "        #text = re.sub(r'\\d+', '', text)\n",
    "        # Remove whitespaces (including new lines and tabs)\n",
    "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "        \n",
    "        processed.append(text)       \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf7afb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>conversation</th>\n",
       "      <th>word_count</th>\n",
       "      <th>A_bad_intent</th>\n",
       "      <th>B_bad_intent</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first participant in conversation is deceiving...</td>\n",
       "      <td>[CLS]A: Hey, I have something really important...</td>\n",
       "      <td>437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>second participant in conversation is deceivin...</td>\n",
       "      <td>[CLS]A: Hey Sarah, I heard you recently went o...</td>\n",
       "      <td>368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>both participants in conversation are deceivin...</td>\n",
       "      <td>[CLS]A: Hey, how have you been? It's been a wh...</td>\n",
       "      <td>400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>none of participants in conversation try to de...</td>\n",
       "      <td>[CLS]A: Hey Sarah! How was your weekend?[SEP]B...</td>\n",
       "      <td>494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first participant in conversation is deceiving...</td>\n",
       "      <td>[CLS]A: Hey there! How are you doing today?[SE...</td>\n",
       "      <td>534</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>none of participants in conversation try to de...</td>\n",
       "      <td>[CLS]A: Good morning! How are you today?[SEP]B...</td>\n",
       "      <td>714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>first participant in conversation is deceiving...</td>\n",
       "      <td>[CLS]A: Hey, how have you been? I was just thi...</td>\n",
       "      <td>560</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>second participant in conversation is deceivin...</td>\n",
       "      <td>[CLS]A: Hey, I've been thinking about that inv...</td>\n",
       "      <td>724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>both participants in conversation are deceivin...</td>\n",
       "      <td>[CLS]A: Hey, long time no see! How have you be...</td>\n",
       "      <td>556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Both A and B were deceiving each other through...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>none of participants in conversation try to de...</td>\n",
       "      <td>[CLS]A: Did you have a chance to review the re...</td>\n",
       "      <td>558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             category  \\\n",
       "0   first participant in conversation is deceiving...   \n",
       "1   second participant in conversation is deceivin...   \n",
       "2   both participants in conversation are deceivin...   \n",
       "3   none of participants in conversation try to de...   \n",
       "4   first participant in conversation is deceiving...   \n",
       "..                                                ...   \n",
       "95  none of participants in conversation try to de...   \n",
       "96  first participant in conversation is deceiving...   \n",
       "97  second participant in conversation is deceivin...   \n",
       "98  both participants in conversation are deceivin...   \n",
       "99  none of participants in conversation try to de...   \n",
       "\n",
       "                                         conversation  word_count  \\\n",
       "0   [CLS]A: Hey, I have something really important...         437   \n",
       "1   [CLS]A: Hey Sarah, I heard you recently went o...         368   \n",
       "2   [CLS]A: Hey, how have you been? It's been a wh...         400   \n",
       "3   [CLS]A: Hey Sarah! How was your weekend?[SEP]B...         494   \n",
       "4   [CLS]A: Hey there! How are you doing today?[SE...         534   \n",
       "..                                                ...         ...   \n",
       "95  [CLS]A: Good morning! How are you today?[SEP]B...         714   \n",
       "96  [CLS]A: Hey, how have you been? I was just thi...         560   \n",
       "97  [CLS]A: Hey, I've been thinking about that inv...         724   \n",
       "98  [CLS]A: Hey, long time no see! How have you be...         556   \n",
       "99  [CLS]A: Did you have a chance to review the re...         558   \n",
       "\n",
       "    A_bad_intent  B_bad_intent  \\\n",
       "0            1.0           0.0   \n",
       "1            0.0           1.0   \n",
       "2            1.0           1.0   \n",
       "3            0.0           0.0   \n",
       "4            1.0           0.0   \n",
       "..           ...           ...   \n",
       "95           0.0           0.0   \n",
       "96           1.0           0.0   \n",
       "97           0.0           1.0   \n",
       "98           1.0           1.0   \n",
       "99           0.0           0.0   \n",
       "\n",
       "                                          description  \n",
       "0                                                 NaN  \n",
       "1                                                 NaN  \n",
       "2                                                 NaN  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "..                                                ...  \n",
       "95                                                NaN  \n",
       "96                                                NaN  \n",
       "97                                                NaN  \n",
       "98  Both A and B were deceiving each other through...  \n",
       "99                                                NaN  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['conversation'] = text_preprocess(list(df['conversation']))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0c7df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of conversation wth more than 512 words\n",
    "long_conversations = sum([1 if len(i.split()) > 512 else 0 for i in df['conversation']])\n",
    "long_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "552e753c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean     505.030000\n",
       "std      109.652109\n",
       "min      272.000000\n",
       "25%      437.750000\n",
       "50%      496.000000\n",
       "75%      566.000000\n",
       "max      982.000000\n",
       "Name: conversation, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['conversation'].str.split().apply(lambda x : len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "941e01f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x158d5ca7010>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAHpCAYAAACmzsSXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQIklEQVR4nO3deVxU9f4/8NewDIs4g4JshYZKKSa5dRU1vQVFZt5M6qaXlNSv3gzMpUytXNLMpZuV5XK9t9RbLuV1KekmKZakIirihuZeWDiQIjMgyvr5/eGPyZFtBmb5zMzr+XjM48Gcc2bmcz7DnPf57AohhAARERFJx8XWCSAiIqLaMUgTERFJikGaiIhIUgzSREREkmKQJiIikhSDNBERkaQYpImIiCTFIG0EIQR0Oh04pJyIiKyJQdoIRUVFUKvVKCoqsnVSiIjIiTBIExERSYpBmoiISFIM0kRERJJikCYiIpIUgzQREZGkbBqk09LSMGjQIISEhEChUGDr1q0G+4UQmDlzJoKDg+Hl5YWYmBicPXvW4JiCggLEx8dDpVLB19cXo0ePRnFxscExx44dw0MPPQRPT0+EhoZi0aJFlj41IiKiJrNpkL5+/ToeeOABLF26tNb9ixYtwpIlS7BixQpkZGSgWbNmiI2Nxc2bN/XHxMfHIzs7Gzt27EBycjLS0tIwduxY/X6dTofHHnsMbdq0QWZmJt59913Mnj0bK1eutPj5ERERNYmQBACxZcsW/fOqqioRFBQk3n33Xf22wsJC4eHhIdavXy+EEOLkyZMCgDh48KD+mG+//VYoFArx22+/CSGEWLZsmWjRooUoLS3VHzN16lRx3333GZ02rVYrAAitVtvY0yMiIjKZtG3SFy9ehEajQUxMjH6bWq1Gz549kZ6eDgBIT0+Hr68vevTooT8mJiYGLi4uyMjI0B/Tr18/KJVK/TGxsbE4ffo0rl27Vutnl5aWQqfTGTyIiIisTdogrdFoAACBgYEG2wMDA/X7NBoNAgICDPa7ubmhZcuWBsfU9h63f8ad5s+fD7VarX+EhoY2/YSIiIhMJG2QtqXp06dDq9XqH5cuXbJ1koiIyAlJG6SDgoIAAHl5eQbb8/Ly9PuCgoKQn59vsL+iogIFBQUGx9T2Hrd/xp08PDygUqkMHkRERNYmbZAOCwtDUFAQUlNT9dt0Oh0yMjIQFRUFAIiKikJhYSEyMzP1x+zatQtVVVXo2bOn/pi0tDSUl5frj9mxYwfuu+8+tGjRwkpnQ0REZDqbBuni4mIcOXIER44cAXCrs9iRI0eQk5MDhUKBiRMn4u2338bXX3+N48ePY8SIEQgJCcHgwYMBAB07dsTjjz+OMWPG4MCBA9i7dy+SkpIwdOhQhISEAAD+9re/QalUYvTo0cjOzsYXX3yBDz/8EJMnT7bRWRMRERnJll3Lv//+ewGgxiMhIUEIcWsY1owZM0RgYKDw8PAQ0dHR4vTp0wbvcfXqVTFs2DDh4+MjVCqVGDlypCgqKjI45ujRo6Jv377Cw8ND3HXXXWLBggUmpZNDsIioMQqvl4pzeUXi8C8F4lx+kSi8Xtrwi4huoxBCCFveJNgDnU4HtVoNrVbL9mkiMkpu4Q1M3XQMP569ot/WL9wfC+IiEeLrZcOUkT2Rtk2aiMheaUvKagRoAEg7ewXTNh2DtqTMRikje8MgTURkZleKy2oE6GppZ6/gSjGDNBmHQZqIyMx0N8vr3V/UwH6iagzSRERmpvJ0r3d/8wb2E1VjkCYiMjN/HyX6hfvXuq9fuD/8fZS17iO6E4M0EZGZqb2VWBAXWSNQ9wv3x8K4SKi9GaTJOByCZQQOwSKixtCWlOFKcRmKbpajuac7/H2UDNBkEjdbJ4CIyFGpvRmUqWlY3U1ERCQpBmkiIiJJMUgTERFJikGaiIhIUgzSREREkmKQJiIikhSDNBERkaQYpImIiCTFIE1ERCQpBmkiIiJJMUgTERFJikGaiIhIUgzSREREkmKQJiIikhSDNBERkaQYpImIiCTFIE1ERCQpBmkiIiJJMUgTERFJikGaiIhIUgzSREREkmKQJiIikhSDNBERkaQYpImIiCTFIE1ERCQpBmkiIiJJMUgTERFJikGaiIhIUgzSREREkmKQJiIikhSDNBERkaQYpImIiCTFIE1ERCQpBmkiIiJJMUgTERFJikGaiIhIUtIH6aKiIkycOBFt2rSBl5cXevfujYMHD+r3CyEwc+ZMBAcHw8vLCzExMTh79qzBexQUFCA+Ph4qlQq+vr4YPXo0iouLrX0qREREJpE+SP/f//0fduzYgc8++wzHjx/HY489hpiYGPz2228AgEWLFmHJkiVYsWIFMjIy0KxZM8TGxuLmzZv694iPj0d2djZ27NiB5ORkpKWlYezYsbY6JSIiIqMohBDC1omoy40bN9C8eXN89dVXGDhwoH579+7dMWDAAMydOxchISF45ZVX8OqrrwIAtFotAgMDsXr1agwdOhSnTp1CREQEDh48iB49egAAtm/fjieeeAK//vorQkJCanxuaWkpSktL9c91Oh1CQ0Oh1WqhUqksfNZERES3SF2SrqioQGVlJTw9PQ22e3l5Yc+ePbh48SI0Gg1iYmL0+9RqNXr27In09HQAQHp6Onx9ffUBGgBiYmLg4uKCjIyMWj93/vz5UKvV+kdoaKgFzo6IiKh+Ugfp5s2bIyoqCnPnzkVubi4qKyvx+eefIz09HZcvX4ZGowEABAYGGrwuMDBQv0+j0SAgIMBgv5ubG1q2bKk/5k7Tp0+HVqvVPy5dumSBsyMiIqqf1EEaAD777DMIIXDXXXfBw8MDS5YswbBhw+DiYrmke3h4QKVSGTyIiIisTfog3a5dO+zevRvFxcW4dOkSDhw4gPLycrRt2xZBQUEAgLy8PIPX5OXl6fcFBQUhPz/fYH9FRQUKCgr0xxAREclI+iBdrVmzZggODsa1a9eQkpKCp556CmFhYQgKCkJqaqr+OJ1Oh4yMDERFRQEAoqKiUFhYiMzMTP0xu3btQlVVFXr27Gn18yAiIjKW1L27ASAlJQVCCNx33304d+4cpkyZAk9PT/z4449wd3fHwoULsWDBAqxZswZhYWGYMWMGjh07hpMnT+o7nA0YMAB5eXlYsWIFysvLMXLkSPTo0QPr1q0zKg06nQ5qtZq9u4mIyKrcbJ2Ahmi1WkyfPh2//vorWrZsibi4OMybNw/u7u4AgNdeew3Xr1/H2LFjUVhYiL59+2L79u0GPcLXrl2LpKQkREdHw8XFBXFxcViyZImtTomIiMgo0pekZcCSNBER2YLdtEkTERE5GwZpIiIiSTFIExERSYpBmoiISFIM0kRERJJikCYiIpIUgzQREZGkGKSJiIgkxSBNREQkKQZpIiIiSTFIExERSYpBmoiISFIM0kRERJJikCYiIpIUgzQREZGkGKSJiIgkxSBNREQkKQZpIiIiSTFIExERSYpBmoiISFIM0kRERJJikCYiIpIUgzQREZGkGKSJiIgkxSBNREQkKQZpIiIiSTFIExERSYpBmoiISFIM0kRERJJikCYiIpIUgzQREZGkGKSJiIgkxSBNREQkKQZpIiIiSTFIExERSYpBmoiISFIM0kRERJJikCYiIpIUgzQREZGkGKSJiIgkxSBNREQkKQZpIiIiSTFIExERSUrqIF1ZWYkZM2YgLCwMXl5eaNeuHebOnQshhP4YIQRmzpyJ4OBgeHl5ISYmBmfPnjV4n4KCAsTHx0OlUsHX1xejR49GcXGxtU+HiIjIJFIH6YULF2L58uX4+OOPcerUKSxcuBCLFi3CRx99pD9m0aJFWLJkCVasWIGMjAw0a9YMsbGxuHnzpv6Y+Ph4ZGdnY8eOHUhOTkZaWhrGjh1ri1MiIiIymkLcXiyVzJNPPonAwEB88skn+m1xcXHw8vLC559/DiEEQkJC8Morr+DVV18FAGi1WgQGBmL16tUYOnQoTp06hYiICBw8eBA9evQAAGzfvh1PPPEEfv31V4SEhDSYDp1OB7VaDa1WC5VKZZmTJSIiuoPUJenevXsjNTUVZ86cAQAcPXoUe/bswYABAwAAFy9ehEajQUxMjP41arUaPXv2RHp6OgAgPT0dvr6++gANADExMXBxcUFGRkatn1taWgqdTmfwICIisjY3WyegPtOmTYNOp0OHDh3g6uqKyspKzJs3D/Hx8QAAjUYDAAgMDDR4XWBgoH6fRqNBQECAwX43Nze0bNlSf8yd5s+fj7feesvcp0NERGQSqUvSX375JdauXYt169bh8OHDWLNmDf7xj39gzZo1Fv3c6dOnQ6vV6h+XLl2y6OcRERHVRuqS9JQpUzBt2jQMHToUANC5c2f88ssvmD9/PhISEhAUFAQAyMvLQ3BwsP51eXl56NKlCwAgKCgI+fn5Bu9bUVGBgoIC/evv5OHhAQ8PDwucERERkfGkLkmXlJTAxcUwia6urqiqqgIAhIWFISgoCKmpqfr9Op0OGRkZiIqKAgBERUWhsLAQmZmZ+mN27dqFqqoq9OzZ0wpnQURE1DhSl6QHDRqEefPmoXXr1ujUqROysrKwePFijBo1CgCgUCgwceJEvP322wgPD0dYWBhmzJiBkJAQDB48GADQsWNHPP744xgzZgxWrFiB8vJyJCUlYejQoUb17CYiIrIVqYdgFRUVYcaMGdiyZQvy8/MREhKCYcOGYebMmVAqlQBuTWYya9YsrFy5EoWFhejbty+WLVuGe++9V/8+BQUFSEpKwrZt2+Di4oK4uDgsWbIEPj4+RqWDQ7CIiKxHW1KGK8Vl0N0sh8rLHf7NlFB7K22dLJuQOkjLgkGaiMg6cgtvYOqmY/jx7BX9tn7h/lgQF4kQXy8bpsw2pG6TJiIi56EtKasRoAEg7ewVTNt0DNqSMhulzHYYpImISApXistqBOhqaWev4EoxgzQREZFN6G6W17u/qIH9johBmoiIpKDydK93f/MG9jsiBmkiIpKCv48S/cL9a93XL9wf/j7O18ObQZqIiKSg9lZiQVxkjUDdL9wfC+MinXIYFodgGYFDsIiIrKd6nHTRzXI093SHv4/zjpOWesYxIiJyPmpv5w3Kd2J1NxERkaQYpImIiCTFIE1ERCQptkk7IE5OT0TkGBikHQwnpycichys7nYgnJyeiMixMEg7EE5OT0TkWBikHQgnpyciciwM0g6Ek9MTETkWBmkHwsnpiYgcC4O0A+Hk9EREjoULbBjB3hbY4OT0RESOgeOkHRAnpycicgys7iYiIpIUgzQREZGkGKSJiIgkxSBNREQkKQZpIiIiSTFIExERSYpBmoiISFIM0kRERJJikCYiIpKUyUH68OHDOH78uP75V199hcGDB+P1119HWRnXKyYiIjIXk4P03//+d5w5cwYAcOHCBQwdOhTe3t7YuHEjXnvtNbMnkIiIyFmZHKTPnDmDLl26AAA2btyIfv36Yd26dVi9ejU2bdpk7vQRERE5LZODtBACVVVVAICdO3fiiSeeAACEhobiypUr5k0dERGREzM5SPfo0QNvv/02PvvsM+zevRsDBw4EAFy8eBGBgYFmTyAREZGzMjlIf/DBBzh8+DCSkpLwxhtvoH379gCA//73v+jdu7fZE0hEROSsFEIIYY43unnzJlxdXeHu7m6Ot5OKTqeDWq2GVquFSqWydXKIiMhJuJnrjTw9Pc31VkRERAQjg3SLFi2gUCiMesOCgoImJYiIiIhuMSpIf/DBB/q/r169irfffhuxsbGIiooCAKSnpyMlJQUzZsywSCKJiIickclt0nFxcXj44YeRlJRksP3jjz/Gzp07sXXrVnOmTwpskyYiIlswOUj7+PjgyJEj+l7d1c6dO4cuXbqguLjYrAmUAYM0ERHZgslDsPz8/PDVV1/V2P7VV1/Bz8/PLIkiIiKiRvTufuutt/B///d/+OGHH9CzZ08AQEZGBrZv345//etfZk8gERGRszK5JP3CCy9g7969UKlU2Lx5MzZv3gyVSoU9e/bghRdeMHsC77nnHigUihqPxMREALfGZycmJsLPzw8+Pj6Ii4tDXl6ewXvk5ORg4MCB8Pb2RkBAAKZMmYKKigqzp5WIiMicTCpJl5eX4+9//ztmzJiBtWvXWipNBg4ePIjKykr98xMnTuDRRx/Fs88+CwCYNGkSvvnmG2zcuBFqtRpJSUkYMmQI9u7dCwCorKzEwIEDERQUhH379uHy5csYMWIE3N3d8c4771jlHIiIiBrD5I5jarUaR44cQVhYmKXSVK+JEyciOTkZZ8+ehU6nQ6tWrbBu3To888wzAICffvoJHTt2RHp6Onr16oVvv/0WTz75JHJzc/Vzi69YsQJTp07F77//DqVSWeMzSktLUVpaqn+u0+kQGhrKjmNERGRVJld3Dx482GbDrMrKyvD5559j1KhRUCgUyMzMRHl5OWJiYvTHdOjQAa1bt0Z6ejqAW2O4O3fubLD4R2xsLHQ6HbKzs2v9nPnz50OtVusfoaGhlj0xIiKiWpjccSw8PBxz5szB3r170b17dzRr1sxg/8svv2y2xN1p69atKCws1Ld9azQaKJVK+Pr6GhwXGBgIjUajP+bO1bmqn1cfc6fp06dj8uTJ+ufVJWkiapi2pAxXisugu1kOlZc7/JspofauWWNFRA0zOUh/8skn8PX1RWZmJjIzMw32KRQKiwbpTz75BAMGDEBISIjFPgMAPDw84OHhYdHPIHJEuYU3MHXTMfx49o+15fuF+2NBXCRCfL1smDIi+2RykL548aIl0tGgX375BTt37sTmzZv124KCglBWVobCwkKD0nReXh6CgoL0xxw4cMDgvap7f1cfQ0RNpy0pqxGgASDt7BVM23QMHw3ryhI1kYlMbpO+nRACZlrpskGrVq1CQEAABg4cqN/WvXt3uLu7IzU1Vb/t9OnTyMnJ0c8rHhUVhePHjyM/P19/zI4dO6BSqRAREWGVtBM5gyvFZTUCdLW0s1dwpbjMyikisn+NCtL/+c9/0LlzZ3h5ecHLywuRkZH47LPPzJ02vaqqKqxatQoJCQlwc/uj8K9WqzF69GhMnjwZ33//PTIzMzFy5EhERUWhV69eAIDHHnsMERERGD58OI4ePYqUlBS8+eabSExMZJU2kRnpbpbXu7+ogf1EtqYtKcP5/GJk5VzD+d+LoS2x/Y2lydXdixcvxowZM5CUlIQ+ffoAAPbs2YMXX3wRV65cwaRJk8yeyJ07dyInJwejRo2qse/999+Hi4sL4uLiUFpaitjYWCxbtky/39XVFcnJyRg3bhyioqLQrFkzJCQkYM6cOWZPJ5EzU3m617u/eQP7iWxJ1v4UJo+TDgsLw1tvvYURI0YYbF+zZg1mz55tszZrS+ICG0QN05aUYfz6LKTVUuXdL9yfbdIkLW1JGZLWZ9XaXGPr/12Tq7svX76M3r1719jeu3dvXL582SyJIiL7o/ZWYkFcJPqF+xts7xfuj4VxkQzQJIXaqrRl7k9hcnV3+/bt8eWXX+L111832P7FF18gPDzcbAkjIvsT4uuFj4Z1xZXiMhTdLEdzT3f4+3CcNMmhrirtl6Prj1227E/RqFWwnnvuOaSlpenbpPfu3YvU1FR8+eWXZk8gEdkXtTeDMsmnviGCL/ZvV+9rbdmfwuTq7ri4OGRkZMDf3x9bt27F1q1b4e/vjwMHDuDpp5+2RBqJiIiapL4q7X0XruKhO5ppqvUL94e/j+1uOk3uOOaM2HGMiMi+ZeVcw9PL9tW6z1vpim3j++Ktr7MNOj5W96cItmHvbpOru0eMGIGHH34Y/fv3R9u2bS2RJiIiIrOqb4hgSVklFICU/SlMru5WKpWYP38+2rdvj9DQUDz//PP497//jbNnz1oifURERE3m76OsMfKgWr9wf/j9/4Vg2gX4oEvrFmgX4GPzAA00obr7t99+Q1paGnbv3o3du3fjzJkzCA4Oxq+//mruNNocq7uJiOxfbuENTNt0TLoq7fqYXN1drUWLFvDz80OLFi3g6+sLNzc3tGrVypxpIyIiMht7HCJockn69ddfxw8//ICsrCx07NgR/fv3x5///Gf069cPLVq0sFQ6bYolaSIisgWTg7SLiwtatWqFSZMmYciQIbj33nstlTZpMEgTEZEtmFzdnZWVhd27d+OHH37Ae++9B6VSqS9N//nPf3aKoE1ERGQNTR4nffToUbz//vtYu3YtqqqqUFlZaa60SYMlaSJD1fMd626WQ+XlDv9mcrfrEdkrk0vSQghkZWXhhx9+wA8//IA9e/ZAp9MhMjIS/fv3t0QaiUgisi7pR43DGy65mRykW7ZsieLiYjzwwAPo378/xowZg4ceegi+vr4WSB4RyeTO+Y+9la4Y1TcMXUN9ceqyDtdLKxDQ3IMXeTvBGy75mVzd/c033+Chhx5yqmpfVncT3XI+vxjRi3cDuBWglwzrilV7L2Lvuav6Y3iRtw8yr6FMfzB5xrGBAwcyUBE5Kd1tS/aN6htWI0ADt1YVmrbpGLQltluDlxpmjjWUa1ubmcyr0ZOZEJHzuX3+466hvvh417laj6u+yLMkJi9dA2skN7SGMqvKrcPkkjQROa/b5z8uraiq99iGLvJkW/UtOAHUv4ZyfWszsxbFvBikichoam8lFsRFol+4Pzzc6r981HeRJ9traMGJ+tZQNkdVORnHqCDdrVs3XLt2DQAwZ84clJSUWDRRRCSv6vmP27fywUONvMiT7d1+w3W76gUn6muqaGpVORnPqN7dXl5eOHv2LO6++264urri8uXLCAgIsEb6pMDe3US1s8dVhchQ9ThpUxacuL2Xf21SJ/dHuwAfcyfVKRnVcaxLly4YOXIk+vbtCyEE/vGPf8DHp/YvYObMmWZNIBHJyx5XFSJDam/Tv6/qqvK0OoZvsRbFfIwqSZ8+fRqzZs3C+fPncfjwYURERMDNrWZ8VygUOHz4sEUSakssSRMRGWItinU0ahUsjUbD6m4iIifXmKpyMo3J46SrquofdkFERM6hMVXlZJpGTWZy/vx5fPDBBzh16hQAICIiAhMmTEC7du3MmjgiIiJnZvI46ZSUFERERODAgQOIjIxEZGQkMjIy0KlTJ+zYscMSaSQiInJKJrdJd+3aFbGxsViwYIHB9mnTpuG7775jxzEiIiIzMTlIe3p64vjx4wgPDzfYfubMGURGRuLmzZtmTaAMGKSJiMgWTK7ubtWqFY4cOVJj+5EjR5yqxzcREZGlmdxxbMyYMRg7diwuXLiA3r17AwD27t2LhQsXYvLkyWZPIBGZR/VwGd3Ncqi83OHfjD1ziWRncnW3EAIffPAB3nvvPeTm5gIAQkJCMGXKFLz88stQKBQWSagtsbqb7B2XFSSyTyYH6dsVFRUBAJo3b262BMmIQZrsmbakDEnrs2pdtahfuD8+GtaVJWoiSTVqnHQ1Rw/ORI7AmGUFGaSJ5MT1pIkcHJcVJLJfDNJEDk7l6V7v/uYN7Cci22GQJnJw1csK1obLChLJzaQgXV5ejujoaJw9e9ZS6SEyK21JGc7nFyMr5xrO/14MbUmZrZNkdWpvJRbERdYI1NXLCrI9mkheJnUcc3d3x7FjxyyVFiKz4rCjP4T4euGjYV25rCA1iOPp5WLyEKxJkybBw8OjxtzdjoxDsOwPhx0RmY43tvIxeQhWRUUFPv30U+zcuRPdu3dHs2bNDPYvXrzYbIkjaiwOOyIyjbakrEaABm79XqZtOsYbWxsxOUifOHEC3bp1A3BrUY3bOeJsY2SfOOyIyDS8sZWTyUH6+++/t0Q6yMFZu52Lw46ITMMbWzk1egjWuXPnkJKSghs3bgC4Nae3Jfz22294/vnn4efnBy8vL3Tu3BmHDh3S7xdCYObMmQgODoaXlxdiYmJq9D4vKChAfHw8VCoVfH19MXr0aBQXF1skvVRTbuENJK3PQvTi3Xh62T5Ev7cb49dnIbfwhsU+k8OOiEzDG1s5mRykr169iujoaNx777144okncPnyZQDA6NGj8corr5g1cdeuXUOfPn3g7u6Ob7/9FidPnsR7772HFi1a6I9ZtGgRlixZghUrViAjIwPNmjVDbGyswbrW8fHxyM7Oxo4dO5CcnIy0tDSMHTvWrGml2jXUzmWpIVEcdkRkGt7Yysnk3t0jRoxAfn4+/v3vf6Njx444evQo2rZti5SUFEyePBnZ2dlmS9y0adOwd+9e/Pjjj7XuF0IgJCQEr7zyCl599VUAgFarRWBgIFavXo2hQ4fi1KlTiIiIwMGDB9GjRw8AwPbt2/HEE0/g119/RUhISI33LS0tRWlpqf65TqdDaGgoe3eboLp6u7SiEk8s2VPncamT+6NdgI/F08FhR0QNyy28gWmbjiHtjt7dC+MiEcze3TZhcpv0d999h5SUFNx9990G28PDw/HLL7+YLWEA8PXXXyM2NhbPPvssdu/ejbvuugsvvfQSxowZAwC4ePEiNBoNYmJi9K9Rq9Xo2bMn0tPTMXToUKSnp8PX11cfoAEgJiYGLi4uyMjIwNNPP13jc+fPn4+33nrLrOfiTG4fxrEsvlu9x1q6nUvtzaBMZCyOp5ePydXd169fh7e3d43tBQUF8PDwMEuiql24cAHLly9HeHg4UlJSMG7cOLz88stYs2YNAECj0QAAAgMDDV4XGBio36fRaBAQEGCw383NDS1bttQfc6fp06dDq9XqH5cuXTLreTmyO6u3Pdzq/xdjOxc5E3uYAU/trUS7AB90ad0C7QJ8GKBtzOSS9EMPPYT//Oc/mDt3LoBbw66qqqqwaNEiPPzww2ZNXFVVFXr06IF33nkHANC1a1ecOHECK1asQEJCglk/63YeHh5mv+FwFncO48i6VIg+7f2w99zVGseynYucCScKsQ5HmzHN5CC9aNEiREdH49ChQygrK8Nrr72G7OxsFBQUYO/evWZNXHBwMCIiIgy2dezYEZs2bQIABAUFAQDy8vIQHBysPyYvLw9dunTRH5Ofn2/wHhUVFSgoKNC/nsznzmEcn+65iCXDugKAQaBmBy5yJpwoxHKqg3JxaTnUXkrM2HoCP55znBshk4P0/fffjzNnzuDjjz9G8+bNUVxcjCFDhiAxMdEgUJpDnz59cPr0aYNtZ86cQZs2bQAAYWFhCAoKQmpqqj4o63Q6ZGRkYNy4cQCAqKgoFBYWIjMzE927dwcA7Nq1C1VVVejZs6dZ00s1h3GUlFXi5fVZGNU3DKP6hEHt5Y4W3kq2c1mBo5Uo7BknCrGM22snkh5pj6ycazVq7ez9RsjkIA3c6pz1xhtvmDstNUyaNAm9e/fGO++8g7/+9a84cOAAVq5ciZUrVwK4VdU+ceJEvP322wgPD0dYWBhmzJiBkJAQDB48GMCtkvfjjz+OMWPGYMWKFSgvL0dSUhKGDh1aa89uaprqYRy39w4tKavEx7vOcc5sK2LVqlzMMVEIb7oM3Vk70TXUFx/vOlfrsfZ8I9SoIH3t2jV88sknOHXqFAAgIiICI0eORMuWLc2auAcffBBbtmzB9OnTMWfOHISFheGDDz5AfHy8/pjXXnsN169fx9ixY1FYWIi+ffti+/bt8PT01B+zdu1aJCUlITo6Gi4uLoiLi8OSJUvMmla69aO5er0Ms/7SCbO/zq4RIFi9bR2sWpVPUycK4U1XTXfWTpRWVNV7vL3OmGbyOOm0tDQMGjQIarVaP6wpMzMThYWF2LZtG/r162eRhNoSV8Fq2O0XEW+lK0b1DUPvtn7wcHeBrxert63pfH4xohfvrnO/pcemU03akjKMX59lUMNUraEaJq7oVrusnGt4etk+/fNPEnpg9JpDdR5vr//3Jg/BSkxMxHPPPYeLFy9i8+bN2Lx5My5cuIChQ4ciMTHREmkkyd1Zcquu3v7bvzPw4c6zDNBWxjmY5dOUGfCMac++kz0M9WqqO2snqkeS1MaeR5KYXN197tw5/Pe//4Wrq6t+m6urKyZPnoz//Oc/Zk0c2Qdn6RRjL22CnINZTo2dKMTUmy5nqRq/s/+Lo44kMTlId+vWDadOncJ9991nsP3UqVN44IEHzJYwsh/OUHKzpwtfbZ33qtlzicIRNGYGPFNuupypP0J17UT1NKbVI0lmPBmBmU9G4EZZpUPMmGZUkD527Jj+75dffhkTJkzAuXPn0KtXLwDA/v37sXTpUixYsMAyqSSpOXrJzd4ufHdevKrZe4nCWZly0+UstVrVnGEaU6M6jrm4uEChUDS4HKVCoUBlZaXZEicLdhyrX1M6xdgDe+2IxcVFHIexC1/c2ZnqTltf6o0urVvUuZ/kY1RJ+uLFi5ZOB9kxRy+52Wt1PhcXcRzGlhgdvVbLGRkVpKtn+CKqiyNXO/HCRzIw5qbLWfoj2EsnTnNo1GQmubm52LNnD/Lz81FVZTiA/OWXXzZLwkg+Df0wHLXk5iwXPrJ/MtVqWSqQ2lMnTnMweTKT1atX4+9//zuUSiX8/PygUCj+eDOFAhcuXDB7Im2NbdLO98O4k7FtgkQysHV/BEtdL5xxYheTg3RoaChefPFFTJ8+HS4uJs+FYpecPUg74w+jNra+8NkrR6uadLTzMTdLXi/stRNnU5hc3V1SUoKhQ4c6TYAmywzrsMcLnaNW51uSo9XAONr5WIIlh4HZayfOpjA50o4ePRobN260RFpIUub+YeQW3kDS+ixEL96Np5ftQ/R7uzF+fRZyC280JZkkmYbGl9vbVJWOdj6WYslA6oydOE0uSc+fPx9PPvkktm/fjs6dO8Pd3TBTFi9ebLbEkRzM+cOwt4lBqPEcbWINRzsfS7FkIHXGTpyNCtIpKSn6aUHv7DhGjsecPwxzXOjssapcdpbIU0ermnS087EUSwZSmXqvW4vJQfq9997Dp59+ihdeeMECySEZmfOH0dQLnTnbBBnsb7FUO6ujVU062vlYiqUDqSPPyVAbk4O0h4cH+vTpY4m0kMTM9cNoyoXOnFXl7AB0iyWbHxytatLRzseSLB1InakTp8kdxyZMmICPPvrIEmkhyam9lWgX4IMurVugXYBPo34k1Re62jR0oWvMurq1YQegP5grT2vTlDWUZeRo52Np5rheUCNK0gcOHMCuXbuQnJyMTp061eg4tnnzZrMljhxPU6rCzNUm6MgdgEytwrd0O6ujVU02dD7O3oTi7OdvCSYHaV9fXwwZMsQSaSEn0dgLt7naBB21A1BjqvCt0c7qaFWTdZ2PszehOPv5W4rJQXrVqlWWSAc5mcZcuM3VJuiIHYAa27bMdlbzcPahhc5+/pbEacPIbpirTbAp7eKyamzbMttZzcOSbfv2wNnP35JMLkmHhYXVOx7aERfYIHmYo43TEcdaNqUK39HajW3BUZtQjOXs529JJgfpiRMnGjwvLy9HVlYWtm/fjilTppgrXUR1Mkcbp6MFpqZW4Ttau7G1OWITiimc/fwtyeQgPWHChFq3L126FIcOHWpygsg52aJXqCMFJrYt25az57+9nr899EY3eanKuly4cAFdunSBTqczx9tJxZmXqrTGPzF7hZoH17y2LWfPf3s7f3u57pgtSC9atAjLli3Dzz//bI63k4qzBmlr/BNzrWrz4prXtuXs+W8v59/Y644tSt4mV3d37drVoOOYEAIajQa///47li1bZtbEke1Ya0iFI08sYguOVIVvj5w9/+3l/Btz3bFVydvkID148GCD5y4uLmjVqhX+/Oc/o0OHDuZKF9mYtYIne4USyc0e2m1NZep1x5bjwE0O0rNmzbJEOkgy1gqe7BVKJC97abc1lanXHVvW+HEyE6qVtYKnI04sQuQIHHkhGlOvO7as8TM6SLu4uMDV1bXeh5ubyQVzsgJtSRnO5xcjK+cazv9ebNSPy1rBkzNeEcnJkWcRM/W6Y8saP6Oj6pYtW+rcl56ejiVLlqCqqsosiSLzaWx1lTVn5eLKQkTycfT+IqZMaGTLceBNGoJ1+vRpTJs2Ddu2bUN8fDzmzJmDNm3amDN9UrDXIVjGDDMAUG8AtPWQClNvMhjQ5cfvyD6czy9G9OLdde5Pndwf7QJ8rJgi27LVOPBG1U/n5uZi1qxZWLNmDWJjY3HkyBHcf//95k4bNVF91VWHfrmGayXlmPHViXoDoC2HVJjao9JRO7k4En5H9sNeZxGzFFtNJWxSxzGtVoupU6eiffv2yM7ORmpqKrZt28YALan6qqtG9Q3DjK3Hpe4UYkqbmCN3cnEU/I7sC/uL1KT2VqJdgA+6tG6BdgE+VskDo0vSixYtwsKFCxEUFIT169fjqaeesmS6yAzq6+zQNdQXH+86V+s+WSYRMaVNjJOiyI/fkf1xtIVo7JHRQXratGnw8vJC+/btsWbNGqxZs6bW4zZv3my2xFHT1FddVRtvpStG9Q1D11BfXL1eBvxebNP2QlN6VDp6Jxdrs0S7Mb8j+2Qvs4g5KqOD9IgRI+pdR5rkU18P7btbGLb/eStdsWRYV6zae9GghG3L9kJT2sQ4KYr5WKrdmN8RkenMtsCGI7PX3t3VauuhDQDj12fpA2DSI+2RlXMNe89drfF6Wy50YWyPSm1JmcH53I4LdRjPkgue8DsiMh2DtBHsPUjX5fYA+ElCD4xeU/d64LYcbmHsMDB7WypPRpYedsPviMg0nCLMid3eKeTq9fp71tqyvbChNrHqIF5cWo65g+9HWUUVrpdWsJNLI1i63ZgdkYhMwyDt5PQBML+43uNkbS/kuFvzska7MTsiGYeTvhDAIE3/nz1OXGDL5eMcVW3/B9W9/nu39YP2RhnO27jXvzPgzSdVk3oVrNmzZ0OhUBg8bl+z+ubNm0hMTISfnx98fHwQFxeHvLw8g/fIycnBwIED4e3tjYCAAEyZMgUVFRXWPhXp2ePEBY68AICt3Pl/UN3rPyvnGv727wwMWZ6O6Pd2Y/z6LOQW3rBxah0TJ32h20lfku7UqRN27typf377SluTJk3CN998g40bN0KtViMpKQlDhgzB3r17AQCVlZUYOHAggoKCsG/fPly+fBkjRoyAu7s73nnnHaufi+zsrb2Q424t4/b/gyohMGdbdo1e/6ytsBxO+kK3kz5Iu7m5ISgoqMZ2rVaLTz75BOvWrcMjjzwCAFi1ahU6duyI/fv3o1evXvjuu+9w8uRJ7Ny5E4GBgejSpQvmzp2LqVOnYvbs2VAq+Y9+J3tqL+S4W8up/j84n1+MH2sZlgcwYFgKbz7pdlJXdwPA2bNnERISgrZt2yI+Ph45OTkAgMzMTJSXlyMmJkZ/bIcOHdC6dWukp6cDuLWEZufOnREYGKg/JjY2FjqdDtnZ2XV+ZmlpKXQ6ncGD5GOtNa+dGQOG9fHmk24ndZDu2bMnVq9eje3bt2P58uW4ePEiHnroIRQVFUGj0UCpVMLX19fgNYGBgdBoNAAAjUZjEKCr91fvq8v8+fOhVqv1j9DQUPOeGJmFPbaj2xsGDOvjzSfdTurq7gEDBuj/joyMRM+ePdGmTRt8+eWX8PKyXA/H6dOnY/LkyfrnOp2OgVpS9taObm/ssde/vatvOl9Huvm01hAzex/KJnWQvpOvry/uvfdenDt3Do8++ijKyspQWFhoUJrOy8vTt2EHBQXhwIEDBu9R3fu7tnbuah4eHvDw8DD/CZBF2FM7ur1xloAhG0e/+bTWELOmfo4MAd6ugnRxcTHOnz+P4cOHo3v37nB3d0dqairi4uIAAKdPn0ZOTg6ioqIAAFFRUZg3bx7y8/MREBAAANixYwdUKhUiIiJsdh5kH+r7gcrw47UWRw8YsnLUm09rzW/Q1M+RZay61EH61VdfxaBBg9CmTRvk5uZi1qxZcHV1xbBhw6BWqzF69GhMnjwZLVu2hEqlwvjx4xEVFYVevXoBAB577DFERERg+PDhWLRoETQaDd58800kJiaypEz1qusHujAuEgKQ4sdrTY4aMMj6rDXErCmfI9NESVIH6V9//RXDhg3D1atX0apVK/Tt2xf79+9Hq1atAADvv/8+XFxcEBcXh9LSUsTGxmLZsmX617u6uiI5ORnjxo1DVFQUmjVrhoSEBMyZM8dWp9RozlRys7X6fqA/nPkd/zt2GT+eM5yRKzLUFz9fuQ6N9gbU3kp+P0R1sNaIgaZ8jkxj1aUO0hs2bKh3v6enJ5YuXYqlS5fWeUybNm3wv//9z9xJsypZql2cRX0/0IDmHjUCtGzrcJN5sYOTeVlrxEBTPkemoYdSB2mSq9rFWdT3Ay2tqDJ4PqpvGFbtvcgZuRyUrB2c7DmgW2vEQFM+R6ahh1KPkybOT20L9f1APdwMfzJdQ31rBOhq/H7sm7Xm0Db1c3ILbyBpfRaiF+/G08v22d1c6taa36ApnyPTWHWWpCUnU7WLs6jvDjy/qNRg350l6zvx+7FfMnZwcpSaNWuNGGjs58g09JBBWnIyVbs4i/p+oA/f2wr9722l33dnyfpO/H7sl4wdnGTq0NRU1hox0NjPkWXoIYO05Djjk2009AO9fZWoh8L9a71w8vuxbzJ2cGLNmnXJMPSQbdKS4/zU5qUtKcP5/GJk5VzD+d+L621XVHsr0S7AB11at0C7AB+DvK7eFx7YHAv5/diEKd9lY1irXdKUz2HNmvNRCCGErRMhO51OB7VaDa1WC5VKZZM0VPfm5IxPjWfJnrr8fqzLmr2u62qXDLbB52hLyjB+fZbBcd5KV4zqG4bebf3g6e7CcfoOhkHaCDIEaWoabUkZktZn1VktbS8dbsj636W1bsCM/ZzbA/rt4/RvH2XAcfqOg23S5BQcqcONs7P2dylbB6fb+0tUCYE527I5Tt+BsU2anAI73DgOfpd/9IlwUSjwI8fpOzQGaXIK7HDjOPhd/oE3LI6PQZqcgkwzCFHT8Lv8A29YHB+DNDkFDmWzf9VDrn6+eh1znrqf3yV4w+IM2LvbCOzd7Tg4VMo+3TnkylvpihlPRqBba1/cKKt06u/SWsPEyDYYpI0ge5C25xVxiBoi+/A5GX5/vPl0XByCZee41jQ5OpmHz8ny+5Nh+kqyDLZJ2zFrLaVHZEuy9mDm74+sgSVpSRlThdaYEoYMVXNEppC1B7PMJXwZ8drTOAzSEjK2Cs3UEoYsVXNEppB1JThZS/gy4rWn8VjdLRlTqtBMKWGwao7slazD52Qt4dfG0iuGNfTZvPY0HkvSkjGlCs2UEgar5uTBaj/TNbS+ty3IWsK/k61Lsbz2NA1L0pIxpQrNlBIGq+bkkFt4A0nrsxC9eDeeXrYP0e/txvj1WcgtvGHrpEmvvvW9bZUeGUv4t5OhFMtrT9OwJC0ZU6vQjC1h2FPVnKNq6IJp7fG+MpToZUhDU8hYwr+dDKVYXnuahkFaMsZUodV2YWsX4NPk9yXLssQFs7FBztZVoLKkwRxkHqMsQynW2GuPvd+wWQqDtGSqq9Bqm+ZvUVwkrpdVNurCVt/7ylI15+jMfcFsbJCToUQvQxqcgQylWGOuPY5yw2YJDNISqqsKDUCt0yMae2GTvWrO0ZnzgtmUICdDFagMaXAGstSg1Xft4Q1b/RikJVVbFdr5/OImX9hkrppzdOa8YDYlyMlQBSpDGpyBTDVodV17eMNWPwZpO8ILm30z5wWzKf8LMlSBypAGZyF7DRqva/VjkLYjvLDZP3NdMJvyvyBDFait0+BsnZTMVYNmiXzjda1+DNJ2xNYXNjIPc1wwm/K/IEMVaG1puH2N6AtXrkPlVWaR4MlOSo1jqXzjda1+XE/aCDKtJ80F3qlaU/8XZFiDuDoN10vLofJSYsbWE/jxnOWCp+xrU8vK0vnG61rdGKSNIFOQBuS4uJIcHOV/wVrB83x+MaIX765zf+rk/g3OOeCMrJFvjvK/bG6s7rZD7KFN1Rzlf8FaPXzZSalxrJFvjvK/bG4M0kQWYEoHG2frxFQbcwQBY/KRnZQah/lmOwzSRGZmSgcbdmK6palBwNh8ZCelxmG+2Q5XwSIyI1NWHZJhhSJZVAeB2jQUBEzJR3tYuUpGzDfbYUmayIxMaVvlTEt/aMqwMFPzUfbJPWTFfLMNBmkiMzKlbZWdmAw1Ngg0Jh/ZSalxmG/WxyBNZEamtK2yM05NjQkC9pyP7DRIDWGQJjIjUzrYsDOOedhrPrLTIBmDHceIzMiUDjbsjGMe9piP7DRIxuKMY0aQbcYxkp8psydxpiXzsKd85MxnZCxWd1OjsT2tbqa0rd55rLakDOfzi5mvJrKnTk3sNEjGYpCmRmF7mmUwXw056o2gPXd2I+uyqzbpBQsWQKFQYOLEifptN2/eRGJiIvz8/ODj44O4uDjk5eUZvC4nJwcDBw6Et7c3AgICMGXKFFRUVFg59Y6D7WmWwXw1lFt4A0nrsxC9eDeeXrYP0e/txvj1WcgtvGHrpDVZUyZvIediN0H64MGD+Oc//4nIyEiD7ZMmTcK2bduwceNG7N69G7m5uRgyZIh+f2VlJQYOHIiysjLs27cPa9aswerVqzFz5kxrn4LDMGbyCDId8/UPjn7DYo+d3cg27KK6u7i4GPHx8fjXv/6Ft99+W79dq9Xik08+wbp16/DII48AAFatWoWOHTti//796NWrF7777jucPHkSO3fuRGBgILp06YK5c+di6tSpmD17NpTKmj+G0tJSlJaW6p/rdDrLn6QdsXR7mqNWcTaE7ZR/cIbZ2DiDFxnDLkrSiYmJGDhwIGJiYgy2Z2Zmory83GB7hw4d0Lp1a6SnpwMA0tPT0blzZwQGBuqPiY2NhU6nQ3Z2dq2fN3/+fKjVav0jNDTUAmdlvyzZnubIVZwNYTvlH5zlhkXtrUS7AB90ad0C7QJ8GKCpBumD9IYNG3D48GHMnz+/xj6NRgOlUglfX1+D7YGBgdBoNPpjbg/Q1fur99Vm+vTp0Gq1+selS5fMcCaOw1LtaY5axVndWzsr5xrO/15c53mwnfIPvGEhukXq6u5Lly5hwoQJ2LFjBzw9Pa32uR4eHvDw8LDa59mbpiyGUB9HrOI0pbe2pfLVHtnrLGJE5iZ1kM7MzER+fj66deum31ZZWYm0tDR8/PHHSElJQVlZGQoLCw1K03l5eQgKCgIABAUF4cCBAwbvW937u/oYMp0l2tNsXcVp7rbwhmoGPhrWtcb7s53yFt6wEN0idZCOjo7G8ePHDbaNHDkSHTp0wNSpUxEaGgp3d3ekpqYiLi4OAHD69Gnk5OQgKioKABAVFYV58+YhPz8fAQEBAIAdO3ZApVIhIiLCuifkYMw9eYQtqzgtMT65sTUD9jQphyXxhoVI8iDdvHlz3H///QbbmjVrBj8/P/320aNHY/LkyWjZsiVUKhXGjx+PqKgo9OrVCwDw2GOPISIiAsOHD8eiRYug0Wjw5ptvIjExkVXakrFVFWdjSrzGMEfNgLP2dK/GGxZydlIHaWO8//77cHFxQVxcHEpLSxEbG4tly5bp97u6uiI5ORnjxo1DVFQUmjVrhoSEBMyZM8eGqaba2KqK01Jt4U2tGeDsY0TEBTaMYK4FNpy9VGQsay+UkJVzDU8v21fn/q0v9UaX1i1Mfl9tSRnGr8+qs2agvhK6tqQMSeuzar15aOi1ROQ47L4kbS9YKjKetas4LdUW3pSaAUfs6U5EpmOQtgJLtXmSeViyLbyxnZ9s3dOdHBdr9OwLg7QVsFQkN0u3hTemZoCTeZAlsEbP/jBIWwFLRfKTbbgPJ/Mgc2ONnn2SflpQR8BSkX2QaR5lrpJE5sZV1uwTS9JWwFIRNYZspXuyb6zRs08sSVsBS0XUWDKV7sm+sUbPPrEkbSUsFRGRLbFGzz5xMhMjmGsyEyIiW8otvFHnKIZg9u6WEoO0ERikichRWHtGP2oaVncTETkRLlpiX9hxjIiISFIM0kRERJJidbckOJ8uERHdiUFaApxPl8iyeBNM9oq9u41gyd7dXDeYyLJ4E0z2jG3SNsb5dIksp6FFJbQl/H2R3BikbYzz6RJZDm+Cyd4xSNsY59MlshzeBJO9Y5C2ser5dGvD+XSJmoY3wWTvGKRtjCtkEVkOb4LJ3rF3txGsMXc359MlsgxTF5XgcC2SCYO0EbjABpF9M/YmmMO1SDYM0kZgkCZyfJyzgGTEGceIiGDccC0Gadtw5iYIBmkiInC4lqycvQmCvbuJiMDhWjLijHEM0kR2RVtShvP5xcjKuYbzvxc7xUXKWjhcSz6cMY7V3UR2w9mr/Sytes6CuoZrOUsbqEzYBMEgTWQXGqr2Y89j8wjx9cJHw7pyzgJJsAmCQZrILrDnsfWovRmUZVHdBJFWx7A4Z2iCYJs0kR1gtR85I06bzJI0kV1gtR85K2dvgmCQJrIDrPYznTNPgOFonLkJgtOCGoHTgpIMTF0owpmxJzw5CgZpIzBIkyy4WlrDOAc3ORJWdxPZEWeu9jMWe8KTI2HvbiJyKOwJT46EQZqIHAp7wpMjYZAmIofCObjJkTBIE5FD4QQY5EjYu9sI7N1NZH/YE54cAXt3E5FDYk94cgSs7iYiIpKU1EF6+fLliIyMhEqlgkqlQlRUFL799lv9/ps3byIxMRF+fn7w8fFBXFwc8vLyDN4jJycHAwcOhLe3NwICAjBlyhRUVFRY+1SIiIhMJnWQvvvuu7FgwQJkZmbi0KFDeOSRR/DUU08hOzsbADBp0iRs27YNGzduxO7du5Gbm4shQ4boX19ZWYmBAweirKwM+/btw5o1a7B69WrMnDnTVqdERERkNLvrONayZUu8++67eOaZZ9CqVSusW7cOzzzzDADgp59+QseOHZGeno5evXrh22+/xZNPPonc3FwEBgYCAFasWIGpU6fi999/h1JZe3tVaWkpSktL9c91Oh1CQ0PZcYyIiKxK6pL07SorK7FhwwZcv34dUVFRyMzMRHl5OWJiYvTHdOjQAa1bt0Z6ejoAID09HZ07d9YHaACIjY2FTqfTl8ZrM3/+fKjVav0jNDTUcidGRERUB+mD9PHjx+Hj4wMPDw+8+OKL2LJlCyIiIqDRaKBUKuHr62twfGBgIDQaDQBAo9EYBOjq/dX76jJ9+nRotVr949KlS+Y9KSIiIiNIPwTrvvvuw5EjR6DVavHf//4XCQkJ2L17t0U/08PDAx4eHhb9DCIiooZIH6SVSiXat28PAOjevTsOHjyIDz/8EM899xzKyspQWFhoUJrOy8tDUFAQACAoKAgHDhwweL/q3t/VxxAREclK+uruO1VVVaG0tBTdu3eHu7s7UlNT9ftOnz6NnJwcREVFAQCioqJw/Phx5Ofn64/ZsWMHVCoVIiIirJ52IiIiU0hdkp4+fToGDBiA1q1bo6ioCOvWrcMPP/yAlJQUqNVqjB49GpMnT0bLli2hUqkwfvx4REVFoVevXgCAxx57DBERERg+fDgWLVoEjUaDN998E4mJiazOJiIi6UkdpPPz8zFixAhcvnwZarUakZGRSElJwaOPPgoAeP/99+Hi4oK4uDiUlpYiNjYWy5Yt07/e1dUVycnJGDduHKKiotCsWTMkJCRgzpw5tjolIiIio9ndOGlb4AIbRERkC1KXpB1d9So9upvlUHm5w78ZFwQgIqI/MEjbSG7hDUzddAw/nr2i39Yv3B8L4iIR4utlw5QREZEs7K53tyPQlpTVCNAAkHb2CqZtOgZtSZmNUkZERDJhSdoGrhSX1QjQ1dLOXsGV4jJWexNZEZueSFYM0jagu1le7/6iBvYTkfmw6YlkxupuG1B5ute7v3kD+4nIPNj0RLJjkLYBfx8l+oX717qvX7g//H1YzUZkDcY0PRHZEoO0Dai9lVgQF1kjUPcL98fCuEi2hRFZCZueSHZsk7aREF8vfDSsK64Ul6HoZjmae7rD34edVYisiU1PJDsGaRtSezMoE9lSddNTWi1V3mx6IhmwupuInBabnkh2nLvbCJy7m8ixVY+TZtMTyYbV3UTk9Nj0RLJidTcREZGkGKSJiIgkxSBNREQkKQZpIiIiSTFIExERSYpBmoiISFIM0kRERJJikCYiIpIUgzQREZGkGKSJiIgkxSBNREQkKQZpIiIiSTFIExERSYqrYBmhejVPnU5n45QQEZEjad68ORQKRZ37GaSNUFRUBAAIDQ21cUqIiMiRaLVaqFSqOvcrRHUxkepUVVWF3NzcBu94jKHT6RAaGopLly7V+8U4O+aT8ZhXxmE+GY95ZRxz5BNL0mbg4uKCu+++26zvqVKp+M9vBOaT8ZhXxmE+GY95ZRxL5hM7jhEREUmKQZqIiEhSDNJW5uHhgVmzZsHDw8PWSZEa88l4zCvjMJ+Mx7wyjjXyiR3HiIiIJMWSNBERkaQYpImIiCTFIE1ERCQpBmkiIiJJMUhb0dKlS3HPPffA09MTPXv2xIEDB2ydJJubP38+HnzwQTRv3hwBAQEYPHgwTp8+bXDMzZs3kZiYCD8/P/j4+CAuLg55eXk2SrEcFixYAIVCgYkTJ+q3MZ9u+e233/D888/Dz88PXl5e6Ny5Mw4dOqTfL4TAzJkzERwcDC8vL8TExODs2bM2TLFtVFZWYsaMGQgLC4OXlxfatWuHuXPn4va+xM6aV2lpaRg0aBBCQkKgUCiwdetWg/3G5EtBQQHi4+OhUqng6+uL0aNHo7i42PTECLKKDRs2CKVSKT799FORnZ0txowZI3x9fUVeXp6tk2ZTsbGxYtWqVeLEiRPiyJEj4oknnhCtW7cWxcXF+mNefPFFERoaKlJTU8WhQ4dEr169RO/evW2Yats6cOCAuOeee0RkZKSYMGGCfjvzSYiCggLRpk0b8cILL4iMjAxx4cIFkZKSIs6dO6c/ZsGCBUKtVoutW7eKo0ePir/85S8iLCxM3Lhxw4Ypt7558+YJPz8/kZycLC5evCg2btwofHx8xIcffqg/xlnz6n//+5944403xObNmwUAsWXLFoP9xuTL448/Lh544AGxf/9+8eOPP4r27duLYcOGmZwWBmkr+dOf/iQSExP1zysrK0VISIiYP3++DVMln/z8fAFA7N69WwghRGFhoXB3dxcbN27UH3Pq1CkBQKSnp9sqmTZTVFQkwsPDxY4dO0T//v31QZr5dMvUqVNF375969xfVVUlgoKCxLvvvqvfVlhYKDw8PMT69eutkURpDBw4UIwaNcpg25AhQ0R8fLwQgnlV7c4gbUy+nDx5UgAQBw8e1B/z7bffCoVCIX777TeTPp/V3VZQVlaGzMxMxMTE6Le5uLggJiYG6enpNkyZfLRaLQCgZcuWAIDMzEyUl5cb5F2HDh3QunVrp8y7xMREDBw40CA/AOZTta+//ho9evTAs88+i4CAAHTt2hX/+te/9PsvXrwIjUZjkE9qtRo9e/Z0qnwCgN69eyM1NRVnzpwBABw9ehR79uzBgAEDADCv6mJMvqSnp8PX1xc9evTQHxMTEwMXFxdkZGSY9HlcYMMKrly5gsrKSgQGBhpsDwwMxE8//WSjVMmnqqoKEydORJ8+fXD//fcDADQaDZRKJXx9fQ2ODQwMhEajsUEqbWfDhg04fPgwDh48WGMf8+mWCxcuYPny5Zg8eTJef/11HDx4EC+//DKUSiUSEhL0eVHbb9GZ8gkApk2bBp1Ohw4dOsDV1RWVlZWYN28e4uPjAYB5VQdj8kWj0SAgIMBgv5ubG1q2bGly3jFIkzQSExNx4sQJ7Nmzx9ZJkc6lS5cwYcIE7NixA56enrZOjrSqqqrQo0cPvPPOOwCArl274sSJE1ixYgUSEhJsnDq5fPnll1i7di3WrVuHTp064ciRI5g4cSJCQkKYVxJhdbcV+Pv7w9XVtUZP27y8PAQFBdkoVXJJSkpCcnIyvv/+e4NlQYOCglBWVobCwkKD450t7zIzM5Gfn49u3brBzc0Nbm5u2L17N5YsWQI3NzcEBgYynwAEBwcjIiLCYFvHjh2Rk5MDAPq84G8RmDJlCqZNm4ahQ4eic+fOGD58OCZNmoT58+cDYF7VxZh8CQoKQn5+vsH+iooKFBQUmJx3DNJWoFQq0b17d6Smpuq3VVVVITU1FVFRUTZMme0JIZCUlIQtW7Zg165dCAsLM9jfvXt3uLu7G+Td6dOnkZOT41R5Fx0djePHj+PIkSP6R48ePRAfH6//m/kE9OnTp8YQvjNnzqBNmzYAgLCwMAQFBRnkk06nQ0ZGhlPlEwCUlJTAxcUwBLi6uqKqqgoA86ouxuRLVFQUCgsLkZmZqT9m165dqKqqQs+ePU37wCZ1eyOjbdiwQXh4eIjVq1eLkydPirFjxwpfX1+h0WhsnTSbGjdunFCr1eKHH34Qly9f1j9KSkr0x7z44ouidevWYteuXeLQoUMiKipKREVF2TDVcri9d7cQzCchbg1Pc3NzE/PmzRNnz54Va9euFd7e3uLzzz/XH7NgwQLh6+srvvrqK3Hs2DHx1FNPOcWwojslJCSIu+66Sz8Ea/PmzcLf31+89tpr+mOcNa+KiopEVlaWyMrKEgDE4sWLRVZWlvjll1+EEMbly+OPPy66du0qMjIyxJ49e0R4eDiHYMnuo48+Eq1btxZKpVL86U9/Evv377d1kmwOQK2PVatW6Y+5ceOGeOmll0SLFi2Et7e3ePrpp8Xly5dtl2hJ3BmkmU+3bNu2Tdx///3Cw8NDdOjQQaxcudJgf1VVlZgxY4YIDAwUHh4eIjo6Wpw+fdpGqbUdnU4nJkyYIFq3bi08PT1F27ZtxRtvvCFKS0v1xzhrXn3//fe1XpcSEhKEEMbly9WrV8WwYcOEj4+PUKlUYuTIkaKoqMjktHCpSiIiIkmxTZqIiEhSDNJERESSYpAmIiKSFIM0ERGRpBikiYiIJMUgTUREJCkGaSIiIkkxSBMREUmKQZqI7IJCocDWrVttnQwiq2KQJpKQRqPB+PHj0bZtW3h4eCA0NBSDBg0ymNTfUc2ePRtdunSpsf3y5csYMGCA9RNEZENcT5pIMj///DP69OkDX19fvPvuu+jcuTPKy8uRkpKCxMRE/PTTT7ZOYp3Ky8vh7u5ukfd25uURyXmxJE0kmZdeegkKhQIHDhxAXFwc7r33XnTq1AmTJ0/G/v37AQA5OTl46qmn4OPjA5VKhb/+9a8G69tWl0Y/++wz3HPPPVCr1Rg6dCiKiooAACtXrkRISIh+WcJqTz31FEaNGqV//tVXX6Fbt27w9PRE27Zt8dZbb6GiokK/X6FQYPny5fjLX/6CZs2aYd68ebh27Rri4+PRqlUreHl5ITw8HKtWrdK/ZurUqbj33nvh7e2Ntm3bYsaMGSgvLwcArF69Gm+99RaOHj0KhUIBhUKB1atX6z/r9uru48eP45FHHoGXlxf8/PwwduxYFBcX6/e/8MILGDx4MP7xj38gODgYfn5+SExM1H8WACxbtgzh4eHw9PREYGAgnnnmmcZ+bUSW0fT1QojIXK5evSoUCoV455136jymsrJSdOnSRfTt21ccOnRI7N+/X3Tv3l30799ff8ysWbOEj4+PGDJkiDh+/LhIS0sTQUFB4vXXXxdCCFFQUCCUSqXYuXOnwWffvi0tLU2oVCqxevVqcf78efHdd9+Je+65R8yePVv/GgAiICBAfPrpp+L8+fPil19+EYmJiaJLly7i4MGD4uLFi2LHjh3i66+/1r9m7ty5Yu/eveLixYvi66+/FoGBgWLhwoVCCCFKSkrEK6+8Ijp16lRj2VIAYsuWLUIIIYqLi0VwcLD+/FJTU0VYWJh+lSIhbi3FqFKpxIsvvihOnToltm3bJry9vfWrYh08eFC4urqKdevWiZ9//lkcPnxYfPjhh4341ogsh0GaSCIZGRkCgNi8eXOdx3z33XfC1dVV5OTk6LdlZ2cLAOLAgQNCiFtB2tvbW+h0Ov0xU6ZMET179tQ/f+qpp8SoUaP0z//5z3+KkJAQUVlZKYQQIjo6usbNwmeffSaCg4P1zwGIiRMnGhwzaNAgMXLkSKPP+d133xXdu3fXP581a5Z44IEHahx3e5BeuXKlaNGihSguLtbv/+abb4SLi4t+jfaEhATRpk0bUVFRoT/m2WefFc8995wQQohNmzYJlUplkEdEsmF1N5FEhBErx546dQqhoaEIDQ3Vb4uIiICvry9OnTql33bPPfegefPm+ufBwcHIz8/XP4+Pj8emTZtQWloKAFi7di2GDh0KF5dbl4WjR49izpw58PHx0T/GjBmDy5cvo6SkRP8+PXr0MEjfuHHjsGHDBnTp0gWvvfYa9u3bZ7D/iy++QJ8+fRAUFAQfHx+8+eabyMnJMSZ7DPLggQceQLNmzfTb+vTpg6qqKpw+fVq/rVOnTnB1da01Dx599FG0adMGbdu2xfDhw7F27VqD8yKSAYM0kUTCw8OhUCjM0jnszg5cCoXCoA160KBBEELgm2++waVLl/Djjz8iPj5ev7+4uBhvvfUWjhw5on8cP34cZ8+ehaenp/642wMlAAwYMAC//PILJk2ahNzcXERHR+PVV18FAKSnpyM+Ph5PPPEEkpOTkZWVhTfeeANlZWVNPt/a1JcHzZs3x+HDh7F+/XoEBwdj5syZeOCBB1BYWGiRtBA1BoM0kURatmyJ2NhYLF26FNevX6+xv7CwEB07dsSlS5dw6dIl/faTJ0+isLAQERERRn+Wp6cnhgwZgrVr12L9+vW477770K1bN/3+bt264fTp02jfvn2NR3Vpuy6tWrVCQkICPv/8c3zwwQdYuXIlAGDfvn1o06YN3njjDfTo0QPh4eH45ZdfDF6rVCpRWVlZ7/t37NgRR48eNcijvXv3wsXFBffdd5/ReeDm5oaYmBgsWrQIx44dw88//4xdu3YZ/XoiS+MQLCLJLF26FH369MGf/vQnzJkzB5GRkaioqMCOHTuwfPlynDx5Ep07d0Z8fDw++OADVFRU4KWXXkL//v1rVD03JD4+Hk8++SSys7Px/PPPG+ybOXMmnnzySbRu3RrPPPMMXFxccPToUZw4cQJvv/12ne85c+ZMdO/eHZ06dUJpaSmSk5PRsWNHALdqCnJycrBhwwY8+OCD+Oabb7BlyxaD199zzz24ePEijhw5grvvvhvNmzeHh4dHjXTPmjULCQkJmD17Nn7//XeMHz8ew4cPR2BgoFHnnpycjAsXLqBfv35o0aIF/ve//6GqqsqkIE9kaSxJE0mmbdu2OHz4MB5++GG88soruP/++/Hoo48iNTUVy5cvh0KhwFdffYUWLVqgX79+iImJQdu2bfHFF1+Y/FmPPPIIWrZsidOnT+Nvf/ubwb7Y2FgkJyfju+++w4MPPohevXrh/fffR5s2bep9T6VSienTpyMyMhL9+vWDq6srNmzYAAD4y1/+gkmTJiEpKQldunTBvn37MGPGDIPXx8XF4fHHH8fDDz+MVq1aYf369TU+w9vbGykpKSgoKMCDDz6IZ555BtHR0fj444+NPndfX19s3rwZjzzyCDp27IgVK1Zg/fr16NSpk9HvQWRpCmFMTxUiIiKyOpakiYiIJMUgTUREJCkGaSIiIkkxSBMREUmKQZqIiEhSDNJERESSYpAmIiKSFIM0ERGRpBikiYiIJMUgTUREJCkGaSIiIkn9P9Xvis3jbUBtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lensofc = [len(i.split()) for i in df['conversation']]\n",
    "long_c_plot = sns.relplot(lensofc)\n",
    "long_c_plot.set_ylabels(\"Number of words\", clear_inner=False)\n",
    "long_c_plot.set_xlabels(\"Conversations\", clear_inner=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6f51b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['conversation'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9028358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find conversations with more than 5000 words\n",
    "longs = [df.index[df['conversation'] == i].tolist()[0] for i in df['conversation'] if len(i.split()) > 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78d958cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a3a4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets drop long conversations\n",
    "df.drop(longs, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5110a5",
   "metadata": {},
   "source": [
    "---\n",
    "### Preaper dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca72843a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='A_bad_intent', ylabel='count'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data = df, x= df['A_bad_intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2e46db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='B_bad_intent', ylabel='count'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data = df, x= df['B_bad_intent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13cdb9",
   "metadata": {},
   "source": [
    "#### balance the dataset or evalue the classification by weighted F1 score???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d0011",
   "metadata": {},
   "source": [
    "+ Normalization /scaling --> no need when using BERT\n",
    "+ Shuffling --> shuffle=True in train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76da635e",
   "metadata": {},
   "source": [
    "---\n",
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e790f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the CPU\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():      \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('using the CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715eaf4",
   "metadata": {},
   "source": [
    "---\n",
    "Classification Model, to label each actor in the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d73bb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@todo use BertForSequenceClassification instead of BertModel?\n",
    "\n",
    "class ParticipantClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name):\n",
    "        super(ParticipantClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        # add a linear layer that maps the hidden state to two outputs, labels\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    " \n",
    "    # input_ids, attention_mask are ouputs of tokenizer\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # pass the input to the model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # [CLS] token representation\n",
    "        logits = self.classifier(pooled_output)\n",
    "        # models confidence score for each calss\n",
    "        logits_a = logits[:, 0]\n",
    "        logits_b = logits[:, 1]\n",
    "        return logits_a, logits_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ccf09",
   "metadata": {},
   "source": [
    "---\n",
    "Tokenize and chunck conversations to segments with less than 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42655e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512, overlap=25): # overlap hardcoded\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.overlap = overlap\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        \"\"\"Tokenize the text into overlapping segments.\"\"\"\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        segments = []\n",
    "        if len(tokens) > self.max_length - 2:  # for [CLS] and [SEP]\n",
    "            start = 0\n",
    "            while start < len(tokens):\n",
    "                end = min(start + self.max_length-2, len(tokens))\n",
    "                segment = tokens[start:end]\n",
    "                #segment = [self.tokenizer.cls_token_id] + segment + [self.tokenizer.sep_token_id] # do I need this?\n",
    "                #pad chunks up to the max length\n",
    "                #padded_segments = segments + [self.tokenizer.pad_token_id] * (self.max_length - len(segments))  # do I need this?\n",
    "                segments.append(segment)              \n",
    "                # update the start and end of the next chunk\n",
    "                start += self.max_length - self.overlap\n",
    "       \n",
    "        else:\n",
    "            segments.append(tokens)\n",
    "        return segments\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['conversation']\n",
    "        segments = self._tokenize(text)\n",
    "        # process each segment\n",
    "        inputs = [self.tokenizer.prepare_for_model(\n",
    "                    seg,\n",
    "                    add_special_tokens=False,  # I add special tokens manually\n",
    "                    max_length=self.max_length,\n",
    "                    padding='max_length',\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True # \n",
    "                  ) for seg in segments]\n",
    "        \n",
    "        label_a = 1 if self.data[idx]['A_bad_intent'] == 1 else 0\n",
    "        label_b = 1 if self.data[idx]['B_bad_intent'] == 1 else 0\n",
    "        \n",
    "        # return a list of segments, each with its own input_ids and attention_mask\n",
    "        return {\n",
    "            'segments': [{\n",
    "                'input_ids': input['input_ids'].squeeze(),\n",
    "                'attention_mask': input['attention_mask'].squeeze()\n",
    "            } for input in inputs],\n",
    "            'A_bad_intent': label_a,\n",
    "            'B_bad_intent': label_b\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5f368",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To ensure that each batch processed by the model has the same shape, collate_fn pads the sequences so that all data in a batch have the same length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe2c87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for handling batches of segmented conversations.\n",
    "    \"\"\"\n",
    "    # lists to store the sequences and labels for all segments across all batch items\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_a_list = []\n",
    "    labels_b_list = []\n",
    " \n",
    "    # list to store the segments for all batch items\n",
    "    batch_segments = []\n",
    " \n",
    "    for item in batch:\n",
    "        # accumulate the labels for each item in the batch\n",
    "        labels_a_list.append(item['A_bad_intent'])\n",
    "        labels_b_list.append(item['B_bad_intent'])\n",
    " \n",
    "        # collect segments from each item in the batch\n",
    "        item_segments = []\n",
    "        for segment in item['segments']:\n",
    "            # for each segment, extract input_ids and attention_mask\n",
    "            input_ids = segment['input_ids']\n",
    "            attention_mask = segment['attention_mask']\n",
    " \n",
    "            # append the segment's input_ids and attention_mask to the respective lists\n",
    "            input_ids_list.append(input_ids)\n",
    "            attention_mask_list.append(attention_mask)\n",
    " \n",
    "            # also collect segments for this item\n",
    "            item_segments.append({\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            })\n",
    " \n",
    "        batch_segments.append(item_segments)\n",
    " \n",
    "    # pad the sequences so that each sequence in the batch has the same length\n",
    "    input_ids_padded = pad_sequence(input_ids_list, batch_first=True, padding_value=0)\n",
    "    attention_mask_padded = pad_sequence(attention_mask_list, batch_first=True, padding_value=0)\n",
    "    # convert labels lists to tensors\n",
    "    labels_a = torch.tensor(labels_a_list)\n",
    "    labels_b = torch.tensor(labels_b_list)\n",
    " \n",
    "    # return the dictionary with padded 'input_ids', 'attention_mask' and 'segments'\n",
    "    return {\n",
    "        'segments': batch_segments,\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_mask_padded,\n",
    "        'A_bad_intent': labels_a,\n",
    "        'B_bad_intent': labels_b\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0061c",
   "metadata": {},
   "source": [
    "---\n",
    "To feed the text to BERT, first it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "335581bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f5c1e296a14c969d06eabb066b04c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9595913c5a314efda0399efe31a7ed62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c00b93bb6364b89926d0d39e6c1e1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e490ca70904bbcafb409bdf967c926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bert-base-uncased model has only lowercase letters!\n",
    "# bert-base-cased: This model is case-sensitive: it makes a difference between english and English.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cca4becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Validation split\n",
    "\n",
    "# shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    " \n",
    "# define split sizes\n",
    "train_size = int(0.7 * len(df)) # reduced train data to 50% to resolve cuda out of memory problem\n",
    "val_size = int(0.15 * len(df))\n",
    " \n",
    "# split the DataFrame\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size + val_size]\n",
    "test_df = df[train_size + val_size:]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02483138",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.to_dict('records')\n",
    "val_data = val_df.to_dict('records')\n",
    "test_data = test_df.to_dict('records')\n",
    "#data\n",
    "# Get the lists of sentences and their labels.\n",
    "#data = df.conversation.values\n",
    "#labels = df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f96c6c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  {'category': 'second participant in conversation is deceiving the first one', 'conversation': \"[CLS]A: Hey, did you hear about that new restaurant that opened up downtown? I've been thinking about checking it out tonight.[SEP]B: Yeah, I've heard about it. It's supposed to be amazing. I actually went there last week.[SEP]A: Really? How was it? What did you have?[SEP]B: The ambiance was fantastic. I had their special, the grilled salmon. It was cooked to perfection. You should totally try it.[SEP]A: That sounds delicious. Was it busy? I'm thinking it might be packed since it's still quite new.[SEP]B: Surprisingly, it wasn't that busy. I guess I went at a good time. You might want to book a table just in case, though.[SEP]A: Good idea. Did you go with anyone I know?[SEP]B: No, I met up with a friend from out of town. Someone you haven't met before.[SEP]A: How's parking around there? I'm not too keen on taking the subway tonight.[SEP]B: Parking is a breeze. There's a lot just a block away that's free after 6 PM. You shouldn't have any trouble.[SEP]A: Perfect. So, anything else I should know about this place? Any dishes to avoid or recommendations?[SEP]B: Well, you might want to steer clear of the pasta dishes. My friend ordered one and it was a bit underwhelming. Stick to the seafood, it's their specialty.[SEP]A: Got it, seafood it is. Did you try any of the desserts?[SEP]B: I did. The chocolate lava cake was divine. You absolutely must get it.[SEP]A: I'm sold. Chocolate lava cake for dessert then. Hey, would you want to join me? It could be fun![SEP]B: I'd love to, but I've got this thing tonight that I can't get out of. Rain check?[SEP]A: Sure, no problem. Maybe next time.[SEP]B: Definitely. Let me know how it goes, though. I want to hear all about it.[SEP]A: Will do. Thanks for the tips![SEP]B: Anytime! Enjoy your evening.[SEP](Later that evening)[SEP]A: Hey, I'm at the restaurant, but there seems to be a problem. They don't have a grilled salmon special, and the waiter said they never had one. Plus, there's no chocolate lava cake on the menu.[SEP]B: Oh, that's odd. Maybe they changed their menu? Places do that all the time.[SEP]A: Maybe, but there's also no free parking lot nearby. The closest one is charging a hefty fee. I'm a bit confused here.[SEP]B: Huh, that's strange. I must have mixed it up with another place. Sorry about that.[SEP]A: And the restaurant said they just had their grand opening two days ago. How did you come here last week?[SEP]B: Last week, two days ago, time flies, you know? I must've got the opening date wrong.[SEP]A: I'm getting the feeling that you've never actually been here, have you?[SEP]B: Look, I'm sorry. I haven't been there. I just wanted to sound knowledgeable. I didn't mean to mislead you. Can you forgive me?[SEP]A: I appreciate the honesty, but this has been a bit of a wild goose chase. Let's just be upfront with each other next time, okay?[SEP]B: Agreed. I really am sorry about this. Let's definitely grab a meal together soon, my treat, to make up for it.[SEP]A: It's okay. I'll hold you to that meal, though. I'm going to try to enjoy the evening regardless. Catch you later.[SEP]B:[SEP]\", 'word_count': 573, 'A_bad_intent': 0.0, 'B_bad_intent': 1.0, 'description': nan}\n",
      "Tokenized:  ['[CLS]', 'A', ':', 'Hey', ',', 'did', 'you', 'hear', 'about', 'that', 'new', 'restaurant', 'that', 'opened', 'up', 'downtown', '?', 'I', \"'\", 've', 'been', 'thinking', 'about', 'checking', 'it', 'out', 'tonight', '.', '[SEP]', 'B', ':', 'Yeah', ',', 'I', \"'\", 've', 'heard', 'about', 'it', '.', 'It', \"'\", 's', 'supposed', 'to', 'be', 'amazing', '.', 'I', 'actually', 'went', 'there', 'last', 'week', '.', '[SEP]', 'A', ':', 'Really', '?', 'How', 'was', 'it', '?', 'What', 'did', 'you', 'have', '?', '[SEP]', 'B', ':', 'The', 'am', '##bian', '##ce', 'was', 'fantastic', '.', 'I', 'had', 'their', 'special', ',', 'the', 'g', '##rille', '##d', 'salmon', '.', 'It', 'was', 'cooked', 'to', 'perfection', '.', 'You', 'should', 'totally', 'try', 'it', '.', '[SEP]', 'A', ':', 'That', 'sounds', 'delicious', '.', 'Was', 'it', 'busy', '?', 'I', \"'\", 'm', 'thinking', 'it', 'might', 'be', 'packed', 'since', 'it', \"'\", 's', 'still', 'quite', 'new', '.', '[SEP]', 'B', ':', 'Sur', '##p', '##rising', '##ly', ',', 'it', 'wasn', \"'\", 't', 'that', 'busy', '.', 'I', 'guess', 'I', 'went', 'at', 'a', 'good', 'time', '.', 'You', 'might', 'want', 'to', 'book', 'a', 'table', 'just', 'in', 'case', ',', 'though', '.', '[SEP]', 'A', ':', 'Good', 'idea', '.', 'Did', 'you', 'go', 'with', 'anyone', 'I', 'know', '?', '[SEP]', 'B', ':', 'No', ',', 'I', 'met', 'up', 'with', 'a', 'friend', 'from', 'out', 'of', 'town', '.', 'Someone', 'you', 'haven', \"'\", 't', 'met', 'before', '.', '[SEP]', 'A', ':', 'How', \"'\", 's', 'parking', 'around', 'there', '?', 'I', \"'\", 'm', 'not', 'too', 'keen', 'on', 'taking', 'the', 'subway', 'tonight', '.', '[SEP]', 'B', ':', 'Park', '##ing', 'is', 'a', 'breeze', '.', 'There', \"'\", 's', 'a', 'lot', 'just', 'a', 'block', 'away', 'that', \"'\", 's', 'free', 'after', '6', 'PM', '.', 'You', 'shouldn', \"'\", 't', 'have', 'any', 'trouble', '.', '[SEP]', 'A', ':', 'Perfect', '.', 'So', ',', 'anything', 'else', 'I', 'should', 'know', 'about', 'this', 'place', '?', 'Any', 'dishes', 'to', 'avoid', 'or', 'recommendations', '?', '[SEP]', 'B', ':', 'Well', ',', 'you', 'might', 'want', 'to', 'steer', 'clear', 'of', 'the', 'past', '##a', 'dishes', '.', 'My', 'friend', 'ordered', 'one', 'and', 'it', 'was', 'a', 'bit', 'under', '##w', '##hel', '##ming', '.', 'Stick', 'to', 'the', 'sea', '##food', ',', 'it', \"'\", 's', 'their', 'specialty', '.', '[SEP]', 'A', ':', 'Got', 'it', ',', 'sea', '##food', 'it', 'is', '.', 'Did', 'you', 'try', 'any', 'of', 'the', 'dessert', '##s', '?', '[SEP]', 'B', ':', 'I', 'did', '.', 'The', 'chocolate', 'lava', 'cake', 'was', 'divine', '.', 'You', 'absolutely', 'must', 'get', 'it', '.', '[SEP]', 'A', ':', 'I', \"'\", 'm', 'sold', '.', 'Chocolate', 'lava', 'cake', 'for', 'dessert', 'then', '.', 'Hey', ',', 'would', 'you', 'want', 'to', 'join', 'me', '?', 'It', 'could', 'be', 'fun', '!', '[SEP]', 'B', ':', 'I', \"'\", 'd', 'love', 'to', ',', 'but', 'I', \"'\", 've', 'got', 'this', 'thing', 'tonight', 'that', 'I', 'can', \"'\", 't', 'get', 'out', 'of', '.', 'Rain', 'check', '?', '[SEP]', 'A', ':', 'Sure', ',', 'no', 'problem', '.', 'Maybe', 'next', 'time', '.', '[SEP]', 'B', ':', 'De', '##fin', '##ite', '##ly', '.', 'Let', 'me', 'know', 'how', 'it', 'goes', ',', 'though', '.', 'I', 'want', 'to', 'hear', 'all', 'about', 'it', '.', '[SEP]', 'A', ':', 'Will', 'do', '.', 'Thanks', 'for', 'the', 'tips', '!', '[SEP]', 'B', ':', 'Any', '##time', '!', 'En', '##joy', 'your', 'evening', '.', '[SEP]', '(', 'Later', 'that', 'evening', ')', '[SEP]', 'A', ':', 'Hey', ',', 'I', \"'\", 'm', 'at', 'the', 'restaurant', ',', 'but', 'there', 'seems', 'to', 'be', 'a', 'problem', '.', 'They', 'don', \"'\", 't', 'have', 'a', 'g', '##rille', '##d', 'salmon', 'special', ',', 'and', 'the', 'waiter', 'said', 'they', 'never', 'had', 'one', '.', 'Plus', ',', 'there', \"'\", 's', 'no', 'chocolate', 'lava', 'cake', 'on', 'the', 'menu', '.', '[SEP]', 'B', ':', 'Oh', ',', 'that', \"'\", 's', 'odd', '.', 'Maybe', 'they', 'changed', 'their', 'menu', '?', 'Places', 'do', 'that', 'all', 'the', 'time', '.', '[SEP]', 'A', ':', 'Maybe', ',', 'but', 'there', \"'\", 's', 'also', 'no', 'free', 'parking', 'lot', 'nearby', '.', 'The', 'closest', 'one', 'is', 'charging', 'a', 'he', '##fty', 'fee', '.', 'I', \"'\", 'm', 'a', 'bit', 'confused', 'here', '.', '[SEP]', 'B', ':', 'Huh', ',', 'that', \"'\", 's', 'strange', '.', 'I', 'must', 'have', 'mixed', 'it', 'up', 'with', 'another', 'place', '.', 'Sorry', 'about', 'that', '.', '[SEP]', 'A', ':', 'And', 'the', 'restaurant', 'said', 'they', 'just', 'had', 'their', 'grand', 'opening', 'two', 'days', 'ago', '.', 'How', 'did', 'you', 'come', 'here', 'last', 'week', '?', '[SEP]', 'B', ':', 'Last', 'week', ',', 'two', 'days', 'ago', ',', 'time', 'flies', ',', 'you', 'know', '?', 'I', 'must', \"'\", 've', 'got', 'the', 'opening', 'date', 'wrong', '.', '[SEP]', 'A', ':', 'I', \"'\", 'm', 'getting', 'the', 'feeling', 'that', 'you', \"'\", 've', 'never', 'actually', 'been', 'here', ',', 'have', 'you', '?', '[SEP]', 'B', ':', 'Look', ',', 'I', \"'\", 'm', 'sorry', '.', 'I', 'haven', \"'\", 't', 'been', 'there', '.', 'I', 'just', 'wanted', 'to', 'sound', 'knowledge', '##able', '.', 'I', 'didn', \"'\", 't', 'mean', 'to', 'mi', '##sle', '##ad', 'you', '.', 'Can', 'you', 'forgive', 'me', '?', '[SEP]', 'A', ':', 'I', 'appreciate', 'the', 'honesty', ',', 'but', 'this', 'has', 'been', 'a', 'bit', 'of', 'a', 'wild', 'goose', 'chase', '.', 'Let', \"'\", 's', 'just', 'be', 'up', '##front', 'with', 'each', 'other', 'next', 'time', ',', 'okay', '?', '[SEP]', 'B', ':', 'A', '##g', '##reed', '.', 'I', 'really', 'am', 'sorry', 'about', 'this', '.', 'Let', \"'\", 's', 'definitely', 'grab', 'a', 'meal', 'together', 'soon', ',', 'my', 'treat', ',', 'to', 'make', 'up', 'for', 'it', '.', '[SEP]', 'A', ':', 'It', \"'\", 's', 'okay', '.', 'I', \"'\", 'll', 'hold', 'you', 'to', 'that', 'meal', ',', 'though', '.', 'I', \"'\", 'm', 'going', 'to', 'try', 'to', 'enjoy', 'the', 'evening', 'regardless', '.', 'Catch', 'you', 'later', '.', '[SEP]', 'B', ':', '[SEP]']\n",
      "Token IDs:  [101, 138, 131, 4403, 117, 1225, 1128, 2100, 1164, 1115, 1207, 4382, 1115, 1533, 1146, 5215, 136, 146, 112, 1396, 1151, 2422, 1164, 9444, 1122, 1149, 3568, 119, 102, 139, 131, 2814, 117, 146, 112, 1396, 1767, 1164, 1122, 119, 1135, 112, 188, 3155, 1106, 1129, 6929, 119, 146, 2140, 1355, 1175, 1314, 1989, 119, 102, 138, 131, 8762, 136, 1731, 1108, 1122, 136, 1327, 1225, 1128, 1138, 136, 102, 139, 131, 1109, 1821, 12324, 2093, 1108, 14820, 119, 146, 1125, 1147, 1957, 117, 1103, 176, 26327, 1181, 17646, 119, 1135, 1108, 13446, 1106, 17900, 119, 1192, 1431, 5733, 2222, 1122, 119, 102, 138, 131, 1337, 3807, 13108, 119, 3982, 1122, 5116, 136, 146, 112, 182, 2422, 1122, 1547, 1129, 8733, 1290, 1122, 112, 188, 1253, 2385, 1207, 119, 102, 139, 131, 17078, 1643, 25443, 1193, 117, 1122, 1445, 112, 189, 1115, 5116, 119, 146, 3319, 146, 1355, 1120, 170, 1363, 1159, 119, 1192, 1547, 1328, 1106, 1520, 170, 1952, 1198, 1107, 1692, 117, 1463, 119, 102, 138, 131, 2750, 1911, 119, 2966, 1128, 1301, 1114, 2256, 146, 1221, 136, 102, 139, 131, 1302, 117, 146, 1899, 1146, 1114, 170, 1910, 1121, 1149, 1104, 1411, 119, 6518, 1128, 3983, 112, 189, 1899, 1196, 119, 102, 138, 131, 1731, 112, 188, 5030, 1213, 1175, 136, 146, 112, 182, 1136, 1315, 11367, 1113, 1781, 1103, 14790, 3568, 119, 102, 139, 131, 1670, 1158, 1110, 170, 10647, 119, 1247, 112, 188, 170, 1974, 1198, 170, 3510, 1283, 1115, 112, 188, 1714, 1170, 127, 14123, 119, 1192, 5380, 112, 189, 1138, 1251, 3819, 119, 102, 138, 131, 12174, 119, 1573, 117, 1625, 1950, 146, 1431, 1221, 1164, 1142, 1282, 136, 6291, 10514, 1106, 3644, 1137, 11859, 136, 102, 139, 131, 2119, 117, 1128, 1547, 1328, 1106, 25284, 2330, 1104, 1103, 1763, 1161, 10514, 119, 1422, 1910, 2802, 1141, 1105, 1122, 1108, 170, 2113, 1223, 2246, 18809, 5031, 119, 24953, 1106, 1103, 2343, 24263, 117, 1122, 112, 188, 1147, 13858, 119, 102, 138, 131, 7348, 1122, 117, 2343, 24263, 1122, 1110, 119, 2966, 1128, 2222, 1251, 1104, 1103, 20392, 1116, 136, 102, 139, 131, 146, 1225, 119, 1109, 8888, 16135, 10851, 1108, 10455, 119, 1192, 7284, 1538, 1243, 1122, 119, 102, 138, 131, 146, 112, 182, 1962, 119, 22011, 16135, 10851, 1111, 20392, 1173, 119, 4403, 117, 1156, 1128, 1328, 1106, 2866, 1143, 136, 1135, 1180, 1129, 4106, 106, 102, 139, 131, 146, 112, 173, 1567, 1106, 117, 1133, 146, 112, 1396, 1400, 1142, 1645, 3568, 1115, 146, 1169, 112, 189, 1243, 1149, 1104, 119, 10463, 4031, 136, 102, 138, 131, 6542, 117, 1185, 2463, 119, 2389, 1397, 1159, 119, 102, 139, 131, 3177, 16598, 3150, 1193, 119, 2421, 1143, 1221, 1293, 1122, 2947, 117, 1463, 119, 146, 1328, 1106, 2100, 1155, 1164, 1122, 119, 102, 138, 131, 3100, 1202, 119, 5749, 1111, 1103, 10538, 106, 102, 139, 131, 6291, 4974, 106, 13832, 18734, 1240, 3440, 119, 102, 113, 2611, 1115, 3440, 114, 102, 138, 131, 4403, 117, 146, 112, 182, 1120, 1103, 4382, 117, 1133, 1175, 3093, 1106, 1129, 170, 2463, 119, 1220, 1274, 112, 189, 1138, 170, 176, 26327, 1181, 17646, 1957, 117, 1105, 1103, 17989, 1163, 1152, 1309, 1125, 1141, 119, 8696, 117, 1175, 112, 188, 1185, 8888, 16135, 10851, 1113, 1103, 13171, 119, 102, 139, 131, 2048, 117, 1115, 112, 188, 5849, 119, 2389, 1152, 2014, 1147, 13171, 136, 5068, 1202, 1115, 1155, 1103, 1159, 119, 102, 138, 131, 2389, 117, 1133, 1175, 112, 188, 1145, 1185, 1714, 5030, 1974, 2721, 119, 1109, 7064, 1141, 1110, 13758, 170, 1119, 27944, 7216, 119, 146, 112, 182, 170, 2113, 4853, 1303, 119, 102, 139, 131, 27158, 117, 1115, 112, 188, 4020, 119, 146, 1538, 1138, 3216, 1122, 1146, 1114, 1330, 1282, 119, 6502, 1164, 1115, 119, 102, 138, 131, 1262, 1103, 4382, 1163, 1152, 1198, 1125, 1147, 5372, 2280, 1160, 1552, 2403, 119, 1731, 1225, 1128, 1435, 1303, 1314, 1989, 136, 102, 139, 131, 4254, 1989, 117, 1160, 1552, 2403, 117, 1159, 10498, 117, 1128, 1221, 136, 146, 1538, 112, 1396, 1400, 1103, 2280, 2236, 2488, 119, 102, 138, 131, 146, 112, 182, 2033, 1103, 2296, 1115, 1128, 112, 1396, 1309, 2140, 1151, 1303, 117, 1138, 1128, 136, 102, 139, 131, 4785, 117, 146, 112, 182, 2959, 119, 146, 3983, 112, 189, 1151, 1175, 119, 146, 1198, 1458, 1106, 1839, 3044, 1895, 119, 146, 1238, 112, 189, 1928, 1106, 1940, 27112, 3556, 1128, 119, 2825, 1128, 10737, 1143, 136, 102, 138, 131, 146, 8856, 1103, 19242, 117, 1133, 1142, 1144, 1151, 170, 2113, 1104, 170, 4098, 20398, 9839, 119, 2421, 112, 188, 1198, 1129, 1146, 11949, 1114, 1296, 1168, 1397, 1159, 117, 3008, 136, 102, 139, 131, 138, 1403, 15825, 119, 146, 1541, 1821, 2959, 1164, 1142, 119, 2421, 112, 188, 5397, 6387, 170, 7696, 1487, 1770, 117, 1139, 7299, 117, 1106, 1294, 1146, 1111, 1122, 119, 102, 138, 131, 1135, 112, 188, 3008, 119, 146, 112, 1325, 2080, 1128, 1106, 1115, 7696, 117, 1463, 119, 146, 112, 182, 1280, 1106, 2222, 1106, 5548, 1103, 3440, 8334, 119, 25079, 1128, 1224, 119, 102, 139, 131, 102]\n"
     ]
    }
   ],
   "source": [
    "# Print the original conversation.\n",
    "print(' Original: ', train_data[30])\n",
    "# Print that split into tokens.\n",
    "tokenized_sample = tokenizer.tokenize(train_data[30]['conversation'])\n",
    "print('Tokenized: ', tokenized_sample)\n",
    "# Print mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenized_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a342912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch and suffle the data\n",
    "\n",
    "train_dataset = ConversationDataset(train_data, tokenizer, max_length=512)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True ,collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = ConversationDataset(val_data, tokenizer, max_length=512)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = ConversationDataset(test_data, tokenizer, max_length=512)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0e129ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (843 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'segments': [{'input_ids': tensor([  101,   138,   131,  4403,   117,  1225,  1128,  2100,  1164,  1115,\n",
       "            1207,  4382,  1115,  1533,  1146,  5215,   136,   146,   112,  1396,\n",
       "            1151,  2422,  1164,  9444,  1122,  1149,  3568,   119,   102,   139,\n",
       "             131,  2814,   117,   146,   112,  1396,  1767,  1164,  1122,   119,\n",
       "            1135,   112,   188,  3155,  1106,  1129,  6929,   119,   146,  2140,\n",
       "            1355,  1175,  1314,  1989,   119,   102,   138,   131,  8762,   136,\n",
       "            1731,  1108,  1122,   136,  1327,  1225,  1128,  1138,   136,   102,\n",
       "             139,   131,  1109,  1821, 12324,  2093,  1108, 14820,   119,   146,\n",
       "            1125,  1147,  1957,   117,  1103,   176, 26327,  1181, 17646,   119,\n",
       "            1135,  1108, 13446,  1106, 17900,   119,  1192,  1431,  5733,  2222,\n",
       "            1122,   119,   102,   138,   131,  1337,  3807, 13108,   119,  3982,\n",
       "            1122,  5116,   136,   146,   112,   182,  2422,  1122,  1547,  1129,\n",
       "            8733,  1290,  1122,   112,   188,  1253,  2385,  1207,   119,   102,\n",
       "             139,   131, 17078,  1643, 25443,  1193,   117,  1122,  1445,   112,\n",
       "             189,  1115,  5116,   119,   146,  3319,   146,  1355,  1120,   170,\n",
       "            1363,  1159,   119,  1192,  1547,  1328,  1106,  1520,   170,  1952,\n",
       "            1198,  1107,  1692,   117,  1463,   119,   102,   138,   131,  2750,\n",
       "            1911,   119,  2966,  1128,  1301,  1114,  2256,   146,  1221,   136,\n",
       "             102,   139,   131,  1302,   117,   146,  1899,  1146,  1114,   170,\n",
       "            1910,  1121,  1149,  1104,  1411,   119,  6518,  1128,  3983,   112,\n",
       "             189,  1899,  1196,   119,   102,   138,   131,  1731,   112,   188,\n",
       "            5030,  1213,  1175,   136,   146,   112,   182,  1136,  1315, 11367,\n",
       "            1113,  1781,  1103, 14790,  3568,   119,   102,   139,   131,  1670,\n",
       "            1158,  1110,   170, 10647,   119,  1247,   112,   188,   170,  1974,\n",
       "            1198,   170,  3510,  1283,  1115,   112,   188,  1714,  1170,   127,\n",
       "           14123,   119,  1192,  5380,   112,   189,  1138,  1251,  3819,   119,\n",
       "             102,   138,   131, 12174,   119,  1573,   117,  1625,  1950,   146,\n",
       "            1431,  1221,  1164,  1142,  1282,   136,  6291, 10514,  1106,  3644,\n",
       "            1137, 11859,   136,   102,   139,   131,  2119,   117,  1128,  1547,\n",
       "            1328,  1106, 25284,  2330,  1104,  1103,  1763,  1161, 10514,   119,\n",
       "            1422,  1910,  2802,  1141,  1105,  1122,  1108,   170,  2113,  1223,\n",
       "            2246, 18809,  5031,   119, 24953,  1106,  1103,  2343, 24263,   117,\n",
       "            1122,   112,   188,  1147, 13858,   119,   102,   138,   131,  7348,\n",
       "            1122,   117,  2343, 24263,  1122,  1110,   119,  2966,  1128,  2222,\n",
       "            1251,  1104,  1103, 20392,  1116,   136,   102,   139,   131,   146,\n",
       "            1225,   119,  1109,  8888, 16135, 10851,  1108, 10455,   119,  1192,\n",
       "            7284,  1538,  1243,  1122,   119,   102,   138,   131,   146,   112,\n",
       "             182,  1962,   119, 22011, 16135, 10851,  1111, 20392,  1173,   119,\n",
       "            4403,   117,  1156,  1128,  1328,  1106,  2866,  1143,   136,  1135,\n",
       "            1180,  1129,  4106,   106,   102,   139,   131,   146,   112,   173,\n",
       "            1567,  1106,   117,  1133,   146,   112,  1396,  1400,  1142,  1645,\n",
       "            3568,  1115,   146,  1169,   112,   189,  1243,  1149,  1104,   119,\n",
       "           10463,  4031,   136,   102,   138,   131,  6542,   117,  1185,  2463,\n",
       "             119,  2389,  1397,  1159,   119,   102,   139,   131,  3177, 16598,\n",
       "            3150,  1193,   119,  2421,  1143,  1221,  1293,  1122,  2947,   117,\n",
       "            1463,   119,   146,  1328,  1106,  2100,  1155,  1164,  1122,   119,\n",
       "             102,   138,   131,  3100,  1202,   119,  5749,  1111,  1103, 10538,\n",
       "             106,   102,   139,   131,  6291,  4974,   106, 13832, 18734,  1240,\n",
       "            3440,   119,   102,   113,  2611,  1115,  3440,   114,   102,   138,\n",
       "             131,  4403,   117,   146,   112,   182,  1120,  1103,  4382,   117,\n",
       "            1133,  1175,  3093,  1106,  1129,   170,  2463,   119,  1220,  1274,\n",
       "               0,     0]),\n",
       "   'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 0, 0])},\n",
       "  {'input_ids': tensor([  114,   102,   138,   131,  4403,   117,   146,   112,   182,  1120,\n",
       "            1103,  4382,   117,  1133,  1175,  3093,  1106,  1129,   170,  2463,\n",
       "             119,  1220,  1274,   112,   189,  1138,   170,   176, 26327,  1181,\n",
       "           17646,  1957,   117,  1105,  1103, 17989,  1163,  1152,  1309,  1125,\n",
       "            1141,   119,  8696,   117,  1175,   112,   188,  1185,  8888, 16135,\n",
       "           10851,  1113,  1103, 13171,   119,   102,   139,   131,  2048,   117,\n",
       "            1115,   112,   188,  5849,   119,  2389,  1152,  2014,  1147, 13171,\n",
       "             136,  5068,  1202,  1115,  1155,  1103,  1159,   119,   102,   138,\n",
       "             131,  2389,   117,  1133,  1175,   112,   188,  1145,  1185,  1714,\n",
       "            5030,  1974,  2721,   119,  1109,  7064,  1141,  1110, 13758,   170,\n",
       "            1119, 27944,  7216,   119,   146,   112,   182,   170,  2113,  4853,\n",
       "            1303,   119,   102,   139,   131, 27158,   117,  1115,   112,   188,\n",
       "            4020,   119,   146,  1538,  1138,  3216,  1122,  1146,  1114,  1330,\n",
       "            1282,   119,  6502,  1164,  1115,   119,   102,   138,   131,  1262,\n",
       "            1103,  4382,  1163,  1152,  1198,  1125,  1147,  5372,  2280,  1160,\n",
       "            1552,  2403,   119,  1731,  1225,  1128,  1435,  1303,  1314,  1989,\n",
       "             136,   102,   139,   131,  4254,  1989,   117,  1160,  1552,  2403,\n",
       "             117,  1159, 10498,   117,  1128,  1221,   136,   146,  1538,   112,\n",
       "            1396,  1400,  1103,  2280,  2236,  2488,   119,   102,   138,   131,\n",
       "             146,   112,   182,  2033,  1103,  2296,  1115,  1128,   112,  1396,\n",
       "            1309,  2140,  1151,  1303,   117,  1138,  1128,   136,   102,   139,\n",
       "             131,  4785,   117,   146,   112,   182,  2959,   119,   146,  3983,\n",
       "             112,   189,  1151,  1175,   119,   146,  1198,  1458,  1106,  1839,\n",
       "            3044,  1895,   119,   146,  1238,   112,   189,  1928,  1106,  1940,\n",
       "           27112,  3556,  1128,   119,  2825,  1128, 10737,  1143,   136,   102,\n",
       "             138,   131,   146,  8856,  1103, 19242,   117,  1133,  1142,  1144,\n",
       "            1151,   170,  2113,  1104,   170,  4098, 20398,  9839,   119,  2421,\n",
       "             112,   188,  1198,  1129,  1146, 11949,  1114,  1296,  1168,  1397,\n",
       "            1159,   117,  3008,   136,   102,   139,   131,   138,  1403, 15825,\n",
       "             119,   146,  1541,  1821,  2959,  1164,  1142,   119,  2421,   112,\n",
       "             188,  5397,  6387,   170,  7696,  1487,  1770,   117,  1139,  7299,\n",
       "             117,  1106,  1294,  1146,  1111,  1122,   119,   102,   138,   131,\n",
       "            1135,   112,   188,  3008,   119,   146,   112,  1325,  2080,  1128,\n",
       "            1106,  1115,  7696,   117,  1463,   119,   146,   112,   182,  1280,\n",
       "            1106,  2222,  1106,  5548,  1103,  3440,  8334,   119, 25079,  1128,\n",
       "            1224,   119,   102,   139,   131,   102,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0]),\n",
       "   'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0])}],\n",
       " 'A_bad_intent': 0,\n",
       " 'B_bad_intent': 1}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84265807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ffe5413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of tokens 843\n",
      "end 510\n",
      "start 460\n",
      "end 843\n",
      "start 920\n",
      "[[101, 138, 131, 4403, 117, 1225, 1128, 2100, 1164, 1115, 1207, 4382, 1115, 1533, 1146, 5215, 136, 146, 112, 1396, 1151, 2422, 1164, 9444, 1122, 1149, 3568, 119, 102, 139, 131, 2814, 117, 146, 112, 1396, 1767, 1164, 1122, 119, 1135, 112, 188, 3155, 1106, 1129, 6929, 119, 146, 2140, 1355, 1175, 1314, 1989, 119, 102, 138, 131, 8762, 136, 1731, 1108, 1122, 136, 1327, 1225, 1128, 1138, 136, 102, 139, 131, 1109, 1821, 12324, 2093, 1108, 14820, 119, 146, 1125, 1147, 1957, 117, 1103, 176, 26327, 1181, 17646, 119, 1135, 1108, 13446, 1106, 17900, 119, 1192, 1431, 5733, 2222, 1122, 119, 102, 138, 131, 1337, 3807, 13108, 119, 3982, 1122, 5116, 136, 146, 112, 182, 2422, 1122, 1547, 1129, 8733, 1290, 1122, 112, 188, 1253, 2385, 1207, 119, 102, 139, 131, 17078, 1643, 25443, 1193, 117, 1122, 1445, 112, 189, 1115, 5116, 119, 146, 3319, 146, 1355, 1120, 170, 1363, 1159, 119, 1192, 1547, 1328, 1106, 1520, 170, 1952, 1198, 1107, 1692, 117, 1463, 119, 102, 138, 131, 2750, 1911, 119, 2966, 1128, 1301, 1114, 2256, 146, 1221, 136, 102, 139, 131, 1302, 117, 146, 1899, 1146, 1114, 170, 1910, 1121, 1149, 1104, 1411, 119, 6518, 1128, 3983, 112, 189, 1899, 1196, 119, 102, 138, 131, 1731, 112, 188, 5030, 1213, 1175, 136, 146, 112, 182, 1136, 1315, 11367, 1113, 1781, 1103, 14790, 3568, 119, 102, 139, 131, 1670, 1158, 1110, 170, 10647, 119, 1247, 112, 188, 170, 1974, 1198, 170, 3510, 1283, 1115, 112, 188, 1714, 1170, 127, 14123, 119, 1192, 5380, 112, 189, 1138, 1251, 3819, 119, 102, 138, 131, 12174, 119, 1573, 117, 1625, 1950, 146, 1431, 1221, 1164, 1142, 1282, 136, 6291, 10514, 1106, 3644, 1137, 11859, 136, 102, 139, 131, 2119, 117, 1128, 1547, 1328, 1106, 25284, 2330, 1104, 1103, 1763, 1161, 10514, 119, 1422, 1910, 2802, 1141, 1105, 1122, 1108, 170, 2113, 1223, 2246, 18809, 5031, 119, 24953, 1106, 1103, 2343, 24263, 117, 1122, 112, 188, 1147, 13858, 119, 102, 138, 131, 7348, 1122, 117, 2343, 24263, 1122, 1110, 119, 2966, 1128, 2222, 1251, 1104, 1103, 20392, 1116, 136, 102, 139, 131, 146, 1225, 119, 1109, 8888, 16135, 10851, 1108, 10455, 119, 1192, 7284, 1538, 1243, 1122, 119, 102, 138, 131, 146, 112, 182, 1962, 119, 22011, 16135, 10851, 1111, 20392, 1173, 119, 4403, 117, 1156, 1128, 1328, 1106, 2866, 1143, 136, 1135, 1180, 1129, 4106, 106, 102, 139, 131, 146, 112, 173, 1567, 1106, 117, 1133, 146, 112, 1396, 1400, 1142, 1645, 3568, 1115, 146, 1169, 112, 189, 1243, 1149, 1104, 119, 10463, 4031, 136, 102, 138, 131, 6542, 117, 1185, 2463, 119, 2389, 1397, 1159, 119, 102, 139, 131, 3177, 16598, 3150, 1193, 119, 2421, 1143, 1221, 1293, 1122, 2947, 117, 1463, 119, 146, 1328, 1106, 2100, 1155, 1164, 1122, 119, 102, 138, 131, 3100, 1202, 119, 5749, 1111, 1103, 10538, 106, 102, 139, 131, 6291, 4974, 106, 13832, 18734, 1240, 3440, 119, 102, 113, 2611, 1115, 3440, 114, 102, 138, 131, 4403, 117, 146, 112, 182, 1120, 1103, 4382, 117, 1133, 1175, 3093, 1106, 1129, 170, 2463, 119, 1220, 1274], [102, 138, 131, 3100, 1202, 119, 5749, 1111, 1103, 10538, 106, 102, 139, 131, 6291, 4974, 106, 13832, 18734, 1240, 3440, 119, 102, 113, 2611, 1115, 3440, 114, 102, 138, 131, 4403, 117, 146, 112, 182, 1120, 1103, 4382, 117, 1133, 1175, 3093, 1106, 1129, 170, 2463, 119, 1220, 1274, 112, 189, 1138, 170, 176, 26327, 1181, 17646, 1957, 117, 1105, 1103, 17989, 1163, 1152, 1309, 1125, 1141, 119, 8696, 117, 1175, 112, 188, 1185, 8888, 16135, 10851, 1113, 1103, 13171, 119, 102, 139, 131, 2048, 117, 1115, 112, 188, 5849, 119, 2389, 1152, 2014, 1147, 13171, 136, 5068, 1202, 1115, 1155, 1103, 1159, 119, 102, 138, 131, 2389, 117, 1133, 1175, 112, 188, 1145, 1185, 1714, 5030, 1974, 2721, 119, 1109, 7064, 1141, 1110, 13758, 170, 1119, 27944, 7216, 119, 146, 112, 182, 170, 2113, 4853, 1303, 119, 102, 139, 131, 27158, 117, 1115, 112, 188, 4020, 119, 146, 1538, 1138, 3216, 1122, 1146, 1114, 1330, 1282, 119, 6502, 1164, 1115, 119, 102, 138, 131, 1262, 1103, 4382, 1163, 1152, 1198, 1125, 1147, 5372, 2280, 1160, 1552, 2403, 119, 1731, 1225, 1128, 1435, 1303, 1314, 1989, 136, 102, 139, 131, 4254, 1989, 117, 1160, 1552, 2403, 117, 1159, 10498, 117, 1128, 1221, 136, 146, 1538, 112, 1396, 1400, 1103, 2280, 2236, 2488, 119, 102, 138, 131, 146, 112, 182, 2033, 1103, 2296, 1115, 1128, 112, 1396, 1309, 2140, 1151, 1303, 117, 1138, 1128, 136, 102, 139, 131, 4785, 117, 146, 112, 182, 2959, 119, 146, 3983, 112, 189, 1151, 1175, 119, 146, 1198, 1458, 1106, 1839, 3044, 1895, 119, 146, 1238, 112, 189, 1928, 1106, 1940, 27112, 3556, 1128, 119, 2825, 1128, 10737, 1143, 136, 102, 138, 131, 146, 8856, 1103, 19242, 117, 1133, 1142, 1144, 1151, 170, 2113, 1104, 170, 4098, 20398, 9839, 119, 2421, 112, 188, 1198, 1129, 1146, 11949, 1114, 1296, 1168, 1397, 1159, 117, 3008, 136, 102, 139, 131, 138, 1403, 15825, 119, 146, 1541, 1821, 2959, 1164, 1142, 119, 2421, 112, 188, 5397, 6387, 170, 7696, 1487, 1770, 117, 1139, 7299, 117, 1106, 1294, 1146, 1111, 1122, 119, 102, 138, 131, 1135, 112, 188, 3008, 119, 146, 112, 1325, 2080, 1128, 1106, 1115, 7696, 117, 1463, 119, 146, 112, 182, 1280, 1106, 2222, 1106, 5548, 1103, 3440, 8334, 119, 25079, 1128, 1224, 119, 102, 139, 131, 102]]\n"
     ]
    }
   ],
   "source": [
    "# test spiliting tokens:\n",
    "tokens = tokenizer.encode(train_data[30]['conversation'], add_special_tokens=False)\n",
    "segments = []\n",
    "print('len of tokens', len(tokens))\n",
    "if len(tokens) > 510:  # for [CLS] and [SEP]\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + 510, len(tokens))\n",
    "        print('end' , end)\n",
    "        segment = tokens[start:end]\n",
    "        #segment = [self.tokenizer.cls_token_id] + segment + [self.tokenizer.sep_token_id] # do I need this?\n",
    "        #pad chunks up to the max length\n",
    "        #padded_segments = segments + [self.tokenizer.pad_token_id] * (self.max_length - len(segments))  # do I need this?\n",
    "        segments.append(segment)              \n",
    "        # update the start and end of the next chunk\n",
    "        start += 510 - 50\n",
    "        print('start' , start)\n",
    "\n",
    "else:\n",
    "    segments.append(tokens)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1df71d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59083bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "893\n"
     ]
    }
   ],
   "source": [
    "# len of all tokens after chunking, considering the overlap\n",
    "all_len = 0\n",
    "for i in segments:\n",
    "    all_len += len(i)\n",
    "print(all_len)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e375309",
   "metadata": {},
   "source": [
    "# sometime there is indices error when training, lets check if :\n",
    "#Token indices sequence length is longer than the specified maximum sequence length for this model\n",
    "for i, j in enumerate(train_dataset):\n",
    "    for a, b in enumerate(train_dataset[i]['segments']):\n",
    "        if len(train_dataset[i]['segments'][a]['input_ids']) > 512:\n",
    "            print('in train dataset:')\n",
    "            print(i,a, len(train_dataset[i]['segments'][a]['input_ids']))\n",
    "        \n",
    "for i, j in enumerate(val_dataset):\n",
    "    for a, b in enumerate(val_dataset[i]['segments']):\n",
    "        if len(val_dataset[i]['segments'][a]['input_ids']) > 512:\n",
    "            print('in val dataset:')\n",
    "            print(i, len(val_dataset[i]['segments'][a]['input_ids']))\n",
    "        \n",
    "for i, j in enumerate(test_dataset):\n",
    "    for a, b in enumerate(test_dataset[i]['segments']):\n",
    "        if len(test_dataset[i]['segments'][a]['input_ids']) > 512:\n",
    "            print('in test dataset:')\n",
    "            print(i, len(test_dataset[i]['segments'][a]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bf3072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove variables after use and collect the garbage\n",
    "def gc_colloctor(a, b) : \n",
    "    del a\n",
    "    del b\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32379646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, dataloader, device):\n",
    "    model.eval()  # put model in evaluation mode\n",
    "    true_labels_a = []\n",
    "    true_labels_b = []\n",
    "    pred_labels_a = []\n",
    "    pred_labels_b = []\n",
    " \n",
    "    with torch.no_grad():  # no need to track gradients for evaluation\n",
    "        for batch in dataloader:\n",
    "            # initialize lists to store logits for all segments\n",
    "            logits_a_list = []\n",
    "            logits_b_list = []\n",
    "            # aggregate logits for each segment in the batch\n",
    "            for segments in batch['segments']:  # 'segments' is a list of dictionaries\n",
    "                segment_logits_a = []\n",
    "                segment_logits_b = []\n",
    "                for segment in segments:\n",
    "                    input_ids = segment['input_ids'].to(device)\n",
    "                    attention_mask = segment['attention_mask'].to(device)\n",
    "                    logits_a, logits_b = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "                    gc_colloctor(input_ids, attention_mask )\n",
    "                    \n",
    "                    segment_logits_a.append(logits_a)\n",
    "                    segment_logits_b.append(logits_b)\n",
    "                    gc_colloctor(logits_a, logits_b)\n",
    "                    \n",
    "                # combine the logits from all segments for the current example\n",
    "                logits_a = torch.mean(torch.stack(segment_logits_a), dim=0)\n",
    "                logits_b = torch.mean(torch.stack(segment_logits_b), dim=0)\n",
    "                gc_colloctor(segment_logits_a, segment_logits_b)\n",
    "                \n",
    "                logits_a_list.append(logits_a)\n",
    "                logits_b_list.append(logits_b)\n",
    "                gc_colloctor(logits_a, logits_b)\n",
    " \n",
    "            # convert logits to probabilities and then to binary predictions\n",
    "            probs_a = torch.sigmoid(torch.cat(logits_a_list)).cpu().numpy()\n",
    "            probs_b = torch.sigmoid(torch.cat(logits_b_list)).cpu().numpy()\n",
    "            preds_a = (probs_a > 0.5).astype(int)\n",
    "            preds_b = (probs_b > 0.5).astype(int)\n",
    " \n",
    "            # collect the true labels and predictions\n",
    "            true_labels_a.extend(batch['A_bad_intent'].numpy())\n",
    "            true_labels_b.extend(batch['B_bad_intent'].numpy())\n",
    "            pred_labels_a.extend(preds_a)\n",
    "            pred_labels_b.extend(preds_b)\n",
    " \n",
    "    # weighted F1 scores becouse dataset is imbalanced\n",
    "    f1_a = f1_score(true_labels_a, pred_labels_a, average='weighted')\n",
    "    f1_b = f1_score(true_labels_b, pred_labels_b, average='weighted')\n",
    " \n",
    "    return {\n",
    "        'f1_a': f1_a,\n",
    "        'f1_b': f1_b\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d128e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation have a very similar proceess, lets put that in a function\n",
    "def process_dataloader(model, dataloader, device, loss_function, step, accumulation_steps = 0):\n",
    "    num_batch = 0\n",
    "    total_val_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        num_batch += 1\n",
    "        print(num_batch , ' batch ...')\n",
    "        #optimizer.zero_grad()  # zero the parameter gradients     \n",
    "        \n",
    "         # labels need to be of float type and reshaped to be of the same size as output logits\n",
    "        labels_a = batch['A_bad_intent'].to(device).float().view(-1, 1)\n",
    "        labels_b = batch['B_bad_intent'].to(device).float().view(-1, 1)\n",
    "        # initialize variables to accumulate the logits for all segments\n",
    "        logits_a_accumulated = []\n",
    "        logits_b_accumulated = []\n",
    "        \n",
    "        # loop over all items in the batch \n",
    "        print('len of sengmentS' , len(batch['segments']))\n",
    "        for segments in batch['segments']:  # now 'segments' is a list of dictionaries   \n",
    "            torch.cuda.empty_cache()\n",
    "            logits_a_list = []\n",
    "            logits_b_list = []\n",
    "            \n",
    "            print('len of sengment..' , len(segments))\n",
    "            for segment in segments:\n",
    "                torch.cuda.empty_cache()\n",
    "                # segment is a dictionary as expected\n",
    "                input_ids = segment['input_ids'].to(device)\n",
    "                attention_mask = segment['attention_mask'].to(device)\n",
    "               \n",
    "                #print(f\"Input IDs shape: {segment['input_ids'].shape}\")\n",
    "                #print(f\"Attention Mask shape: {segment['attention_mask'].shape}\")\n",
    "                \n",
    "                # forward pass for this segment\n",
    "                logits_a, logits_b = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "                gc_colloctor(input_ids, attention_mask )\n",
    "                #print('Model output:', logits_a, logits_b)\n",
    "                \n",
    "                logits_a_list.append(logits_a)\n",
    "                logits_b_list.append(logits_b)\n",
    "                gc_colloctor(logits_a, logits_b )\n",
    "                \n",
    "            # aggregate the results for all segments of this item\n",
    "            # here we use the mean of the logits, can choose other methods like max\n",
    "            logits_a_item = torch.mean(torch.stack(logits_a_list), dim=0)\n",
    "            logits_b_item = torch.mean(torch.stack(logits_b_list), dim=0)\n",
    "            gc_colloctor(logits_a_list, logits_b_list )\n",
    "            \n",
    "            # zccumulate the logits for all items\n",
    "            logits_a_accumulated.append(logits_a_item)\n",
    "            logits_b_accumulated.append(logits_b_item)\n",
    "            gc_colloctor(logits_a_item, logits_b_item )\n",
    "            \n",
    "        # Combine the accumulated logits for the whole batch\n",
    "        logits_a = torch.cat(logits_a_accumulated, dim=0)\n",
    "        logits_b = torch.cat(logits_b_accumulated, dim=0)\n",
    "        gc_colloctor(logits_b_accumulated, logits_a_accumulated )\n",
    "\n",
    "        # compute loss for both outputs\n",
    "        loss_a = loss_function(logits_a.view(-1, 1), labels_a)\n",
    "        loss_b = loss_function(logits_b.view(-1, 1), labels_b)\n",
    "        \n",
    "        gc_colloctor(logits_a, logits_b )\n",
    "        gc_colloctor(labels_a, labels_b )\n",
    "        \n",
    "        loss = (loss_a + loss_b) / 2  # combine the losses\n",
    "        gc_colloctor(loss_a, loss_b )\n",
    "        \n",
    "        if (step == 'train'):\n",
    "            loss = loss / accumulation_steps  # normalize our loss (if averaged)\n",
    "            # backward pass and optimize\n",
    "            loss.backward()      \n",
    "            #optimizer.step()\n",
    "            #perform optimization every 'accumulation_steps' iterations\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "            total_val_loss = 0 # no need to that for training, just to return from function\n",
    "        elif(step == 'val'):\n",
    "            total_val_loss += loss.item()  # accumulate the total validation loss for validation\n",
    "    \n",
    "    \n",
    "    return total_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "289df9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, device, loss_function):\n",
    "    accumulation_steps = 4  # should be adjust to  fit within avaialable memory limits\n",
    "    model.train()  # set the model to training mode\n",
    "    process_dataloader(model, dataloader, device, loss_function, 'train', accumulation_steps)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "564b7d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, loss_function):\n",
    "    model.eval()  # put model in evaluation mode\n",
    " \n",
    "    with torch.no_grad():  # no need to track gradients for evaluation\n",
    "        total_val_loss = process_dataloader(model, dataloader, device, loss_function, 'val')        \n",
    " \n",
    "    avg_val_loss = total_val_loss / len(dataloader)\n",
    "    val_metrics = compute_metrics(model, dataloader, device)\n",
    "    return avg_val_loss, val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14000125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cb825acb7e49278e995259f25ec2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model & Optimizer\n",
    "model = ParticipantClassifier('bert-base-cased')\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8) # eps: a very small number to prevent any division by zero\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "909c4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ba6d60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb15c9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eee2c8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (28996, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:    \n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:    \n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "994beddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@todo check oov words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdb175e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@todo improve metrics calculation -- check overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f01bbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@todo plot validation training f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3475a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5097ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0f63ac2f",
   "metadata": {},
   "source": [
    "#gives a readable summary of memory allocation and allows to figure the reason of CUDA running out of memory\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a8c5be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch ===  0\n",
      "Training ***\n",
      "1  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "2  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "3  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 1\n",
      "4  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "5  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "6  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "7  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "8  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "9  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 1\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 1\n",
      "10  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "11  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 1\n",
      "12  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "13  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 3\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "14  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "15  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "16  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "17  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "18  batch ...\n",
      "len of sengmentS 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "Validation ***\n",
      "1  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "2  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "3  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "4  batch ...\n",
      "len of sengmentS 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "Validation Loss after epoch 0: 0.6741509735584259\n",
      "Validation Metrics after epoch 0: {'f1_a': 0.5333333333333333, 'f1_b': 0.44999999999999996}\n",
      "Test ***\n",
      "Test Metrics after epoch 0: {'f1_a': 0.16666666666666666, 'f1_b': 0.7111111111111111}\n",
      "epoch ===  1\n",
      "Training ***\n",
      "1  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "2  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "3  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "4  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 1\n",
      "5  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "6  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "7  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "8  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "9  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "10  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "11  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "12  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 1\n",
      "len of sengment.. 2\n",
      "13  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "14  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 1\n",
      "len of sengment.. 1\n",
      "len of sengment.. 3\n",
      "15  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "16  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "17  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "18  batch ...\n",
      "len of sengmentS 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "Validation ***\n",
      "1  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "2  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "3  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "4  batch ...\n",
      "len of sengmentS 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "Validation Loss after epoch 1: 0.7451899647712708\n",
      "Validation Metrics after epoch 1: {'f1_a': 0.16666666666666666, 'f1_b': 0.2285714285714286}\n",
      "Test ***\n",
      "Test Metrics after epoch 1: {'f1_a': 0.5333333333333333, 'f1_b': 0.06666666666666667}\n",
      "epoch ===  2\n",
      "Training ***\n",
      "1  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "2  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "3  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "4  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 1\n",
      "len of sengment.. 3\n",
      "len of sengment.. 1\n",
      "len of sengment.. 2\n",
      "5  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 1\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "6  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "7  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "8  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "9  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "10  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "11  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "12  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "13  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 1\n",
      "14  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "15  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "16  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "17  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 3\n",
      "18  batch ...\n",
      "len of sengmentS 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "Validation ***\n",
      "1  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "2  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "3  batch ...\n",
      "len of sengmentS 4\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 3\n",
      "len of sengment.. 2\n",
      "4  batch ...\n",
      "len of sengmentS 3\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "len of sengment.. 2\n",
      "Validation Loss after epoch 2: 0.7094635665416718\n",
      "Validation Metrics after epoch 2: {'f1_a': 0.5333333333333333, 'f1_b': 0.2285714285714286}\n",
      "Test ***\n",
      "Test Metrics after epoch 2: {'f1_a': 0.2592592592592593, 'f1_b': 0.06666666666666667}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = []\n",
    "validation_metrics = []\n",
    "validation_loss = []\n",
    "for epoch in range(3):  # 3 epochs?\n",
    "    \n",
    "    print('epoch === ', epoch)\n",
    "    \n",
    "    print('Training ***')\n",
    "    train_model(model, train_dataloader, device, loss_function)\n",
    "    \n",
    "    print('Validation ***')\n",
    "    avg_val_loss, val_metrics = evaluate_model(model, val_dataloader, device, loss_function)\n",
    "    print(f'Validation Loss after epoch {epoch}: {avg_val_loss}')\n",
    "    print(f'Validation Metrics after epoch {epoch}: {val_metrics}')\n",
    "    validation_loss.append(avg_val_loss)\n",
    "    validation_metrics.append(val_metrics)\n",
    " \n",
    "    print('Test ***')\n",
    "    metrics = compute_metrics(model, test_dataloader, device)\n",
    "    test_metrics.append(metrics)\n",
    "    print(f'Test Metrics after epoch {epoch}:', metrics)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed288ddc",
   "metadata": {},
   "source": [
    "#gives a readable summary of memory allocation and allows to figure the reason of CUDA running out of memory\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6fa449c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'f1_a': 0.16666666666666666, 'f1_b': 0.7111111111111111},\n",
       " {'f1_a': 0.5333333333333333, 'f1_b': 0.06666666666666667},\n",
       " {'f1_a': 0.2592592592592593, 'f1_b': 0.06666666666666667}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15eba6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'f1_a': 0.5333333333333333, 'f1_b': 0.44999999999999996},\n",
       " {'f1_a': 0.16666666666666666, 'f1_b': 0.2285714285714286},\n",
       " {'f1_a': 0.5333333333333333, 'f1_b': 0.2285714285714286}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "341379a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6741509735584259, 0.7451899647712708, 0.7094635665416718]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f38fbf",
   "metadata": {},
   "source": [
    "---\n",
    "#### To do list:\n",
    "\n",
    "2. Improve chunking tokens --> chunck tokens bsed on each turn in the conversation and include prevouse and later turns for each chunck\n",
    "3. improve model performance + plot the results   \n",
    "4. Try other models(DistilBERT, ALBERT, RoBerta)\n",
    "5. Evaluate model on diplomacy dataset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
