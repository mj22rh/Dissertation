{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk import bigrams, trigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textstat import flesch_reading_ease # exploring text complixity\n",
    "# for sentiment analysis\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data'\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "for i in ['train.jsonl', 'test.jsonl', 'validation.jsonl']:\n",
    "    filename = os.path.join(folder_path, i)\n",
    "    if os.path.exists(filename):\n",
    "        print(f\" File {filename} already exists.\")\n",
    "    else:\n",
    "        url = \"https://github.com/DenisPeskov/2020_acl_diplomacy/raw/master/data/\"+i\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"File '{filename}' has been downloaded and saved to '{folder_path}'.\")\n",
    "            # join all three files into a file for feature extraction, EDA and cleaning \n",
    "            with open(folder_path + '/all_data.jsonl', 'ab') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"File '{filename}' added to all_data file.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json('data/all_data.jsonl', lines=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_ids = []\n",
    "senders = []\n",
    "receivers = []\n",
    "messages = []\n",
    "sender_labels = []\n",
    "receiver_labels = []\n",
    "scores = []\n",
    "\n",
    "\n",
    "for i, j in df.iterrows():       \n",
    "    for c, d in enumerate(j['messages']):\n",
    "        #print(c)\n",
    "        game_ids.append(j['game_id'])\n",
    "        senders.append(j['speakers'][c])\n",
    "        receivers.append(j['receivers'][c])\n",
    "        messages.append(j['messages'][c])\n",
    "        sender_labels.append(j['sender_labels'][c])\n",
    "        receiver_labels.append(j['receiver_labels'][c])\n",
    "        scores.append(j['game_score'][c])\n",
    "        \n",
    "new_df = pd.DataFrame({'game_id': game_ids, 'sender': senders, 'receiver' : receivers, 'message': messages ,\n",
    "                       'sender_label': sender_labels, 'receiver_label': receiver_labels, 'score': scores })\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df['game_id'] == 5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(message):\n",
    "    processed = []   \n",
    "    for text in message:\n",
    "        # replaace URLs\n",
    "        text = re.sub(r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\",'URL',text)  \n",
    "        \n",
    "        # Replace all non alphabets.\n",
    "        #text = re.sub( \"[^a-zA-Z0-9]\", \" \", text)  # this will replace emojies as well!\n",
    "        \n",
    "        # Remove HTML/XML tags (if any)\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "        # Remove punctuation and symbols -- (not for now)\n",
    "        # text = re.sub(r'[^\\w\\s]', '', text) \n",
    "\n",
    "        # Remove numbers -- not for now\n",
    "        #text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        # Remove whitespaces (including new lines and tabs)\n",
    "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "        \n",
    "        processed.append(text)       \n",
    "    return processed\n",
    "\n",
    "new_df['processed_message'] = text_preprocess(list(new_df['message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e51ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df['processed_message'] == 'URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd338f0",
   "metadata": {},
   "source": [
    "---\n",
    "#### Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae886bf0",
   "metadata": {},
   "source": [
    "+ number of words in the sentence\n",
    "+ frequency of functional words in the sentence\n",
    "+ frequency of certain Parts-of-Speech (PoS) tags in the sentence such as:\n",
    "+ + frequency of pronouns (total, first person, second person, third person)\n",
    "+ + negations (e.g. use of \"not\"-type words)\n",
    "+ + frequency of articles\n",
    "+ + frequency of prepositions\n",
    "+ frequency of certain N-grams in the sentence\n",
    "+ sentence complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bce8cb",
   "metadata": {},
   "source": [
    "+ frequency of words used found in some curated, potentially domain-specific dictionary\n",
    "+ frequency of words of length > 6\n",
    "+ sentiment\n",
    "+ certainty (from self-doubting to absolutely certain)\n",
    "+ implied causality\n",
    "+ inclusivity/exclusivity\n",
    "+ frequency of motion verbs\n",
    "+ frequency of past tense verbs\n",
    "+ frequency of future tense verbs\n",
    "+ word/sentence embeddings (from e.g. bert-base encoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce1619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in the sentence\n",
    "new_df['num_words_sentence'] = new_df['processed_message'].apply(lambda x: len(x.split()))\n",
    "new_df[['processed_message' , 'num_words_sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761fb9ba",
   "metadata": {},
   "source": [
    "---\n",
    "#### frequency of functional words in the sentence --> (stop words!?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def count_stop_words(sentence):\n",
    "    words = sentence.split()\n",
    "    stop_words_in_sentence = [word for word in words if word.lower() in stop_words]\n",
    "    return len(stop_words_in_sentence)\n",
    "\n",
    "new_df['stop_words_count'] = new_df['processed_message'].apply(count_stop_words)\n",
    "new_df[['processed_message' , 'stop_words_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd28cf3",
   "metadata": {},
   "source": [
    "---\n",
    "#### frequency of certain Parts-of-Speech (PoS) tags in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#Noun (NN)\n",
    "#Verb (VB)\n",
    "#Adjective(JJ)\n",
    "#Adverb(RB)\n",
    "#Preposition (IN)\n",
    "#Conjunction (CC)\n",
    "#Pronoun(PRP)\n",
    "#Interjection (INT)\n",
    "\n",
    "# here i get all pos!\n",
    "# the result of this function would be json which wont be really useful as a feature\n",
    "def pos_tag_frequency(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    #select the second element in each tuple\n",
    "    tags = [tag for word, tag in pos_tags if tag == 'PRP']\n",
    "    tag_freq = Counter(tags)\n",
    "    return tag_freq\n",
    "\n",
    "#new_df['pos_tag_freq'] = new_df['processed_message'].apply(pos_tag_frequency)\n",
    "#new_df[['processed_message' , 'pos_tag_freq']]\n",
    "\n",
    "pron_freq = []\n",
    "article_freq = []\n",
    "prep_freq = []\n",
    "adj_freq = []\n",
    "\n",
    "for i, j in new_df.iterrows():       \n",
    "    words = word_tokenize(j['processed_message'])\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    pron_freq.append(len([tag for word, tag in pos_tags if tag == 'PRP']))\n",
    "    article_freq.append(sum([1 for word in words if word.lower() in ['the', 'a', 'an']]))\n",
    "    prep_freq.append(len([tag for word, tag in pos_tags if tag == 'IN']))\n",
    "    adj_freq.append(len([tag for word, tag in pos_tags if tag == 'JJ']))\n",
    "        \n",
    "new_df['pron_freq'] = pron_freq\n",
    "new_df['article_freq'] = article_freq\n",
    "new_df['prep_freq'] = prep_freq\n",
    "new_df['adj_freq'] = adj_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cbfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[['processed_message' , 'pron_freq', 'article_freq', 'prep_freq', 'adj_freq']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32436b68",
   "metadata": {},
   "source": [
    "---\n",
    "#### frequency of certain N-grams in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e801518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should actually check how many of top n-grams each sentenc contains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4cdee545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the top N n-grams from text\n",
    "def get_top_ngram(corpus, n=None, m=20):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]       \n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b84cc85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('if you', 1320),\n",
       " ('in the', 1175),\n",
       " ('going to', 891),\n",
       " ('you re', 761),\n",
       " ('to be', 691),\n",
       " ('want to', 667),\n",
       " ('do you', 623),\n",
       " ('you can', 570),\n",
       " ('we can', 553),\n",
       " ('would be', 515),\n",
       " ('need to', 469),\n",
       " ('to do', 458),\n",
       " ('to get', 454),\n",
       " ('are you', 447),\n",
       " ('of the', 442),\n",
       " ('that you', 437),\n",
       " ('you and', 431),\n",
       " ('you have', 419),\n",
       " ('me to', 412),\n",
       " ('on the', 411)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_bigrams=get_top_ngram(new_df['processed_message'],2)\n",
    "top_n_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "878562fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be able to', 239),\n",
       " ('do you think', 201),\n",
       " ('you want to', 199),\n",
       " ('what do you', 190),\n",
       " ('let me know', 162),\n",
       " ('if you re', 158),\n",
       " ('in the fall', 144),\n",
       " ('support you into', 144),\n",
       " ('to work with', 142),\n",
       " ('at this point', 137),\n",
       " ('if you want', 131),\n",
       " ('in the north', 127),\n",
       " ('going to be', 119),\n",
       " ('do you want', 115),\n",
       " ('don want to', 112),\n",
       " ('the north sea', 111),\n",
       " ('it would be', 110),\n",
       " ('we need to', 110),\n",
       " ('what are your', 106),\n",
       " ('is going to', 104)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_trigrams=get_top_ngram(new_df['processed_message'],3)\n",
    "top_n_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8a23737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the sentence includes top bigrams or trigrams:\n",
    "top_bigrams = [i[0] for i in top_n_bigrams]\n",
    "top_trigrams =  [i[0] for i in top_n_trigrams]\n",
    "\n",
    "def count_ngrams(sentence, n):\n",
    "    words = word_tokenize(sentence)\n",
    "    separator = ' '\n",
    "    if n == 2:\n",
    "        n_grams = sum([1 if separator.join(i) in top_bigrams else 0 for i in list(bigrams(words))])\n",
    "    elif n == 3:\n",
    "        n_grams = sum([1 if separator.join(i) in top_trigrams else 0 for i in list(trigrams(words))])\n",
    "    else:\n",
    "        return 0\n",
    "    return n_grams\n",
    "\n",
    "new_df['top_bigram_freq'] = new_df['processed_message'].apply(lambda x: count_ngrams(x, 2))\n",
    "new_df['top_trigram_freq'] = new_df['processed_message'].apply(lambda x: count_ngrams(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6a91d280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_message</th>\n",
       "      <th>top_bigram_freq</th>\n",
       "      <th>top_trigram_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany!  Just the person I want to speak with...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've whet my appetite, Italy. What's the sug...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>👍</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It seems like there are a lot of ways that cou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah, I can’t say I’ve tried it and it works, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17284</th>\n",
       "      <td>You and Austria are the most importand. Italy ...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17285</th>\n",
       "      <td>Hello, Turkey?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17286</th>\n",
       "      <td>Hello???</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17287</th>\n",
       "      <td>Helloooo, turkey</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17288</th>\n",
       "      <td>I don’t understand these messages or tone. Are...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17289 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       processed_message  top_bigram_freq  \\\n",
       "0      Germany!  Just the person I want to speak with...                1   \n",
       "1      You've whet my appetite, Italy. What's the sug...                0   \n",
       "2                                                      👍                0   \n",
       "3      It seems like there are a lot of ways that cou...                0   \n",
       "4      Yeah, I can’t say I’ve tried it and it works, ...                2   \n",
       "...                                                  ...              ...   \n",
       "17284  You and Austria are the most importand. Italy ...                6   \n",
       "17285                                     Hello, Turkey?                0   \n",
       "17286                                           Hello???                0   \n",
       "17287                                   Helloooo, turkey                0   \n",
       "17288  I don’t understand these messages or tone. Are...                3   \n",
       "\n",
       "       top_trigram_freq  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "17284                 1  \n",
       "17285                 0  \n",
       "17286                 0  \n",
       "17287                 0  \n",
       "17288                 0  \n",
       "\n",
       "[17289 rows x 3 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['processed_message' , 'top_bigram_freq' , 'top_trigram_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "565545a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>message</th>\n",
       "      <th>sender_label</th>\n",
       "      <th>receiver_label</th>\n",
       "      <th>score</th>\n",
       "      <th>processed_message</th>\n",
       "      <th>num_words_sentence</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>motion_verb_count</th>\n",
       "      <th>num_past_tense</th>\n",
       "      <th>num_future_tense</th>\n",
       "      <th>pron_freq</th>\n",
       "      <th>article_freq</th>\n",
       "      <th>prep_freq</th>\n",
       "      <th>adj_freq</th>\n",
       "      <th>top_bigram_freq</th>\n",
       "      <th>top_trigram_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>italy</td>\n",
       "      <td>germany</td>\n",
       "      <td>Well, at least I have an idea of who to trust....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>Well, at least I have an idea of who to trust....</td>\n",
       "      <td>116</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089216</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>italy</td>\n",
       "      <td>germany</td>\n",
       "      <td>How are things going with England? I think tha...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>How are things going with England? I think tha...</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>italy</td>\n",
       "      <td>germany</td>\n",
       "      <td>Two bits of advice: #1 I suggest you tell Russ...</td>\n",
       "      <td>True</td>\n",
       "      <td>NOANNOTATION</td>\n",
       "      <td>6</td>\n",
       "      <td>Two bits of advice: #1 I suggest you tell Russ...</td>\n",
       "      <td>88</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>italy</td>\n",
       "      <td>germany</td>\n",
       "      <td>#2 Here is the move set I would suggest right ...</td>\n",
       "      <td>True</td>\n",
       "      <td>NOANNOTATION</td>\n",
       "      <td>6</td>\n",
       "      <td>#2 Here is the move set I would suggest right ...</td>\n",
       "      <td>82</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>germany</td>\n",
       "      <td>italy</td>\n",
       "      <td>I think me and England are really on the same ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>I think me and England are really on the same ...</td>\n",
       "      <td>66</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129563</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17228</th>\n",
       "      <td>11</td>\n",
       "      <td>turkey</td>\n",
       "      <td>russia</td>\n",
       "      <td>Well, I have no issue with you guaranteeing yo...</td>\n",
       "      <td>True</td>\n",
       "      <td>NOANNOTATION</td>\n",
       "      <td>4</td>\n",
       "      <td>Well, I have no issue with you guaranteeing yo...</td>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17251</th>\n",
       "      <td>11</td>\n",
       "      <td>france</td>\n",
       "      <td>russia</td>\n",
       "      <td>Germany is going to move into sweden with engl...</td>\n",
       "      <td>True</td>\n",
       "      <td>NOANNOTATION</td>\n",
       "      <td>6</td>\n",
       "      <td>Germany is going to move into sweden with engl...</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17256</th>\n",
       "      <td>11</td>\n",
       "      <td>france</td>\n",
       "      <td>england</td>\n",
       "      <td>Hey England, how are you? I would like to disc...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>Hey England, how are you? I would like to disc...</td>\n",
       "      <td>81</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17265</th>\n",
       "      <td>11</td>\n",
       "      <td>england</td>\n",
       "      <td>france</td>\n",
       "      <td>how do you feel about supporting me into the n...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>how do you feel about supporting me into the n...</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17284</th>\n",
       "      <td>11</td>\n",
       "      <td>france</td>\n",
       "      <td>turkey</td>\n",
       "      <td>You and Austria are the most importand. Italy ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NOANNOTATION</td>\n",
       "      <td>8</td>\n",
       "      <td>You and Austria are the most importand. Italy ...</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022857</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1585 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       game_id   sender receiver  \\\n",
       "12           1    italy  germany   \n",
       "25           1    italy  germany   \n",
       "47           1    italy  germany   \n",
       "48           1    italy  germany   \n",
       "51           1  germany    italy   \n",
       "...        ...      ...      ...   \n",
       "17228       11   turkey   russia   \n",
       "17251       11   france   russia   \n",
       "17256       11   france  england   \n",
       "17265       11  england   france   \n",
       "17284       11   france   turkey   \n",
       "\n",
       "                                                 message  sender_label  \\\n",
       "12     Well, at least I have an idea of who to trust....          True   \n",
       "25     How are things going with England? I think tha...          True   \n",
       "47     Two bits of advice: #1 I suggest you tell Russ...          True   \n",
       "48     #2 Here is the move set I would suggest right ...          True   \n",
       "51     I think me and England are really on the same ...          True   \n",
       "...                                                  ...           ...   \n",
       "17228  Well, I have no issue with you guaranteeing yo...          True   \n",
       "17251  Germany is going to move into sweden with engl...          True   \n",
       "17256  Hey England, how are you? I would like to disc...         False   \n",
       "17265  how do you feel about supporting me into the n...          True   \n",
       "17284  You and Austria are the most importand. Italy ...         False   \n",
       "\n",
       "      receiver_label score                                  processed_message  \\\n",
       "12              True     3  Well, at least I have an idea of who to trust....   \n",
       "25              True     4  How are things going with England? I think tha...   \n",
       "47      NOANNOTATION     6  Two bits of advice: #1 I suggest you tell Russ...   \n",
       "48      NOANNOTATION     6  #2 Here is the move set I would suggest right ...   \n",
       "51              True     4  I think me and England are really on the same ...   \n",
       "...              ...   ...                                                ...   \n",
       "17228   NOANNOTATION     4  Well, I have no issue with you guaranteeing yo...   \n",
       "17251   NOANNOTATION     6  Germany is going to move into sweden with engl...   \n",
       "17256           True     3  Hey England, how are you? I would like to disc...   \n",
       "17265           True     2  how do you feel about supporting me into the n...   \n",
       "17284   NOANNOTATION     8  You and Austria are the most importand. Italy ...   \n",
       "\n",
       "       num_words_sentence  stop_words_count  ... sentiment  motion_verb_count  \\\n",
       "12                    116                57  ...  0.089216                  7   \n",
       "25                     20                11  ...  0.083333                  1   \n",
       "47                     88                42  ...  0.050000                  5   \n",
       "48                     82                37  ...  0.261905                  3   \n",
       "51                     66                33  ...  0.129563                  2   \n",
       "...                   ...               ...  ...       ...                ...   \n",
       "17228                  58                31  ...  0.000000                  3   \n",
       "17251                  17                 5  ...  0.000000                  2   \n",
       "17256                  81                39  ...  0.270833                  3   \n",
       "17265                  11                 7  ...  0.250000                  0   \n",
       "17284                  74                41  ... -0.022857                  5   \n",
       "\n",
       "       num_past_tense  num_future_tense  pron_freq  article_freq  prep_freq  \\\n",
       "12                  0                 0         10            10         13   \n",
       "25                  0                 0          3             1          3   \n",
       "47                  1                 0         15             1          8   \n",
       "48                  0                 0          9             1          7   \n",
       "51                  1                 0          7             3         11   \n",
       "...               ...               ...        ...           ...        ...   \n",
       "17228               1                 0         11             3          5   \n",
       "17251               0                 1          0             0          2   \n",
       "17256               0                 0         11             5          6   \n",
       "17265               0                 0          2             1          2   \n",
       "17284               0                 0          6             2          8   \n",
       "\n",
       "       adj_freq  top_bigram_freq  top_trigram_freq  \n",
       "12            8                1                 1  \n",
       "25            1                0                 1  \n",
       "47            3                2                 2  \n",
       "48            1                1                 1  \n",
       "51            4                1                 1  \n",
       "...         ...              ...               ...  \n",
       "17228         2                2                 1  \n",
       "17251         2                1                 1  \n",
       "17256         6                4                 1  \n",
       "17265         1                1                 1  \n",
       "17284         1                6                 1  \n",
       "\n",
       "[1585 rows x 25 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[new_df['top_trigram_freq'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c2f28",
   "metadata": {},
   "source": [
    "---\n",
    "#### sentence complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flesch_reading_ease => 206.835 - 1.015 * (total_words / total_sentences) - 84.6 * (total_syllables / total_words)\n",
    "new_df['text_ease'] = new_df['processed_message'].apply(lambda x : flesch_reading_ease(x))\n",
    "new_df[new_df['text_ease']== new_df['text_ease'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904d79b",
   "metadata": {},
   "source": [
    "linguistic complexity is a broad field and there are many other factors could be considered, like the use of passive voice, nominalizations, advanced punctuation, lexical density, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of words used found in some curated, potentially domain-specific dictionary\n",
    "# This is challenging, which domain should be explored??\n",
    "# wordnet from nltk.corpus might be useful if know which domain we are exploring!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e9e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of words of length > 6\n",
    "#get the number of words longer than 6 in each message\n",
    "new_df['num_long_words'] = new_df['processed_message'].str.split().apply(lambda x : [1 if len(i) > 6 else 0 for i in x]).map(lambda x: sum(x))\n",
    "new_df[['processed_message' , 'num_long_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment\n",
    "# textblob is more sensitive to negative sentiments than nltk\n",
    "new_df['sentiment'] = new_df['processed_message'].apply(lambda x: TextBlob(x).sentiment.polarity)  \n",
    "new_df[['processed_message' , 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117b738",
   "metadata": {},
   "source": [
    "---\n",
    "#### certainty (from self-doubting to absolutely certain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e64e64",
   "metadata": {},
   "source": [
    "This one cannot be implemented simply using nlp tools like nltk or by ML models eaither fine-tunned pre-traiined ones or those traind on a labled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6360fa6",
   "metadata": {},
   "source": [
    "---\n",
    "#### implied causality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b765cac",
   "metadata": {},
   "source": [
    "---\n",
    "#### inclusivity/exclusivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dbc29",
   "metadata": {},
   "source": [
    "---\n",
    "#### frequency of motion verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9500dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of all verbs in the dataset\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_unique_verbs(texts):\n",
    "    unique_verbs = set()  #  set to store unique verbs\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        # add the lemmatized form of the word to the set if it is a verb.\n",
    "        unique_verbs.update(token.lemma_ for token in doc if token.pos_ == \"VERB\")\n",
    "    return unique_verbs\n",
    "\n",
    "verbs = list(get_unique_verbs(new_df['processed_message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9fffc7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['copy', 'accept', 'havta', 'apply', 'approach', 'view', 'protect', 'wack', 'would', 'invite', 'live', 'trample', 'sus', 'lower', 'plug', 'raise', 'identify', 'insult', 'debate', 'hardcappe', 'stillcgold', 'seek', 'hatch', 'go', 'crumble', 'unapose', 'ease', 'decline', 'oblige', 'vindicate', 'sit', 'starve', 'takey', 'take', 'schedule', 'pivot', 'dispose', 'board', 'outta', 'neighbor', 'find', 'bait', 'return', 'finish', 'bank', 'propose-', 'garunteee', 'offset', 'relish', 'spare', 'observe', 'beef', 'volunteer', 'reread', 'cast', 'scan', 'exceed', 'gain', 'say', 'prefer', 'begrudge', 'side', 'yank', 'sil', 'announce', 'break', 'see-', 'dunno', 'doxxe', 'hesitate', 'dip', 'die', 'solve', 'waffle', 'gear', 'flower', 'breakup', 'CANT', 'command', 'wanna', 'laugh', 'read', 'extract', 'know', 'accord', 'bar', 'leverage', 'induce', 'devote', 'present', 'alarm', 'devour', 'pile', 'court', 'stomp', 'last', 'crack', 'recall', 'establish', 'prod', 'compound', 'win', 'disentangle', 'reserve', 'flash', 'disagree', 'backstabbr', 'reply', 'see', 'shoulda', 'prey', 'trust', 'time', 'deceive', 'incentivize', 'lit', 'organise', 'crank', 'conclude', '-with', 'finalise', 'boot', 'renege', 'appreciatie', 'enlist', 'skip', 'improve', 'divid', 'scare', 'spiral', 'surprised', 'love', 'form', 'waste', 'feck', 'scratch', 'refine', 'relieve', 'overlap', 'grant', 'fail', 'surprise', 'need', 'circulate', 'chill', 'snipe', 'collapse', 'borrow', 'spitballe', 'dispatch', 'ski', 'reveal', 'wind', 'equip', 'whuppe', 'shed', 'hint', 'crown', 'capture', 'pop', 'characterize', 'areyour', 'foster', 'pose', 'define', 'localize', 'await', 'incase', 'shock', 'stem', 'shame', 'wait', 'harass', 'struggle', 'decommission', 'panic', 'reap', 'involvee', 'overjoy', 'everyone', 'm', 'destine', 'tab', 'complain', 'favor', 'listen', 'affect', 'compliment', 'wana', 'keep', 'reincarnate', 'act', 'suppport', 'shoot', 'chime', 'wash', 'easy', 'spur', 'turn', 'abide', 'destabilize', 'enlighten', 'swamp', '/', 'squash', 'care', 'facilitate', 'apologise', 'monitor', 'expel', 'matter', 'require', 'marshal', 'stress', 'solo', '’ll', 'occupy', 'can-', 'modify', 'angle', '💀', 'burg', 'bing', 'teach', 'enact', 'capitalize', 'spoke', 'battle', 'assess', 'disable', 'wme', 'crunch', 'design', 'stymie', 'chumpe', 'push', 'mismanage', 'happen', 'perform', 'instruct', 'o', 'rebound', 'vary', 'ramp', 'rest', 'devolve', 'correct', 'cope', \"fillet'e\", 'weigh', 'outgunne', 'reclaim', 'supersede', 'occur', 'exchange', 'advocate', 'penetrate', 'bridge', 'demilitarize', 'austria', 'broadcast', 'dance', 'couple', 'invest', 'misunderstood', 'dream', 'compel', 'scold', 'commiserate', 'broker', 'reinforce', 'overextend', 'suprise', 'include', 'secure', 'knit', 'paint', 'den', 'motivate', 'replace', 'shoe', 'mitigate', 'assert', 'prop', 'sure', 'sevastopol', 'offer', 'originate', 'ink', 'guarrenteed', 'consume', \"they're\", 'resolve', 'brainstorm', 'shrink', 'absorb', 'suppose', 'bind', 'chivalry', 'prove', 'slack', 'shove', 'handcuff', 'wrap', 'confer', 'dump', 'profit', 'hamper', 'continue', 'bum', 'autocorrect', 'refuse', 'fuck', 'straighten', 'pit', 'stand', 'lie', 'strike', 'stagnate', 'advise', 'rescue', 'feed', 'unguarde', 'mobilize', 'draw', 'outlast', 'readjust', 'opinionate', 'bet', 'suppourte', 'imply', 'expire', 'reneg', 'agreed', 'twitch', 'owe', 'jockey', 'land', 'cannibalize', 'discount', 'issue', 'breathe', 'feel', 'recieve', 'worsen', 'publish', 'nudge', 'speed', 'buy', 'track', 'overbear', 'comment', 'fledge', 'thumb', 'spike', 'group', 'direct', 'curb', '-', 'enable', 'double', 'curse', 'participate', 'suspect', 'appear', 'recur', 'hasten', 'duke', 'haha', 'enter', 'cool', 'lead', 'brew', 'screenshot', 'distrust', 'institute', 'unnerve', 'outcome', 'bieng', 'crash', '🥳', 'hack', 'decimate', 'underestimate', 'sense', 'quote', 'dearest', 'ptfoe', 'encourage', 'insist', 'front', 'focus', 'ask', 'stitch', 'garauntee', 'lash', 'necessitate', 'balloon', 'quiet', 'grumble', 'reduce', ':', 'tbh', 'execute', 'scrap', 'impact', 'counterattack', 'finalize', 'ping', 'reiterate', 'trace', 'foreclose', 'rob', 'reverse', 'fight', 'gift', '-keepe', 'nevermind', 'decie', 'brest', 'prompt', 'transfer', 'overpower', 'eek', 'slate', 'extend', 'reach', 'invent', 'justify', 'ridicule', 'finagle', 'worthwhile', 'possilble', 'ground', 'pan', 'disallow', 'gape', 'bore', 'apu', 'click', 'excuse', 'piedmont', 'I’m', 'juggle', 'uphold', 'DMZd', 'nibble', 'sail', 'label', 'test', 'swipe', 'manufacture', 'ally', 'congratulate', 'suicide', 'recapture', 'maximize', 'gang', 'grind', 'force', 'send', 'select', 'attain', 'prayer', 'pursue', 'drive', 'redact', '😛', 'greet', 'convert', 'muscle', 'dissuade', 'base', 'muddy', 'insinuate', 'prow', 'turtle', 'trend', 'surround', 'arrive', 'strategerize', 'aught', 'dig', 'hoist', 'juggernaut', 'barrel', 'displace', '😭', 'garunteed', '😅', 'fall', 'harm', 'free', 'constitute', 'edi', 'ion', 'welp', 'hover', 'bother', 'sev', 'pluck', 'let', 'accomplish', 'hem', 'spread', 'entice', 'mix', 'poke', 'square', 'comply', 'switch', 'train', 'doom', 'catapult', 'demiliterize', 'retire', 'hard', 'dick', 'approve', 'adjust', 'relay', 'consdere', 'piss', 'stave', 'fabricate', 'gridlock', 'inconvenience', 'surrender', 'want', 'unfold', 'kiel', 'jiggle', 'reassure', 'undo', 'fool', 'increase', 'context', 'outflank', 'gaslight', 'knife', 'rage', 'revive', 'fire', 'donezo', 'interpret', 'ste', 'activate', 'suggest', 'consolidate', 'swing', 'mull', 'miscount', 'maneuver', 'fetch', 'venice', 'ser', 'shudder', 'snatch', 'pende', 'outmatch', 'mount', 'block', 'dominate', 'backfire', 'lure', 'england', 'plan', 'input', 'bul', 'damage', 'swap', 'divide', 'lie-', 'troll', 'five', 'broach', 'annihilate', 'btw', 'detour', 'state', 'ruhr', 'snuck', 'open', 'overrun', 'keept', 'maintain', 'word', 'knock', 'topple', 'leak', 'loan', 'honor', 'cut', 'decide', 'stink', 'grease', 'trade', 'deconstruct', 'b', 'stretch', 'realise', 'galavante', 'demonstrate', 'fart', 'ruin', 'flow', 'sip', 'fold', 'demilitirize', 'bleed', 'randomize', 'isolate', 'sweden', 'endorse', 'vacate', 'e', 'trigger', 'bear', 'concentrate', 'backstabbre', 'cock', 'hand', 'mesh', 'tackle', 'pitch', 'hug', 'make', 'team', 'sneak', 'antagonize', 'gotcha', 'crush', 'agree-', 'joke', 'build', 'restrategize', 'fry', '-even', 'adjusticate', 'box', 'relinquish', 'annoy', 'muse', 'cage', 'fecke', 'withe', 'clean', 'share', 'deny', 'deploy', 'blush', 'trieste', 'harp', 'brainfarte', 'mosey', 'rule', 'sh*tkicke', 'hollow', 'beleagure', 'dive', 'skeve', 'testify', 'loosen', 'shuffle', 'manage', 'pretend', 'getcha', 'bargain', 'prospect', 'militarize', 'signal', 'regret', 'killie', 'abandon', 'believe', 'develop', 'gue', 'redirect', 'picture', 'retain', '>.>', 'sow', 'possibe', 'reconcile', 'claim', 'ruh', 'recuse', 'overestimate', 'protecct', 'envision', 'explain', 'misplay', 'stir', 'killin', 'misdirect', 'stonewall', 'alert', 'bounce', 'cover', 'agree', 'assure', 'handle', 'reciprocate', 'idc', 'set', 'accommodate', 'guess', 'compete', 'touch', 'powerfull', 'warp', 'promise', 'et', 'rat', 'lift', 'got', 'realize', 'sabotage', 'mannnn', 'conflict', 'cradle', 'fix', 'pick', 'blast', 'avoid', 'toy', 'bug', 'kiss', 'evict', 'sing', 'rank', '’s', 'strategize', 'respone', 'pic', 'MAO-', 'scooch', 'offend', 'hunker', 'consist', 'pressure', 'incentivise', 'single', 'reopen', 'pain', 'center', 'intyrolia', 'save', 'baffle', 'nvm', 'hypothetical', 'dabble', 'flee', 'tifficult', 'co', 'steam', 'eradicate', 'assume', 'discover', 'irk', 'blab', 'crouch', 'bring', 'whittle', 'walk', 'foment', 'sink', 'step', 'boneheade', 'clarify', 'interrupt', 'down', 'postpone', 'really', 'hedgehog', 'excersize', 'opt', 'smooth', 'mess', 'pause', 'instate', 'finland', 'plot', 'combine', 'greece', 'wierd', 'endeavour', 'skill', 'doxe', 'assault', 'voice', 'impress', 'revise', 'round', 'deliver', 'remember', 'sandwich', 'fuel', 'communicate', 'mansplain', 'derail', 'presume', 'tho', 'sha', 'be', 'mistake', 'coud', 'appearance', 'backfille', 'cooperate', 'conquer', '’ve', 'outplay', 'clear', 'breed', 'work', 'outnumber', 'crap', 'backstabber', 'hose', 'upload', 'abondon', 'ride', 'amuse', 'spa', 'shape', 'blabber', 'hide', 'help@me', 'aid', 'differ', 'warry', 'whet', 'notify', 'swallow', 'ready', 'tiebreake', 'refocus', 'authenticate', 'rectify', 'hold', 'excite', 'ice', 'complement', 'scream', 'mught', 'humor', 'curtain', 'explore', 'come', 'follow', 'conviy', 'fulfil', 'denmark', 'welcome', 'dw', 'lol', 'defer', 'sum', 'likey', 'strand', 'tan', 'perceive', 'fish', 'waive', 'punish', 'conceal', 'onboard', 'scheme', 'guard', 'become', 'part', 'triangulate', 'reevaluate', 'rock', 'rate', 'misclicke', 'leave', 'cripple', 'contact', 'deescalate', 'simulate', 'suppress', 'shot', 'clown', 'reposte', 'telegraph', 'appal', 'intend', 'spin', 'report', 'incorporate', 'Sev', 'recognise', 'hurt', 'criticize', 'supprort', 'risk', 'suck', 'sell', 'float', 'accelerate', 'expect', 'judge', 'reroute', 'pour', 'dispense', 'witness', 'predetermine', 'scroll', 'tip', 'deprive', 'spring', 'eject', 'asap', 'eliminate', 'assuage', 'lock', 'legit', 'cagey', 'cease', 'edit', 'hang', 'sike', 'downvote', 'snitch', 'trouble', 'scramble', 'evolve', 'spurn', 'stab', 'suffer', 'exact', 'piece', 'bro', 'challenge', 'overreact', 'freak', 'damn', 'critique', 'destroy', 'vacation', 'bust', 'gal', 'blitzkrieg', 'reckon', 'use', 'sympathize', 'mate', 'adhere', 'mow', 'revert', 'waver', 'arm', 'ugh', 'bid', 'deduce', 'google', 'dot', 'afford', 'aggress', 'remove', 'exit', 'inch', 'strongarm', 'lepantoe', 'number', 'key', 'drift', 'attempt', 'desire', 'disgusting', 'divvy', 'confide', 'string', 'torture', 'throat', 'complete', 'undefende', 'impose', 'rise', 'reassess', 'floor', 'proceed', 'dismantle', 'flip', 'cruise', 'luck', 'help', 'slingshot', 'concert', 'roll', 'drink', 'clamp', 'nose', 'austrai', 'bla', 'calculate', 'stock', 'squeeze', 'count', 'distribute', 'gunboat', 'smooshe', 'illustrate', 'blue', 'date', 'function', 'research', 'rotate', 'whisper', 'discourage', 'burn', 'slam', 'tap', 'xposte', 'hahaha-', 'pair', 'paste', 'spoil', 'smack', 'subject', 'foresee', 'forward', 'throw', 'oppose', 'conference', 'reflect', 'entrench', 'mark', 'chow', 'persuade', 'propose', 'rome', 'plunge', 'collude', 'reposition', 'top', 'snap', 'bre', 'evade', 'evacuate', 'menace', 'distrupte', 'snub', 'rearrange', 'elliminate', 'nab', 'contend', 'hit', 'attacked', 'convince', 'dissolve', 'serve', 'wage', 'expand', 'face', 'have', 'preppe', 'combat', 'sober', 'lend', 'dedicate', 'appease', 'lull', 'prioritize', 'tear', 'focusse', 'shake', 'benefit', 'squish', 'sue', 'coordinate-', 'fling', 'flaunt', 'neutralise', 'misinterpret', 'remain', 'enjoy', 'ring', 'review', 'neutralize', 'line', 'dude', 'log', 'zone', 'trail', 'provoke', 'retype', 'fate', 'forgot', 'guarantee', 'creep', 'explode', 'cheerlead', 'begin', 'bullshit', 'backfill', 'gte', 'evertyhe', 'relax', 'lmao', 'materialize', 'inquire', 'imagine', 'visit', 'allay', 'hope', '*', 'end', 'object', 'ukr', 'will(most', 'defeat', 'redeploy', 'peel', 'loose', 'spice', 'revisit', 'alliance', 'annye', 'trick', 'sent', 'doing', 'evidence', 'tyr', 'shit', 'belong', 't', 'kinda', 'disbelieve', 'coach', 'deserve', 'complicate', 'what’re', 'charm', 'recalculate', 'disappear', 'texte', 'NAf', 'swear', 'urge', 'weather', 'prevail', 'haaaad', 'demilitarise', 'attach', 'clutch', 'bury', 'mind', 'ik', 'leant', 'discord', 'orchestrate', 'bluff', 'deadset', 'shower', 'bite', 'score', 'think', 'tangle', 'rally', 'forget', 'wish', 'chat', 'lumber', 'disrupt', 'overlook', 'trip', 'lose', 'gather', 'divvye', 'head', 'strengthen', 'smell', 'disrespect', 'wal', 'shatter', 'bolster', 'parachute', 'kill', 'circle', 'submit', 'diffuse', 'sweeten', 'scoop', 'resist', 'incriminate', 'address', 'forge', 'Btw', 'tend', 'compensate', 'reconsider', 'incite', 'remedy', 'coincide', 'verify', 'beniftte', 'disengage', 'descend', 'lmk', 'disappoint', 'meddle', 'reconvene', 'punch', 'sad', 'flood', 'structure', '’re', 'jackass', 'escalate', 'decommision', 'achieve', 'breach', 'forego', 'travel', 'commit', 'ponder', 'threaten', 'crave', 'react', 'outmaneuver', 'gun', 'confirm', 'bunch', 'mask', 'survive', 'turkey', 'war', 'standby', \"arn't\", 'blitz', 'fill', 'dammit', 'steal', 're', 'day-', 'geek', 'suit', 'display', 'sign', 'eat', 'shut', 'upset', 'ai', 'swe', 'wreak', 'sleep', 'transit', 'disband', 'defuse', \"havn't\", 'provide', 'cement', 'equal', 'place', 'tyroli', 'unite', 'saty', 'protest', 'renew', 'flub', 'mean-', 'answer', 'point', 'manipulate', 'heartless', 'toast', 'estimate', 'create', 'prime', 'access', 'revenge', 'aggravate', 'image', 'disclose', 'forgo', 'influence', 'backstab', 'divert', 'write', 'cascade', 'hinge', 'navigate', 'indulge', 'mun', 'utilize', 'connect', 'anoye', 'piff', 'renderee', 'attend', 'eye', 'root', 'seem', 'procede', 'respond', 'poise', 'misread', 'seize', 'subdue', 'comfort', 'restore', 'miss', 'slander', 'waltz', 'gre', 'earn', 'do-', 'suport', 'like', 'thank', 'call', 'distract', 'choose', 'argue', '’', 'hol', 'prevent', 'benifit', 'gobble', 'update', 'tweet', 'receive', 'further', 'uncoordinate', 'blow', 'hedgehogge', 'hamstring', 'yesssss', 'encode', 'emphasize', 'miff', 'anger', 'sandboxe', 'gussy', 'organize', 'engage', 'request', 'amp', 'rope', 'tun', 'screen', 'brake', 'contest', 'shout', 'entertain', 'overcommitte', 'percieve', 'oust', 'tie', 'withdraw', 'represent', 'firm', 'resende', 'bicker', 'revolt', 'misunderstand', 'rape', 'reward', 'flirt', 'yo', '😁', 'change', 'butter', 'tri', 'accuse', 'ferret', 'pack', 'lean', 'bump', 'sound', 'cyberstalk', 'grow', 'station', 'govern', 'overcommit', 'figure', 'h', 'fit', 'reallocate', 'value', 'sweep', 'bamboozle', 'ack', 'mistype', 'honour', 'rack', 'dicey', 'rebuild', 'cute', 'screw', 'maul', 'butt', 'deteriorate', 'arrivederci', 'ur', 'compare', 'smash', 'rid', 'quibble', 'rely', 'facetime', 'plane', 'coexist', 'cap', 'twiddle', 'psyche', 'ouch', 'support', 'confuse', 'close', 'deal', 'succeed', 'treat', 'conspire', 'park', 'wonder', 'predict', 'teeter', 'recommend', 'reorient', 'acquire', 'whipe', 'fare', 'plop', 'fracture', 'pool', 'snarke', 'check', 'succumb', 'fixate', 'le', 'process', 'slip', 'message', 'chew', 'wake', 'overload', 'bend', 'shift', 'plow', 'confess', 'coax', 'ya', 'stamp', 'is-', 'us', 'gallivant', 'pounce', 'resign', 'echo', 'cave', 'ruse', 'ignore', 'drag', 'veto', 'preffer', 'fly', 'gamble', \"wasn't;t\", 'tether', 'thing', '😝', 'notice', 'slice', 'sweettalke', 'playdip', 'blame', 'acknowledge', 'bout', 'retake', 'converge', 'forbid', 'warn', 'fortify', 'own', \"dmz'e\", 'ward', 'lepanto', 'berate', 'yor', 'tbqh', 'contemplate', 'contradict', 'recover', 'discard', 'Let', 'simplify', 'nip', 'stage', 'snowball', 'intrigue', 'judtcrebuild', 'loom', 'evaporate', 'settle', 'rn-', 'admit', 'specify', 'diminish', 'ascertain', 'stare', 'recruit', 'bare', 'total', 'overwhelm', 'pray', 'amazement', 'do', 'overcompensate', 'project', 'warm', 'articulate', 'play', 'convey', 'betray', 'wear', 'fan', 'def', 'doubt', 'r', 'genocide', 'kid', 'align', 'trap', 'start', 'consider', 'dislodge', 'catch', 'put', 'flounder', 'fineove', 'contribute', 'committed', 'testament', 'emerge', 'obtain', 'blaze', 'snag', 'bypass', 'confront', 'adjudicate', 'hell', 'wither', 'botch', 'control', 'learn', 'hammer', 'correspond', \"'\", '😟', 'tempt', \"did't\", '😀', 'thrill', 'retreat', 'vote', 'vie', 'expose', 'introduce', 'render', 'inform', 'empty', 'warsaw', 'position', 'apologize', 'collaborate', 'match', 'dictate', 'game', 'delete', 'carry', 'salvage', 'mishandle', 'net', 'target', 'melt', 'frustrate', 's', 'swindle', 'bequeath', 'smile', 'watch', 'shore', 'mean', 'arouse', 'fend', 'hop', 'cry', 'note', 'join', 'shead', 'light', 'calm', 'bogge', 'regroup', 'corner', 'truce', '🏻', 'stack', 'crowd', 'add', 'plant', 'trifle', 'allow', 'prepare', 'order', 'flag', 'obscure', 'obsess', 'carve', 'rum', 'tread', 'contain', 'torque', 'squabble', 'escape', 'wreck', 'nail', 'retaliate', 'respect', 'cheer', 'tyrolia', 'aim', 'dart', 'renegotiate', 'dox', 'cross', 'in', 'race', 'backstabbe', 'sort', 'foil', 'probe', 'type', 'dupe', 'remind', 'get', 'clobber', 'pin', 'coerce', 'post', 'interfere', 'mediate', 'surge', 'convoy', 'pull', 'cost', 'sup', 'mop', 'fancy', 'spot', 'arrange', 'beat', 'patch', 'attack', 'reason', 'slow', 'involve', 'hedge', 'concern', 'courtmartiale', 'book', 'negotiate', 'overselle', 'ish', 'preoccupy', 'gut', 'trl+boh+sil', 'speak', 'declare', 'roam', 'rear', 'betrahe', 'separate', 'misjudge', 'operate', 'look', 'spook', 'steer', 'glance', 'skate', 'regain', 'bottle', 'pay', 'teyz', 'terrify', 'gtg', 'demand', 'overcome', 'gauge', 'solidify', 'withhold', 'appreciate', 'anticipate', 'bide', 'result', 'counter', 'preserve', 'to@assure', 'tks', 'run', 'waiver', 'd', 'guide', 'assist', 'embroil', 'dare', 'alter', 'cede', 'feign', 'nmr', 'c’m', 'repeat', 'jeapordize', 'infer', 'exist', 'decorate', 'sacrifice', 'deter', 'balance', 'decieve', 'warrant', 'discuss', 'paw', 'tighten', 'garuntee', 'beg', 'vow', 'consummate', 'upgrade', 'stop', 'rout', 'posture', 'swinge', 'mention', 'tyn', 'configure', 'concede', 'invade', 'chase', 'transpire', 'yield', 'enthuse', 'entangle', 'express', 'germany', 'mob', 'partition', 'overstep', 'taunt', 'kayak', 'kick', 'depend', 'ceasefire', 'suffice', 'idk', 'please', 'repair', 'launch', 'seal', 'unsure', 'enforce', 'resume', 'fake', 'steamroll', 'sniff', 'peak', 'grill', 'give', 'hash', 'move', 'show', 'vouch', 'boh', 'stall', 'detente', 'prearrange', 'drain', 'understand', 'bolo', 'dmz', 'press', 'recognize', 'cause', 'query', 'curtail', 'collect', 'relate', 'yell', 'pass', 'talk', 'resent', 'quit', 'border', 'lay', 'pry', '👌', 'coordinate', 'slide', 'goof', 'extrapolate', 'solo’d', 'question', 'chance', 'gucci', 'worry', 'makd', 'wipe', 'beleive', 'attract', 'lobby', 'stalemate', 'amass', 'iron', 'weaken', 'W/', 'mistrust', 'sweat', 'batten', 'stay', 'ukraine', 'arise', 'jump', 'greeting', 'exclude', 'tease', 'evaluate', 'arggggg', 'exploit', 'rush', 'restart', 'meet', 'split', 'dry', 'onslaught', 'cancel', 'notwant', 'stp', 'rip', 'refrain', 'giggle', 'band', 'dislike', 'lace', 'concur', 'remebwrb', '’d', 'intervene', 'erode', 'determine', 'limit', 'norway', 'advance', 'wheedle', '🙂', 'spite', 'deadlocke', 'stoop', 'ghost', 'march', \"ol'-fashione\", 'associate', 'gl', 'back', 'mistreat', 'near', 'tell', 'grab', 'defend', 'fear', 'pocket', 'finesse', 'singe', 'spend', 'drop', 'hear', 'forgive', 'buddye', 'portugal', 'hate', 'were)', 'delay', 'prep', 'lvn', 'swoop', 'refer', 'barge', 'regard', 'wainte', 'describe', 'aassume', 'indicate', 'tune', 'blitzkreige', 'experience', '’m', 'ensure', 'pledge', 'sway', 'erase', 'coordainate', 'exacerbate', 'misclick', 'stick', 'peal', 'hinder', 'out', 'try', 'doin', 'marry', 'bode', 'suggesst', 'progress', 'partner', 'maroon']\n"
     ]
    }
   ],
   "source": [
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "78533c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract motion verbs from the list of all verbs using Chat-Gpt\n",
    "motion_verbs = ['pitch', 'run', 'shudder', 'chase', 'track', 'arise', 'shrink', 'ride', 'roll', 'fly', 'bounce', 'mow', 'cut',\n",
    "                'regroup', 'chill', 'overrun', 'descend', 'explode', 'switch', 'hug', 'eat', 'jiggle', 'dance', 'park', 'sit', \n",
    "                'seek', 'return', 'gallivant', 'transit', 'dive', 'pop', 'bump', 'send', 'steal', 'wash', 'walk', 'swing', \n",
    "                'float', 'squeeze', 'scroll', 'stick', 'poke', 'catch', 'evacuate', 'melt', 'raise', 'board', 'pour', 'move', \n",
    "                'scramble', 'hang', 'trigger', 'fling', 'crash', 'start', 'dry', 'answer', 'stall', 'peel', 'capture', 'run', \n",
    "                'box', 'end', 'look', 'try', 'hover', 'jump', 'acquire', 'tie', 'dip', 'recover', 'explore', 'fish', 'hop',\n",
    "                'take', 'execute', 'rob', 'launch', 'arrive', 'collect', 'harm', 'do', 'hand', 'gather', 'give', 'split', \n",
    "                'trip', 'close', 'part', 'snap', 'bury', 'rush', 'greet', 'weigh', 'swing', 'pay', 'enter', 'put', 'separate',\n",
    "                'unite', 'fix', 'wrap', 'skate', 'tell', 'lead', 'rescue', 'eject', 'perceive', 'rum', 'loosen', 'welcome',\n",
    "                'break', 'tread', 'speak', 'freak', 'protect', 'impose', 'explain', 'attend', 'prepare', 'tend', 'upgrade', \n",
    "                'advance', 'sneak', 'kick', 'consume', 'continue', 'train', 'drag', 'hint', 'barge', 'volunteer', 'swoop',\n",
    "                'rotate', 'head', 'live', 'match', 'exit', 'ward', 'string', 'tease', 'side', 'offer', 'pull', 'turn', 'shut', \n",
    "                'laugh', 'extract', 'click', 'overcome', 'race', 'grill', 'lay', 'declare', 'ski', 'face', 'snag', 'deceive',\n",
    "                'survive', 'design', 'spin', 'build', 'discard', 'slam', 'reconsider', 'have', 'pin', 'realize', 'foreclose', \n",
    "                'emerge', 'provide', 'picture', 'ascertain', 'pass', 'work', 'cascade', 'emerge', 'contact', 'alter', 'receive',\n",
    "                'dig', 'interpret', 'mop', 'spread', 'need', 'plan', 'smell', 'hit', 'object', 'block', 'pretend', 'empty', \n",
    "                'blow', 'bind', 'stab', 'reason', 'watch', 'standby', 'crave', 'verify', 'corner', \"approach\",\"arrive\",\"ascend\",\n",
    "                \"backpedal\",\"balance\",\"bounce\",\"climb\",\"crawl\",\"creep\",\"dance\",\"descend\",\"dodge\",\"drag\",\"drift\",\"drive\",\"fall\",\n",
    "                \"float\",\"fly\",\"gallop\",\"glide\",\"hop\",\"jump\",\"kick\",\"leap\",\"march\",\"move\",\"paddle\",\"pedal\",\"pull\",\"push\",\"run\",\n",
    "                \"sail\",\"scoot\",\"skate\",\"slide\",\"spin\",\"split\",\"sprint\",\"stand\",\"stop\",\"stroll\",\"swim\",\"swing\",\"trot\",\"walk\",\n",
    "                \"waltz\",\"wiggle\",\"zoom\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "46e60234",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def count_motion_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    # count motion verbs\n",
    "    count = sum(1 for token in doc if token.pos_ == \"VERB\" and token.lemma_ in motion_verbs)\n",
    "    return count\n",
    "\n",
    "new_df['motion_verb_count'] = new_df['processed_message'].apply(count_motion_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "baba2816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_message</th>\n",
       "      <th>motion_verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany!  Just the person I want to speak with...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've whet my appetite, Italy. What's the sug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>👍</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It seems like there are a lot of ways that cou...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah, I can’t say I’ve tried it and it works, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17284</th>\n",
       "      <td>You and Austria are the most importand. Italy ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17285</th>\n",
       "      <td>Hello, Turkey?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17286</th>\n",
       "      <td>Hello???</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17287</th>\n",
       "      <td>Helloooo, turkey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17288</th>\n",
       "      <td>I don’t understand these messages or tone. Are...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17289 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       processed_message  motion_verb_count\n",
       "0      Germany!  Just the person I want to speak with...                  4\n",
       "1      You've whet my appetite, Italy. What's the sug...                  0\n",
       "2                                                      👍                  0\n",
       "3      It seems like there are a lot of ways that cou...                  3\n",
       "4      Yeah, I can’t say I’ve tried it and it works, ...                 11\n",
       "...                                                  ...                ...\n",
       "17284  You and Austria are the most importand. Italy ...                  5\n",
       "17285                                     Hello, Turkey?                  0\n",
       "17286                                           Hello???                  0\n",
       "17287                                   Helloooo, turkey                  0\n",
       "17288  I don’t understand these messages or tone. Are...                  4\n",
       "\n",
       "[17289 rows x 2 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['processed_message', 'motion_verb_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d7f69b",
   "metadata": {},
   "source": [
    "---\n",
    "#### frequency of past tense verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dcc419ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\766619\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "def get_past_tense_verbs(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(word_tokens)\n",
    "    past_tense_verbs = [word for word, pos in tagged if pos == 'VBD']\n",
    "    return len(past_tense_verbs)\n",
    "\n",
    "new_df['num_past_tense'] = new_df['processed_message'].apply(get_past_tense_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ab372368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_message</th>\n",
       "      <th>num_past_tense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany!  Just the person I want to speak with...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've whet my appetite, Italy. What's the sug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>👍</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It seems like there are a lot of ways that cou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah, I can’t say I’ve tried it and it works, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17284</th>\n",
       "      <td>You and Austria are the most importand. Italy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17285</th>\n",
       "      <td>Hello, Turkey?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17286</th>\n",
       "      <td>Hello???</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17287</th>\n",
       "      <td>Helloooo, turkey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17288</th>\n",
       "      <td>I don’t understand these messages or tone. Are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17289 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       processed_message  num_past_tense\n",
       "0      Germany!  Just the person I want to speak with...               2\n",
       "1      You've whet my appetite, Italy. What's the sug...               0\n",
       "2                                                      👍               0\n",
       "3      It seems like there are a lot of ways that cou...               0\n",
       "4      Yeah, I can’t say I’ve tried it and it works, ...               3\n",
       "...                                                  ...             ...\n",
       "17284  You and Austria are the most importand. Italy ...               0\n",
       "17285                                     Hello, Turkey?               0\n",
       "17286                                           Hello???               0\n",
       "17287                                   Helloooo, turkey               0\n",
       "17288  I don’t understand these messages or tone. Are...               1\n",
       "\n",
       "[17289 rows x 2 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['processed_message', 'num_past_tense']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e15ab8",
   "metadata": {},
   "source": [
    "---\n",
    "#### frequency of future tense verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2d47b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eec99c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future_tense_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    future_tense_verbs = []\n",
    "    for token in doc:\n",
    "        # check if the token is not the first or the last in the doc\n",
    "        if token.i > 0 and token.i < len(doc) - 1:\n",
    "            # handling the case for \"will\" and \"shall\"\n",
    "            if token.lower_ in [\"will\", \"shall\"]:\n",
    "                next_token = token.nbor()  # The token after \"will\" or \"shall\"\n",
    "                if next_token.pos_ == \"VERB\":\n",
    "                    future_tense_verbs.append(next_token.text)\n",
    "            # handling the case for \"is going to\" --------------- should also handle \"are going to\" ------------\n",
    "            elif token.lower_ == \"going\" and token.nbor(-1).lower_ == \"is\" and token.nbor(1).lower_ == \"to\":\n",
    "                next_token = token.nbor(1)  # the token after \"going\", expected \"to\"\n",
    "                next_next_token = next_token.nbor()  # the token after \"to\"\n",
    "                if next_next_token.pos_ == \"VERB\":\n",
    "                    future_tense_verbs.append(next_next_token.text)\n",
    "    # for the case where \"will\" or \"shall\" is the last word in a sentence\n",
    "    for sent in doc.sents:\n",
    "        last_word = sent[-1]\n",
    "        if last_word.lower_ in [\"will\", \"shall\"]:\n",
    "            # handle cases or add context-specific actions\n",
    "            pass  # or add to list if contextually correct in sentences like \"Tomorrow, I will.\"\n",
    "    return len(future_tense_verbs)\n",
    " \n",
    "\n",
    "new_df['num_future_tense'] = new_df['processed_message'].apply(get_future_tense_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2df84324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_message</th>\n",
       "      <th>num_future_tense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany!  Just the person I want to speak with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You've whet my appetite, Italy. What's the sug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>👍</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It seems like there are a lot of ways that cou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeah, I can’t say I’ve tried it and it works, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17284</th>\n",
       "      <td>You and Austria are the most importand. Italy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17285</th>\n",
       "      <td>Hello, Turkey?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17286</th>\n",
       "      <td>Hello???</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17287</th>\n",
       "      <td>Helloooo, turkey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17288</th>\n",
       "      <td>I don’t understand these messages or tone. Are...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17289 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       processed_message  num_future_tense\n",
       "0      Germany!  Just the person I want to speak with...                 0\n",
       "1      You've whet my appetite, Italy. What's the sug...                 0\n",
       "2                                                      👍                 0\n",
       "3      It seems like there are a lot of ways that cou...                 0\n",
       "4      Yeah, I can’t say I’ve tried it and it works, ...                 0\n",
       "...                                                  ...               ...\n",
       "17284  You and Austria are the most importand. Italy ...                 0\n",
       "17285                                     Hello, Turkey?                 0\n",
       "17286                                           Hello???                 0\n",
       "17287                                   Helloooo, turkey                 0\n",
       "17288  I don’t understand these messages or tone. Are...                 0\n",
       "\n",
       "[17289 rows x 2 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[['processed_message', 'num_future_tense']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8f0946e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>message</th>\n",
       "      <th>sender_label</th>\n",
       "      <th>receiver_label</th>\n",
       "      <th>score</th>\n",
       "      <th>processed_message</th>\n",
       "      <th>num_words_sentence</th>\n",
       "      <th>stop_words_count</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>motion_verb_count</th>\n",
       "      <th>num_past_tense</th>\n",
       "      <th>num_future_tense</th>\n",
       "      <th>pron_freq</th>\n",
       "      <th>article_freq</th>\n",
       "      <th>prep_freq</th>\n",
       "      <th>adj_freq</th>\n",
       "      <th>top_bigram_freq</th>\n",
       "      <th>top_trigram_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>italy</td>\n",
       "      <td>germany</td>\n",
       "      <td>Just an FYI: I’ve now had both England and Fra...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>Just an FYI: I’ve now had both England and Fra...</td>\n",
       "      <td>80</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053571</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>germany</td>\n",
       "      <td>italy</td>\n",
       "      <td>Okay—sorry for being nosy! I will try for bur ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>Okay—sorry for being nosy! I will try for bur ...</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>germany</td>\n",
       "      <td>italy</td>\n",
       "      <td>You know italy, I think we *do* need to coordi...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>You know italy, I think we *do* need to coordi...</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>germany</td>\n",
       "      <td>italy</td>\n",
       "      <td>It looks like England's not willing to try for...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>It looks like England's not willing to try for...</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>italy</td>\n",
       "      <td>germany</td>\n",
       "      <td>I mean it sincerely. I think that England will...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>I mean it sincerely. I think that England will...</td>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177500</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17221</th>\n",
       "      <td>11</td>\n",
       "      <td>russia</td>\n",
       "      <td>turkey</td>\n",
       "      <td>Germany will help take care of Austria if we w...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>Germany will help take care of Austria if we w...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17235</th>\n",
       "      <td>11</td>\n",
       "      <td>turkey</td>\n",
       "      <td>russia</td>\n",
       "      <td>Sometimes that will happen when no one hears f...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>Sometimes that will happen when no one hears f...</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17251</th>\n",
       "      <td>11</td>\n",
       "      <td>france</td>\n",
       "      <td>russia</td>\n",
       "      <td>Germany is going to move into sweden with engl...</td>\n",
       "      <td>True</td>\n",
       "      <td>NOANNOTATION</td>\n",
       "      <td>6</td>\n",
       "      <td>Germany is going to move into sweden with engl...</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17262</th>\n",
       "      <td>11</td>\n",
       "      <td>france</td>\n",
       "      <td>england</td>\n",
       "      <td>Ok, I didn't think Germany would betray me but...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>Ok, I didn't think Germany would betray me but...</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17282</th>\n",
       "      <td>11</td>\n",
       "      <td>france</td>\n",
       "      <td>turkey</td>\n",
       "      <td>Ok, I didn't think Germany would betray me but...</td>\n",
       "      <td>False</td>\n",
       "      <td>NOANNOTATION</td>\n",
       "      <td>8</td>\n",
       "      <td>Ok, I didn't think Germany would betray me but...</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>802 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       game_id   sender receiver  \\\n",
       "13           1    italy  germany   \n",
       "63           1  germany    italy   \n",
       "80           1  germany    italy   \n",
       "84           1  germany    italy   \n",
       "98           1    italy  germany   \n",
       "...        ...      ...      ...   \n",
       "17221       11   russia   turkey   \n",
       "17235       11   turkey   russia   \n",
       "17251       11   france   russia   \n",
       "17262       11   france  england   \n",
       "17282       11   france   turkey   \n",
       "\n",
       "                                                 message  sender_label  \\\n",
       "13     Just an FYI: I’ve now had both England and Fra...          True   \n",
       "63     Okay—sorry for being nosy! I will try for bur ...          True   \n",
       "80     You know italy, I think we *do* need to coordi...          True   \n",
       "84     It looks like England's not willing to try for...          True   \n",
       "98     I mean it sincerely. I think that England will...          True   \n",
       "...                                                  ...           ...   \n",
       "17221  Germany will help take care of Austria if we w...          True   \n",
       "17235  Sometimes that will happen when no one hears f...          True   \n",
       "17251  Germany is going to move into sweden with engl...          True   \n",
       "17262  Ok, I didn't think Germany would betray me but...         False   \n",
       "17282  Ok, I didn't think Germany would betray me but...         False   \n",
       "\n",
       "      receiver_label score                                  processed_message  \\\n",
       "13              True     3  Just an FYI: I’ve now had both England and Fra...   \n",
       "63              True     4  Okay—sorry for being nosy! I will try for bur ...   \n",
       "80              True     5  You know italy, I think we *do* need to coordi...   \n",
       "84              True     5  It looks like England's not willing to try for...   \n",
       "98              True     7  I mean it sincerely. I think that England will...   \n",
       "...              ...   ...                                                ...   \n",
       "17221           True     4  Germany will help take care of Austria if we w...   \n",
       "17235           True     4  Sometimes that will happen when no one hears f...   \n",
       "17251   NOANNOTATION     6  Germany is going to move into sweden with engl...   \n",
       "17262           True     8  Ok, I didn't think Germany would betray me but...   \n",
       "17282   NOANNOTATION     8  Ok, I didn't think Germany would betray me but...   \n",
       "\n",
       "       num_words_sentence  stop_words_count  ... sentiment  motion_verb_count  \\\n",
       "13                     80                44  ... -0.053571                  4   \n",
       "63                     18                11  ...  0.000000                  1   \n",
       "80                     50                25  ...  0.183333                  3   \n",
       "84                     48                19  ... -0.031250                  5   \n",
       "98                     40                18  ...  0.177500                  2   \n",
       "...                   ...               ...  ...       ...                ...   \n",
       "17221                  13                 6  ...  0.000000                  1   \n",
       "17235                  19                10  ...  0.000000                  0   \n",
       "17251                  17                 5  ...  0.000000                  2   \n",
       "17262                  31                16  ...  0.650000                  5   \n",
       "17282                  31                16  ...  0.650000                  5   \n",
       "\n",
       "       num_past_tense  num_future_tense  pron_freq  article_freq  prep_freq  \\\n",
       "13                  1                 1         11             4          7   \n",
       "63                  0                 1          2             1          3   \n",
       "80                  0                 1          4             1          7   \n",
       "84                  0                 1          6             2          7   \n",
       "98                  0                 1          8             0          5   \n",
       "...               ...               ...        ...           ...        ...   \n",
       "17221               0                 1          1             0          2   \n",
       "17235               0                 1          0             1          3   \n",
       "17251               0                 1          0             0          2   \n",
       "17262               2                 1          5             0          1   \n",
       "17282               2                 1          5             0          1   \n",
       "\n",
       "       adj_freq  top_bigram_freq  top_trigram_freq  \n",
       "13            2                3                 0  \n",
       "63            1                1                 0  \n",
       "80            2                1                 0  \n",
       "84            3                2                 0  \n",
       "98            4                2                 0  \n",
       "...         ...              ...               ...  \n",
       "17221         0                1                 0  \n",
       "17235         1                0                 0  \n",
       "17251         2                1                 1  \n",
       "17262         0                0                 0  \n",
       "17282         0                0                 0  \n",
       "\n",
       "[802 rows x 25 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[new_df['num_future_tense'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea18a56",
   "metadata": {},
   "source": [
    "---\n",
    "#### word/sentence embeddings (from e.g. bert-base encoder)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
